---
layout: post
title: 读《Attention》相关论文
date:   2019-01-01
categories: 深度学习
---
# 概述  
本文涉及论文包括:    
《Effective Approaches to Attention-based Neural Machine Translation》  
《Key-value Attention Mechanism for Neural Machine Translation》  
《A STRUCTURED SELF - ATTENTIVE SENTENCE EMBEDDING》  
以上论文提到了不同的Attention机制  

##无Attention机制的MNT：    
图中蓝色是Encoder，红色是Decoder    
NMT使用了recurrent architecture  
## Global attention model&Local attention model  




### Global attention model：  

### Local attention model  


## Key-value Attention Mechanism  
## Self-Attention  
