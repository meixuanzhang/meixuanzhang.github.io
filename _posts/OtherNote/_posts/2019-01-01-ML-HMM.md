---
layout: post
title: 隐马尔可夫模型（Hidden Markov Model，HMM）
date:   2019-01-01
categories: 
---

# 隐马尔可夫模型概念

当需要计算可观测序列概率时，可以使用Markov Chain,但很多时候我们感兴趣的是隐藏状态，没办法直接观测,例如：句子语法标记，我们不能直接观测到每次词标记，需要从词序列推断出词的标记。把标记叫作隐藏状态，无法直接观察出来。词记作观测值。    
HMM通过隐藏状态和观测值，建立概率模型，描述了一个含有隐含未知参数的马尔可夫过程。

**Notation**  

$$Q$$:隐藏状态**集合**$${q_{1},q_{2}..,q_{N}}$$  

$$V$$:观测值**集合**$${v_{1},v_{2}..,v_{M}}$$   

$$O$$:观测值序列 $$(o_{1},..o_{t},o_{t+1}...)$$

$$I$$:隐藏状态序列$$(i_{1},..i_{t},i_{t+1}...)$$

$$A$$:从一个隐藏状态转到另一个隐藏状态的概率矩阵,维度N*N    

$$B$$:不同隐藏状态下不同测量值概率矩阵，维度是N*M  

$$\pi$$:初始化隐藏状态概率分布


$$A=[a_{ij}]_{N \ast N}$$   

其中$$a_{ij}=P(i_{t+1}=q_{j} \mid i_{t}=q_{i}), i=1,2,3,..N;j=1,2,..N$$    

表示从隐藏状态$$q_{i}$$(t时刻的状态)转到隐藏状态$$q_{j}$$(t+1时刻的状态),也表示在时刻t处于状态$$q_{i}$$的条件下时刻t+1转移到状态$$q_{j}$$的概率。   

$$B=[b_{j}(k)]_{N \ast M}$$

其中$$b_{j}(k)=P(o_{t}=v_{k} \mid i_{t}=q_{j}), i=1,2,3,..N;j=1,2,..N$$  

表示在时刻t处于状态$$q_{i}$$的条件下生成观测值$$v_{k}$$  

$$\pi_{i}=P(i_{1}=q_{i}),i=1,2,..N$$  

隐马尔可夫模型(HMM)由初始概率状态$$\pi$$、隐藏转移概率矩阵$$A$$和观测值概率矩阵$$B$$决定，$$\pi ,A$$决定隐藏状态序列，$$B$$决定观测值序列，$$A,B,\pi$$是隐马尔可夫模型的三要素，用三元符号表示$$\lambda=(A,B,\pi)$$。  

隐藏状态转移概率矩阵$$A$$与初始状态概率向量$$\pi$$确定了隐藏的马尔可夫链，生成不可观测的状态序列，观测值概率矩阵$$B$$确定了如何从隐藏状态生成观测值，与隐藏状态序列综合确定了如何产生观测值序列。

隐马尔可夫模型的两个基本假设：  
1、齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t+1的隐藏状态依赖于t时刻的隐藏状态，与其他时刻的隐藏状态及观测值无关。  

$$P(i_{t+1} \mid i_{t},..i_{1},o_{t}..o_{1})=P(i_{t+1} \mid i_{t}), t=1,2,...T$$   

2、观测值独立性假设。即假设任意时刻的观测值只依赖于该时刻的马尔可夫链的隐藏状态，与其他观测值及隐藏状态无关。
 
$$P(o_{t} \mid i_{t},..i_{1},o_{t-1}..o_{1})=P(o_{t} \mid i_{t})$$   

# 观测值序列生成过程

输入：隐马尔可夫模型$$\lambda=(A,B,\pi)$$，观测序列长度T；  
输出：观测序列$$O=(o_{1},o_{2},...o_{T})$$    
1、按照初始状态分布$$\pi$$产生隐藏状态$$i_{1}$$  
2、令$$t=1$$  
3、按照隐藏状态$$i_{t}$$的观测概率分布$$b_{i_{t}}(k)$$生成观测$$o_{t}$$  
4、按照隐藏状态$$i_{t}$$的状态转移概率分布$$\{a_{i_{t}i_{t+1}}\}$$产生隐藏状态$$i_{t+1}$$    
5、令$$t=t+1:$$如果$$t<T$$,转步3；否则终止

# 隐马尔可夫模型的3种问题
1、概率计算问题。给定模型$$\lambda=(A,B,\pi)$$和观测序列$$O=(o_{1},o_{2},..,o_{T})$$，计算在模型$$\lambda$$下观测序列$$O$$出现的概率$$P(O ;\lambda)$$。   
2、学习问题。已知观测序列$$O=(o_{1},o_{2},...o_{T})$$,估计模型$$\lambda=(A,B,\pi)$$参数，使得在该模型下观测序列概率$$P(O \mid \lambda)$$最大，即使用极大似然估计的方法。  
3、预测问题（Decoding）。已知模型$$\lambda=(A,B,\pi)$$和观测序列$$O=(o_{1},o_{2},...o_{T})$$，求给定观测序列条件概率$$P(I \mid O)$$最大的状态序列。  

## 概率计算问题  

已知：模型$$\lambda=(A,B,\pi)$$和观测序列$$O=(o_{1},o_{2},...o_{T})$$   
计算：$$P(O ;\lambda)$$  

### 直接计算法  

$$
P(O ;\lambda)=\sum_{I}P(O,I;\lambda)\\
=\sum_{I}P(O \mid I;\lambda)P(I;\lambda)
$$   

状态序列$$I=(i_{1},i_{2},..i_{T})$$的概率是:  

$$P(I;\lambda)=\pi_{i_{1}} a_{i_{1}i_{2}}a_{i_{2}i_{3}}...a_{i_{T-1}i_{T}}$$


对于固定的状态序列$$I=(i_{1},i_{2},..i_{T})$$,观测序列$$O=(o_{1},o_{2},..o_{T})$$的概率是：  

$$P(O \mid I;\lambda)=b_{i_{1}}(o_{1})b_{i_{2}}(o_{2})..b_{i_{T}}(o_{T})$$

$$O,I$$联合概率是：  

$$P(O,I;\lambda)=\pi_{i_{1}} b_{i_{1}}(o_{1}) a_{i_{1}i_{2}} b_{i_{2}}(o_{2})....a_{i_{T-1}i_{T}} b_{i_{T}}(o_{T})$$  

观测序列$$O$$的概率是$$P(O ;\lambda)$$:   

$$P(O;\lambda)=\sum_{I}P(O \mid I;\lambda)P(I;\lambda)\\
=\sum_{i_{1},i_{2},..i_{T}} \pi_{i_{1}} b_{i_{1}}(o_{1}) a_{i_{1}i_{2}} b_{i_{2}}(o_{2})....a_{i_{T-1}i_{T}} b_{i_{T}}(o_{T})
$$  

### 前向算法  
定义到$$t$$时刻部分观测序列为$$o_{1},o_{2},...o_{t}$$且t时刻状态为$$q_{i}$$的概率为前向概率，记作：

$$a_{t}(i)=P(o_{1},o_{2},...o_{t},i_{t}=q_{i};\lambda)$$

1、初值  

$$a_{1}(i)=P(o_{1},q_{i};\lambda)=P(q_{i})P(o_{1} \mid q_{i} ;\lambda)=\pi_{i}b_{i}(o_{1}),i=1,2..N$$     

这里$$q_{i}$$i是值隐藏状态取值对应下标   

2、递推   

$$
a_{t+1}(i)=P(o_{1},o_{2},...o_{t},o_{t+1},i_{t+1}=q_{i};\lambda)\\
=[\sum_{j=1}^N P(o_{1},o_{2},...o_{t},i_{t}=q_{j};\lambda)P(i_{t+1}=q_{i} \mid i_{t}=q_{j};\lambda)]P(o_{t+1}\mid i_{t+1}=q_{i};\lambda)\\
=[\sum_{j=1}^N a_{t}(j)a_{ji}]b_{t}(o_{t+1}),i=1,2..N
$$   

3、终止   

$$
P(O;\lambda)=\sum_{i=1}^Na_{T}(i)\\
a_{T}(i)=P(o_{1},o_{2},...o_{T},i_{T}=q_{i};\lambda)\\
$$   

### 后向算法  
给定隐马尔可夫模型$$\lambda$$,定义时刻$$t$$状态为$$q_{i}$$的条件下，从$$t+1$$到$$T$$的部分观测序列为$$o_{t+1},o_{t+2},...o_{T}$$的概率为后向概率： 

$$\beta_{t}(i)=P(o_{t+1},o_{t+2},...o_{T} \mid i_{t}=q_{i};\lambda)$$  

1、$$T$$时刻$$\beta$$:   

$$\beta_{T}(i)=1,i=1,2,..,N$$  

2、对$$t=T-1,T-2,..1$$：  

$$
\beta_{t}(i)=P(o_{t+1},o_{t+2},...o_{T} \mid i_{t}=q_{i};\lambda)\\
=\sum_{j=1}^NP(o_{t+1},o_{t+2},...o_{T},i_{t+1}=q_{j}\mid i_{t}=q_{i};\lambda)\\
=\sum_{j=1}^N \frac{P(i_{t+1}=q_{j};\lambda)}{P(i_{t}=q_{i};\lambda)}\frac{P(o_{t+1},o_{t+2},...o_{T},i_{t}=q_{i},i_{t+1}=q_{j})}{P(i_{t+1}=q_{j};\lambda)}\\
=\sum_{j=1}^N \frac{P(i_{t+1}=q_{j};\lambda)}{P(i_{t}=q_{i};\lambda)}P(o_{t+1},o_{t+2},...o_{T},i_{t}=q_{i} \mid i_{t+1}=q_{j};\lambda)\\
=\sum_{j=1}^N \frac{1}{P(i_{t}=q_{i};\lambda)}P(i_{t+1}=q_{j};\lambda)P(i_{t}=q_{i}\mid i_{t+1}=q_{j};\lambda) P(o_{t+1},o_{t+2},...o_{T}\mid i_{t+1}=q_{j};\lambda)\\
=\sum_{j=1}^N\frac{1}{P(i_{t}=q_{i};\lambda)}P(i_{t}=q_{i},i_{t+1}=q_{j};\lambda)P(o_{t+1},o_{t+2},...o_{T}\mid i_{t+1}=q_{j};\lambda)\\
= \sum_{j=1}^N P(i_{t+1}=q_{j}\mid i_{t}=q_{i};\lambda)P(o_{t+1}\mid i_{t+1}=q_{j};\lambda)P(o_{t+2},...o_{T} \mid i_{t+1}=q_{j};\lambda)\\
=\sum_{j=1}^Na_{ij}b_{j}(o_{t+1})\beta_{t+1}(j),i=1,2,..N   
$$

3、计算$$P(O;\lambda)$$  
P(O;\lambda)=\sum_{i=1}^N

参考：  
李航《统计学习》
