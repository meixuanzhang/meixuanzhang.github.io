---
layout: post
title: 译Lecture 5 Parameter Estimation in Fully Observed Bayesian Networks
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍全观测贝叶斯网络(fully observed Bayesian Networks)参数估计 。  

# 介绍(Introduction)  

上一课，我们介绍了两种主要的图模型任务类型：推理(Inference)和学习(Learning)，并且我们主要讨论了推理(Inference)。 在本讲座中，我们将开始介绍学习(Learning)任务，并探索可用于全观测贝叶斯网络学习任务的技术。    

# 学习图模型(Learning Graphical Models)   

在开始之前，最好先了解一下什么是学习(Learning)以及实现的目标是什么。

## 目标(The goal)  

目标：给定一组独立样本(随机变量的赋值),找到最佳（或最有可能）贝叶斯网络(DAG和CPD)。 

如下图所示，我们得到了一组独立样本（二进制随机变量的赋值）。假设它是一个DAG，我们将学习节点之间的有向链接(因果关系),这个过程称为结构学习(Structural Learning)。学习条件概率是另一个被称为参数学习(Parameter learning)的任务。


![_config.yml]({{ site.baseurl }}/images/10708/image79.png)   

## 总览(Overview)

如下所示，我们对以下几个学习场景感兴趣：  

1、全观测的图模型(Completely observed GMs)   

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)  

2、部分或未观测到的图模型(Partially or unobserved GMs)  

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)（一个开放的研究课题）   

这里还列出了一些有用的估算原理：  

最大似然估计(MLE):Maximal likelihood estimation
贝叶斯估计:Bayesian estimation
最大条件似然: Maximal conditional likelihood
最大“边际”：Maximal “Margin”
最大熵：Maximum entropy  

我们使用learning作为估计参数这一过程的名字，在某些情况下，使用数据来估计网络的拓扑结构。

# 参数学习(Parameter Learning)  

在本节中，我们将介绍参数学习( Parameter Learning)和一些有用的相关模型。

在进行参数学习时，我们假设图模型$$G$$本身是已知的并且是固定的，$$G$$可能来自专家设计或迭代结构学习的中间结果。  

参数学习的目标是从$$N$$个独立的数据集中估计参数,具有相同分布的（i.i.d.）训练样本$$D=x_{1},...x_{N}$$  

通常，每个训练样本$$\mathbf{x}_{n}=(x_{n,1},..,x_{n,M})$$是一个$$M$$维向量，每一维是一个节点。根据模型，如果模型是完全可观测的，那么$$\mathbf{x}_{n}$$中的元素都是已知的（没有丢失的值，没有隐藏的变量）,如果模型是部分可观测的,那么$$\mathbf{x}_{n}$$中的元素部分可知($$\exists_{i},x_{n,1}$$is not observed)  

在本课程中，我们主要考虑学习在给定结构下完全可观测的$$BN$$的参数。   

# 分布族  

分布族(families of distributions),是一组分布集合，它们共享相同的参数形式，并且仅在选择具体参数时才不同。   

通常,一旦选择了网络的全局结构和局部结构，意味为CPDs的具体选择定义了可以通过不同的参数来获得所有分布的一个族。  

# 指数族分布(Exponential Family Distributions)  

有各种密度估计任务可以看作是单节点的GMs(参见补充部分的示例)，它们实际上是指数族分布的实例。  

指数族分布是普遍GMs的构造块，具有很好的性质，可以方便地进行MLE和贝叶斯估计。  

## 定义(Definition)    

对于数值随机变量$$x$$:   

$$x$$上的指数族由以下四个成分指定：  

+ 从$$x$$的赋值到$$R^K$$的一个充分统计量函数(sufficient statistic function)$$T(x)$$    

+ 参数空间是合法参数(legal parameter)的一个凸集(convex set)$$\theta \subseteq R^M$$构成的   

+ 从$$R^M$$到$$R^K$$的一个自然参数函数(natural parameter function)$$\psi$$   

+ $$x$$上的辅助测量$$h(x)$$  

函数$$A(\eta)=logZ(\eta)$$是 log normalizer.   

$$\psi(\theta)=\eta^T$$  

![_config.yml]({{ site.baseurl }}/images/10708/image88.png)  

$$A(\eta)=log \int h(x)exp\{\eta^T T(x)\} dx$$   

## 例子   

我们可以看许多概率分布实际上属于该族的(包括伯努利，多项式，高斯，泊松，伽玛)

**Example 1: 多元高斯分布(Multivariate Gaussian Distribution)**  

随机变量为连续向量$$x\in R^K$$的多元高斯分布密度函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image89.png)  

用指数族表示：  

![_config.yml]({{ site.baseurl }}/images/10708/image90.png)   

经过这种转换，我们看到多元高斯分布确实属于指数族。  

请注意，$$K$$维多元高斯分布具有$$K+K^2$$(均值,方差,协方差)维自然参数(和足够的统计量)。但是，由于$$\sum$$必须是对称的并且是半正定矩阵(PSD:positive semidefinite matrix)，因此参数实际上具有较低的自由度。        

**Example 2: 多项式分布(Multinomial Distribution)**  

对于随机变量二进制向量$$x \backsim multinomial(x\mid \pi)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image91.png)   

$$=exp\{\sum_{k=1}^{K-1}x_{k} ln(\frac{\pi_{k}}{1-\sum_{k=1}^{K} \pi_{k}}) + ln(1-\sum_{k=1}^{K} \pi_{k})\}$$  

其中：  

$$
(1- \sum_{k=1}^{K-1} x_{k})ln(1- \sum_{k=1}^{K-1} \pi_{k})=(1- \sum_{k=1}^{K-1} x_{k})ln\pi_{K}\\
=x_{K}ln\pi_{K}
$$

线性约束$$\sum_{k=1}^{K} x_{k}=1$$  

用指数族表示：    

![_config.yml]({{ site.baseurl }}/images/10708/image92.png)   

## 指数族的性质  

**矩生成的属性(Moment generating property)**  

指数族的$$q^{th}$$阶导数给出$$q^{th}$$阶中心矩。   

![_config.yml]({{ site.baseurl }}/images/10708/image93.png)   

因此，我们可以利用这个优势，通过log normalizer $$A(\eta)$$的导数来计算任意指数族分布的中心矩(Central moment)   

注：当充分统计量为叠加向量时，需要考虑偏导数。  

**矩和规范参数(Moment vs. canonical parameters)**   

应用矩生成的属性，可以通过以下方式从自然(规范)参数中得出矩参数$$\mu$$:   

![_config.yml]({{ site.baseurl }}/images/10708/image94.png)   

因此，我们可以反转关系并从矩参数推断出规范参数(1-对-1)  

$$\eta = \psi(\mu)$$  

因此我们可以说，指数族中的分布不仅可以通过$$\eta$$-规范参数化进行参数化，还可以通过$$\mu$$-矩参数化进行参数化。   

## MLE for Exponential Family  

对于独立同分布的数据，我们有对数似然：  

![_config.yml]({{ site.baseurl }}/images/10708/image95.png)   

取导数并将其设置为零：  

![_config.yml]({{ site.baseurl }}/images/10708/image96.png)   

这相当于矩匹配(moment matching), 我们可以用$$\tilde{\eta}_{MLE} = \psi(\tilde{\mu}_{MLE})$$推断规范参数 

## 充分性(Sufficiency)   

从上面我们可以看到$$p(x\mid \theta)$$，$$X$$表示随机变量，$$T(X)$$是统计量(statistic)，假设$$X$$分布取决于$$\theta$$，如果在$$T(X)$$条件中,$$X$$没有关于$$\theta$$的信息,$$T(X)$$对$$\theta$$是充足的(sufficient)，就是说，观测到$$T(X)$$之后，为了推论$$\theta$$，我们可以扔掉$$X$$。 

这可以用贝叶斯论(Bayesian view)和频率论(Frequentist view)来说明。    

贝叶斯论： 

$$\theta$$ 是一个随机变量，如果满足$$\theta \bot X \mid T(X)$$，我们说$$T(X)$$对$$\theta$$是充足的,直观上的概念，即$$T(X)$$包含$$X$$中关于$$\theta$$的所有基本信息。

![_config.yml]({{ site.baseurl }}/images/10708/image97.png)    

$$p(\theta\mid T(x),x)=p(\theta\mid T(x))$$    

(贝叶斯论中,概率是对一个事件的信念或确定性。观测数据被认为是固定的，但是模型参数是随机的(不确定的)，被认为是从某种概率分布中提取的.) 

频率论:  

$$\theta$$是一个label，而不是随机变量，同样在给定$$T(X)$$条件下$$\theta$$和$$X$$相互独立  

![_config.yml]({{ site.baseurl }}/images/10708/image98.png) 

$$p(x \mid T(x),\theta)=p(x \mid T(x))$$ 

(在频率统计中，观测到的数据被认为是随机的；如果你收集到更多的观测数据，它们将根据潜在的分布(即总体分布)而有所不同。然而，模型的参数被认为是固定的。)  

**The Neyman factorization theorem**

Bayesian和frequentist充分性定义都说明了$$p(x\mid \theta)$$的因子分解。为了得到因子分解，我们使用无向图模型形式。

![_config.yml]({{ site.baseurl }}/images/10708/image99.png)  

$$T(X)$$对$$\theta$$是充足的如果：  

$$p(x,T(x),\theta)=\psi_{1}(T(x),\theta)\psi_{2}(x,T(x))$$   

$$p(x\mid \theta)=g(T(x), \theta)h(x,T(x))$$  


**例子**    

![_config.yml]({{ site.baseurl }}/images/10708/image100.png)    

![_config.yml]({{ site.baseurl }}/images/10708/image101.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image102.png)  

# 广义线性模型(Generalized Linear Models)   

有各种条件密度估计任务可以看作是两节点图模型(GMs)(参见补充部分的示例)。许多是广义线性模型的实例。

广义线性模型（GLM）是普通线性回归的灵活概括，它允许线性模型通过链接函数(link function)与响应变量( response variables)相关联，这些响应变量具有除正态分布外的误差分布模型。 例如，线性回归和logistic回归都可以用广义线性模型统一起来。  

![_config.yml]({{ site.baseurl }}/images/10708/image103.png)  

$$X$$和$$Y$$是可观测的，与线性回归和(判别)线性分类模型一样，我们专注于$$X$$和$$Y$$之间的条件关系   

## 共性(Commonality)   

线性回归模型和判别线性分类模型的一个共同特点都是是对$$y$$的条件期望的一种特殊的表示   

我们模拟了$$y$$的期望$$E_{p}(\mathbf{y})=\mu=f(\theta^T\mathbf{x})$$  
     
$$p$$是$$y$$的条件概率分布     

$$f$$是响应函数(response function),$$f(\cdot)$$对于线性回归是恒等函数(identity function)$$\theta^T\mathbf{x} = f(\theta^T\mathbf{x})$$,对于线性分类模型，可以有不同的选择，包括逻辑斯蒂函数(用于logisti 回归)和累积高斯（用于概率回归）。   

我们赋予$$y$$一个特定的条件概率分布，以$$\mu$$为参数，GLIM(generalized linear model)对条件概率分布$$p(y\mid x)$$的形式做出三个假设:    

+ 假定观测到的输入$$\mathbf{x}$$通过线性组合$$\xi = \theta^T\mathbf{x}$$进入模型   

+ 条件平均值$$\mu$$表示为线性组合$$\xi$$的函数$$f(\xi)$$，$$f$$被称为$$\xi = \theta^T\mathbf{x}$$的链接函数

+ 假设观测到的输出$$y$$描述为具有条件均值$$\mu$$的指数族分布     

![_config.yml]({{ site.baseurl }}/images/10708/image108.png)  

指数族分布的选择通常受到$$Y$$的性质的限制。 因此，类别标签由伯努利(Bernoulli)或多项式(multinomial)分布表示，计数由泊松(Poisson)分布表示，区间由指数(exponential)或伽马(gamma)分布表示，等等    

这使"选择响应函数"作为GLIM的主要自由度。例如我们通常希望对响应函数施加约束来反映了对条件期望的约束。例如，对于伯努利分布和多项式分布，条件期望值必须介于0和1之间，这表明我们应该选择一个范围为(0,1)的响应函数。同样对于伽马分布。随机变量为非负，应该选择一个范围为$$(0,\infty)$$的响应函数  

这样的约束仅给出粗略的指导，但是，通常对于任何给定的分布，都有许多可能的响应函数选择。 但是，有一个特殊的响应函数，即规范响应函数(canonical response function)，它是唯一与给定的指数族分布存在相关？，并且具有一些吸引人的数学特性。假设$$\xi = \eta$$,则$$f(\cdot)=\psi^{-1}(\cdot)$$,则$$f$$是canonical response function，注意一旦选择了指数族密度函数则$$\psi(\cdot)$$是确定的。因此，如果我们决定使用规范响应函数，指数族密度函数的选择完全决定了GLIM.  

$$f(\eta) = \psi^{-1}(\eta)=a'(\eta)=E[Y\mid \eta]$$

**Example 1: 线性回归 (Linear Regression)**   

让我们快速回顾一下线性回归    

假设目标变量和输入之间的关系如下：   

$$y_{i} = \theta^T\mathbf{x}_{i} + \epsilon_{i}$$   

$$\epsilon$$是未建模效应(unmodeled effects)或随机噪声的误差项   

现在假设$$\epsilon$$服从高斯分布$$N(0,\sigma)$$   

$$p(y_{i}\mid \mathbf{x}_{i};\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(- \frac{(y_{i}- \theta^T\mathbf{x}_{i})^2}{2\sigma^2})$$

我们可以使用LMS算法（一种梯度上升/下降方法）来估算参数。 

**Example 2: 逻辑斯蒂回归 (Logistic Regression (sigmoid classifier, perceptron, etc.))**   

条件分布为伯努利： 

$$p(y\mid x)=\mu(x)^y(1-\mu(x))^{1-y}$$  

$$\mu$$是 logistic函数  

$$\mu(x) = \frac{1}{e^{-\theta^Tx}}$$    

我们可以像LR一样使用蛮力(force)梯度法。但我们也可以通过观察$$p(y∣x)$$是一个指数族函数-一个广义线性模型来应用一般规律,。

## More examples: 参数化图模型  

**马尔可夫随机场(Markov random fields)**   

![_config.yml]({{ site.baseurl }}/images/10708/image104.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image109.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image110.png)  

**受限波尔兹曼机 (Restricted Boltzmann Machines)**   

![_config.yml]({{ site.baseurl }}/images/10708/image105.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image111.png)  

**条件随机场(Conditional Random Fields)**   

![_config.yml]({{ site.baseurl }}/images/10708/image106.png)  

判别模型：  

![_config.yml]({{ site.baseurl }}/images/10708/image112.png)  


$$X$$被假定为相互依赖的特征   

在标记$$X$$时，考虑了未来的观测结果。  


## MLE for GLIMs  


![_config.yml]({{ site.baseurl }}/images/10708/image107.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image113.png)  

## 迭代加权最小二乘法(Iteratively Reweighted Least Squares,IRLS)


# 全局和局部参数独立性

简单的图模型可以看作是复杂图形模型的构建块。在相同的概念下，如果我们假设每个局部条件概率分布的参数是全局独立的，并且所有节点都被完全观测到，那么对数似然函数可以分解为一个局部项之和，每个节点一个：


**板(Plate)**  

板是允许子图被复制的宏(Macro,是一种批量处理的称谓)。按照惯例，不会单独绘制每个重复变量，而是使用一个板将这些重复变量分组为一个子图，并在板上绘制一个数字来表示子图在板中的重复次数。  

![_config.yml]({{ site.baseurl }}/images/10708/image115.png)  

板的规则：将方框中的每一个结构重复若干次，由方框角的整数(例如N)给出，并在执行过程中更新板块索引变量(例如n),通过将箭头连接到结构的每个副本，复制进入板的每个箭头和离开板的每个箭头。

例如，在有向无环网络中(directed acyclic network)，它可以分解为:   

$$p(x\mid \theta)=\sum_{i=1}^n p(x_{i}\mid \mathbf{x}_{\pi i})=p(x_{1}\mid \theta_{1})p(x_{2}\mid x_{1},\theta_{2})p(x_{3}\mid x_{1},\theta_{3})p(x_{4}\mid x_{2},x_{3},\theta_{4})$$

![_config.yml]({{ site.baseurl }}/images/10708/image116.png)   

全局参数独立性：对于每个DAG模型，令$$G$$是一个具有参数$$\theta=(\theta_{x_{1}\mid Pa_{x_{1}}},...,\theta_{x_{n}\mid Pa_{x_{n}}})$$的贝叶斯网结构，先验$$p(\theta)=\prod_{i}P(\theta_{x_{i}\mid Pa_{x_{i}}})$$。$$ Pa_{x_{i}}$$表示$$x_{i}$$父节点

局部参数独立性：对于每个节点，令$$x$$是一个父节点为$$U$$的变量。先验$$P(\theta_{x\mid U})=\prod_{u}P(x\mid u)$$。$$u$$是变量$$U$$不同取值  

**全局参数共享(Global parameter sharing)**  

考虑由参数集$$\theta$$参数化的一组变量$$\chi = \{X_{1},..,X_{n}\}$$上的网络结构$$G$$。每个变量$$X_{i}$$都与一个CPD $$P(X_{i}\mid U_{i},\theta)$$相关。现在与其假定每个CPD都有自己的参数$$\theta_{X_{i}\mid U_{i}}$$，不如假定我们有一组共享参数，供网络中的多个变量使用。这就是全局参数共享。  

假定$$\theta$$被划分为一些不相交的子集$$\theta^1,..,\theta^K$$,每一个子集相关一组变量$$\nu^k \subset  \chi$$,$$\nu^1,..\nu^K$$是$$\chi$$的一个不相交的划分。对于$$X_{i}\in \nu^k$$,有$$P(X_{i} \mid U_{i},\theta) = P(X_{i} \mid U_{i},\theta^k) $$     

假设CPD形式是一样的对于$$X_{i},X_{j}\in \nu^k$$,有$$P(X_{i}\mid U_{i},\theta^k)=P(X_{j}\mid U_{j},\theta^k)$$

# 监督的机器学习估计(Supervised ML estimation)   

## 隐马尔可夫模型（HMM） 

给定一个HMM,隐藏状态为$$y_{1},..y_{N}$$,观测变量为$$x_{1},..x_{N}$$  

在训练时间内，我们可以记录从隐藏状态到另一个隐藏状态，或从隐藏状态到观察状态的每个转换的频率。   

定义$$A_{ij}$$表示从隐藏状态$$i$$过渡到隐藏状态$$j$$,$$B_{ik}$$表示从隐藏状态$$i$$过渡到隐藏状态$$k$$,我们通过以下方法使用最大似然估计来计算参数：  

![_config.yml]({{ site.baseurl }}/images/10708/image117.png)  

如果我们观察到的是连续的，则可以将它们视为高斯，并为高斯应用相应的学习规则

**PROS**  

这种方法为我们提供了“最合适”的参数，或使看到训练数据的可能性最大化。 因此，对于测试数据，直觉上至少应该接近真实情况。 MLE在现实中往往会提供良好的性能。

**CONS** 

我们只是证明，一个测试数据计算概率的参数取决于训练时相同模式(测试数据与训练数据相同时)的频率。这会导致一个问题。如果有一个在训练数据中从未见过的测试数据案例，那么我们将自动为该测试数据分配一个零概率，这是“无限错误”的，因为它将导致从现实世界到我们的模型的无穷交叉熵。

这也说明了过度拟合的问题，即我们的训练数据拟合得太好，以至于我们无法将我们的模型很好地推广到现实世界。

## 伪计数(Pseudocounts)  


为了解决这个问题，我们可以在所有案例中添加“hallucinated counts”，这样即使是在训练时间中从未见过的案例也可以获得一定的计数。 因此，在测试时间不会将零概率分配给任何情况。

**HOW MANY PSEUDOCOUNTS DO WE ADD**  

想象一下，如果训练中案例的总频率是100，并且我们在一个案例中添加10000个计数，那么在测试时间内，该案例将被分配一个非常高的概率。我们刚才所做的等于说“我坚信这会发生”。换句话说，我们对那件事很有信心。

但是，如果我们只在每个案例中添加一个伪数，那么频繁案例的概率会降低，但不会太大。他们在概率上的排名不会改变，因为这只是他们在最大似然估计中的分母发生了同样的变化。剩余概率将分配给训练中从未见过的病例。它们很小，但这消除了测试中零概率的问题。这就是我们所说的平滑。

我们也可以将其视为在具有“参数强度(parameter strength)”的均匀先验下的贝叶斯估计，同时将伪数加入到案例中。

# 补充(Supplementary)  

密度估计可以看作是单节点的图模型，它是普遍GMs的构造块(building block)   

密度估计：在有限次观测$$x_{1},\dots,x_{N}$$的前提下，对随机变量$$x$$的概率分布$$P(x)$$进行建模

对于密度估计，我们有：

+ 最大似然估计
+ 贝叶斯估计 

## 离散分布(Discrete Distributions)  

伯努利分布：$$BER(P)$$      

$$P(x)=p^x(1-p)^{1-x}$$   

多项式分布; $$MULTINOMIAL(N,\theta)$$   

它一般与二项式分布( Bernoulli)类似。当参数中的“1”表示将只有一个试验时它类似于伯努利分布(Bernoulli)。现在我们有$$k$$个可能发生的实例，每个实例都有一个概率$$\theta_{i}$$,$$\sum_{i=1}^k \theta_{i}=1$$   

假设$$n$$是观测向量:  

![_config.yml]({{ site.baseurl }}/images/10708/image80.png)   

式子靠左分数涉及排列组合   

## MLE：拉格朗日乘子约束优化   

目标函数：

![_config.yml]({{ site.baseurl }}/images/10708/image81.png)  

约束条件：  

![_config.yml]({{ site.baseurl }}/images/10708/image82.png)   

带拉格朗日乘数的约束成本函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image83.png)   

## 贝叶斯估计  

狄利克雷分布(Dirichlet distribution)也称多元Beta分布(multivariate Beta distribution) 

[Dirichlet distribution](https://meixuanzhang.github.io/NLP-LDA/)   



## 序列贝叶斯更新  

## 层次贝叶斯模型(Hierarchical Bayesian Models)  

$$\theta$$ 是似然$$P(x\mid \theta)$$的参数，$$P(\theta\mid x)=P(x\mid \theta)P(\theta ; \alpha) $$   

$$\alpha$$ 是先验$$P(\theta ; \alpha)$$的参数,Dirichlet distribution分布的参数

我们可以有超参数，等等。  

当超参数的选择对边际似然没有影响时，我们停止；通常使超参数成为常数。

## Dirichlet先验的局限   

Dirichlet先验只能对所有坐标或单个坐标进行强调/偏移；例如，它不能强调两个坐标。不同超参数$$\alpha $$对应的先验如下图所示：  

![_config.yml]({{ site.baseurl }}/images/10708/image84.png)  

## The Logistic Normal Prior    

优点：协方差结构
缺点：非共轭

## 连续分布(Continuous Distributions)  

**均匀概率密度函数:**    

![_config.yml]({{ site.baseurl }}/images/10708/image85.png)  

**正态(高斯)概率密度函数:**  

![_config.yml]({{ site.baseurl }}/images/10708/image86.png)  

**多元高斯**

![_config.yml]({{ site.baseurl }}/images/10708/image87.png)  

## 多元高斯分布的极大似然估计   

可以证明MLE估计$$\mu$$和$$\sum$$为：  

![_config.yml]({{ site.baseurl }}/images/10708/image118.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image119.png)  

散射矩阵(scatter matrix)为：  

注意$$X^TX=\sum_{n}x_{n}x_{n}^T$$可能不是满秩的(如，如果$$N < D$$),这种情况下$$\sum_{ML}$$是不可逆的   


## 高斯分布的贝叶斯参数估计   

使用贝叶斯方法的原因：  

+ 随时间顺序更新估计值  

+ 我们可能对参数的预期值有先验知识  

+ 如果没有足够数据MLE计算的$$\sum$$可能不是满秩  

现在，我们仅考虑共轭先验，并考虑各种复杂性增加的情况：  

+ 未知$$\mu$$，已知$$\sigma$$     

+ 已知$$\mu$$,未知$$\sigma$$  

+ 未知$$\mu$$,未知$$\sigma$$ 

**未知$$\mu$$，已知$$\sigma$$**  

![_config.yml]({{ site.baseurl }}/images/10708/image121.png)  

后验均值$$\mu$$是样本均值和先验均值的凸组合(convex combination)，后验精度(precision)是先验精度加上每个观测数据点贡献的$$1/\sigma^2$$   

**已知$$\mu$$，未知$$\lambda = \sigma^{-2}$$**  

Conjugate prior 共轭先验  

![_config.yml]({{ site.baseurl }}/images/10708/image122.png)   

**未知$$\mu$$，未知$$\lambda$$**  

![_config.yml]({{ site.baseurl }}/images/10708/image123.png)  

## 两节点完全观测贝叶斯网络(BNs)   

![_config.yml]({{ site.baseurl }}/images/10708/image124.png)  

+ Conditional Mixtures：  

线性/Logistic 回归  

+ 分类  

生成(Generative)和判别(discriminative)方法

## 分类(Classification)   

目标：希望学习$$f: X \to Y$$  

生成式(Generative)：对数据的联合分布进行建模,即$$P(x,y)$$   

判别式(Discriminative)：仅对条件分布建模,即$$P(y \mid x)$$ 

**条件高斯**  

数据：$$\{(x_{1},y_{1}),(x_{2},y_{2}),..,(x_{n},y_{n})\}$$  

$$y$$是一个类别指标向量(one-hot encoding):$$p(y_{n})=multi(y_{n};\pi)=\prod_{k=1}^K  \pi_{k}^{y_{n,k}}$$

## 线性回归(Linear Regression)   

# 完全观测GMs的机器学习(ML)结构学习   

参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



