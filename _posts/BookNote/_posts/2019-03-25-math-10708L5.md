---
layout: post
title: 译Lecture 5 Parameter Estimation in Fully Observed Bayesian Networks
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍全观测贝叶斯网络(fully observed Bayesian Networks)参数估计 。  

# 介绍(Introduction)  

上一课，我们介绍了两种主要的图模型任务类型：推理(Inference)和学习(Learning)，并且我们主要讨论了推理(Inference)。 在本讲座中，我们将开始介绍学习(Learning)任务，并探索可用于全观测贝叶斯网络学习任务的技术。    

# 学习图模型(Learning Graphical Models)   

在开始之前，最好先了解一下什么是学习(Learning)以及实现的目标是什么。

## 目标(The goal)  

目标：给定一组独立样本(随机变量的赋值),找到最佳（或最有可能）贝叶斯网络(DAG和CPD)。 

如下图所示，我们得到了一组独立样本（二进制随机变量的赋值）。假设它是一个DAG，我们将学习节点之间的有向链接(因果关系),这个过程称为结构学习(Structural Learning)。学习条件概率是另一个被称为参数学习(Parameter learning)的任务。


![_config.yml]({{ site.baseurl }}/images/10708/image79.png)   

## 总览(Overview)

如下所示，我们对以下几个学习场景感兴趣：  

1、全观测的图模型(Completely observed GMs)   

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)  

2、部分或未观测到的图模型(Partially or unobserved GMs)  

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)（一个开放的研究课题）   

这里还列出了一些有用的估算原理：  

最大似然估计(MLE):Maximal likelihood estimation
贝叶斯估计:Bayesian estimation
最大条件似然: Maximal conditional likelihood
最大“边际”：Maximal “Margin”
最大熵：Maximum entropy  

我们使用learning作为估计参数这一过程的名字，在某些情况下，使用数据来估计网络的拓扑结构。

# 参数学习(Parameter Learning)  

在本节中，我们将介绍参数学习( Parameter Learning)和一些有用的相关模型。

在进行参数学习时，我们假设图模型$$G$$本身是已知的并且是固定的，$$G$$可能来自专家设计或迭代结构学习的中间结果。  

参数学习的目标是从$$N$$个独立的数据集中估计参数,具有相同分布的（i.i.d.）训练样本$$D=x_{1},...x_{N}$$  

通常，每个训练样本$$\mathbf{x}_{n}=(x_{n,1},..,x_{n,M})$$是一个$$M$$维向量，每一维是一个节点。根据模型，如果模型是完全可观测的，那么$$\mathbf{x}_{n}$$中的元素都是已知的（没有丢失的值，没有隐藏的变量）,如果模型是部分可观测的,那么$$\mathbf{x}_{n}$$中的元素部分可知($$\exists_{i},x_{n,1}$$is not observed)  

在本课程中，我们主要考虑学习在给定结构下完全可观测的$$BN$$的参数。   

# 分布族  

分布族(families of distributions),是一组分布集合，它们共享相同的参数形式，并且仅在选择具体参数时才不同。   

通常,一旦选择了网络的全局结构和局部结构，意味为CPDs的具体选择定义了可以通过不同的参数来获得所有分布的一个族。  

# 指数族分布(Exponential Family Distributions)  

有各种密度估计任务可以看作是单节点的GMs(参见补充部分的示例)，它们实际上是指数族分布的实例。  

指数族分布是普遍GMs的构造块，具有很好的性质，可以方便地进行MLE和贝叶斯估计。  

## 定义(Definition)    

对于数值随机变量$$x$$:   

$$x$$上的指数族由以下四个成分指定：  

+ 从$$x$$的赋值到$$R^K$$的一个充分统计量函数(sufficient statistic function)$$T(x)$$    

+ 参数空间是合法参数(legal parameter)的一个凸集(convex set)$$\theta \subseteq R^M$$构成的   

+ 从$$R^M$$到$$R^K$$的一个自然参数函数(natural parameter function)$$t$$   

+ $$x$$上的辅助测量$$h(x)$$  

函数$$A(\eta)=logZ(\eta)$$是 log normalizer.   

$$t(\theta)=\eta^T$$  

![_config.yml]({{ site.baseurl }}/images/10708/image88.png)  

$$A(\eta)=log \int h(x)exp\{\eta^T T(x)\} dx$$   

## 例子   

我们可以看许多概率分布实际上属于该族的(包括伯努利，多项式，高斯，泊松，伽玛)

**Example 1: 多元高斯分布(Multivariate Gaussian Distribution)**  

随机变量为连续向量$$x\in R^K$$的多元高斯分布密度函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image89.png)  

用指数族表示：  

![_config.yml]({{ site.baseurl }}/images/10708/image90.png)   

经过这种转换，我们看到多元高斯分布确实属于指数族。  

请注意，$$K$$维多元高斯分布具有$$K+K^2$$(均值,方差,协方差)维自然参数(和足够的统计量)。但是，由于$$\sum$$必须是对称的并且是半正定矩阵(PSD:positive semidefinite matrix)，因此参数实际上具有较低的自由度。        

**Example 2: 多项式分布(Multinomial Distribution)**  

对于随机变量二进制向量$$x \backsim multinomial(x\mid \pi)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image91.png)   

$$=exp\{\sum_{k=1}^{K-1}x_{k} ln(\frac{\pi_{k}}{1-\sum_{k=1}^{K} \pi_{k}}) + ln(1-\sum_{k=1}^{K} \pi_{k})\}$$  

其中：  

$$
(1- \sum_{k=1}^{K-1} x_{k})ln(1- \sum_{k=1}^{K-1} \pi_{k})=(1- \sum_{k=1}^{K-1} x_{k})ln\pi_{K}\\
=x_{K}ln\pi_{K}
$$

线性约束$$\sum_{k=1}^{K} x_{k}=1$$  

用指数族表示：    

![_config.yml]({{ site.baseurl }}/images/10708/image92.png)   

## 指数族的性质  

**矩生成的属性(Moment generating property)**  

指数族的$$q^{th}$$阶导数给出$$q^{th}$$阶中心矩。   

![_config.yml]({{ site.baseurl }}/images/10708/image93.png)   

因此，我们可以利用这个优势，通过log normalizer $$A(\eta)$$的导数来计算任意指数族分布的中心矩(Central moment)   

注：当充分统计量为叠加向量时，需要考虑偏导数。  

**矩和规范参数(Moment vs. canonical parameters)**   

应用矩生成的属性，可以通过以下方式从自然(规范)参数中得出矩参数$$\mu$$:   

![_config.yml]({{ site.baseurl }}/images/10708/image94.png)   

因此，我们可以反转关系并从矩参数推断出规范参数(1-对-1)  

$$\eta = \psi(\mu)$$  

因此我们可以说，指数族中的分布不仅可以通过$$\eta$$-规范参数化进行参数化，还可以通过$$\mu$$-矩参数化进行参数化。   

## MLE for Exponential Family  

对于独立同分布的数据，我们有对数似然：  

![_config.yml]({{ site.baseurl }}/images/10708/image95.png)   

取导数并将其设置为零：  

![_config.yml]({{ site.baseurl }}/images/10708/image96.png)   

这相当于矩匹配(moment matching), 我们可以用$$\tilde{\eta}_{MLE} = \psi(\tilde{\mu}_{MLE})$$推断规范参数 

## 充分性(Sufficiency)   

从上面我们可以看到$$p(x\mid \theta)$$，$$X$$表示随机变量，$$T(X)$$是统计量(statistic)，假设$$X$$分布取决于$$\theta$$，如果在$$T(X)$$条件中,$$X$$没有关于$$\theta$$的信息,$$T(X)$$对$$\theta$$是充足的(sufficient)，就是说，观测到$$T(X)$$之后，为了推论$$\theta$$，我们可以扔掉$$X$$。 

这可以用贝叶斯论(Bayesian view)和频率论(Frequentist view)来说明。    

贝叶斯论： 

$$\theta$$ 是一个随机变量，如果满足$$\theta \bot X \mid T(X)$$，我们说$$T(X)$$对$$\theta$$是充足的,直观上的概念，即$$T(X)$$包含$$X$$中关于$$\theta$$的所有基本信息。

![_config.yml]({{ site.baseurl }}/images/10708/image97.png)    

$$p(\theta\mid T(x),x)=p(\theta\mid T(x))$$    

(贝叶斯论中,概率是对一个事件的信念或确定性。观测数据被认为是固定的，但是模型参数是随机的(不确定的)，被认为是从某种概率分布中提取的.) 

频率论:  

$$\theta$$是一个label，而不是随机变量，同样在给定$$T(X)$$条件下$$\theta$$和$$X$$相互独立  

![_config.yml]({{ site.baseurl }}/images/10708/image98.png) 

$$p(x \mid T(x),\theta)=p(x \mid T(x))$$ 

(在频率统计中，观测到的数据被认为是随机的；如果你收集到更多的观测数据，它们将根据潜在的分布(即总体分布)而有所不同。然而，模型的参数被认为是固定的。)  

**The Neyman factorization theorem**

Bayesian和frequentist充分性定义都说明了$$p(x\mid \theta)$$的因子分解。为了得到因子分解，我们使用无向图模型形式。

![_config.yml]({{ site.baseurl }}/images/10708/image99.png)  

$$T(X)$$对$$\theta$$是充足的如果：  

$$p(x,T(x),\theta)=\psi_{1}(T(x),\theta)\psi_{2}(x,T(x))$$   

$$p(x\mid \theta)=g(T(x), \theta)h(x,T(x))$$  


**例子**    

![_config.yml]({{ site.baseurl }}/images/10708/image100.png)    

![_config.yml]({{ site.baseurl }}/images/10708/image101.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image102.png)  

# 广义线性模型(Generalized Linear Models)   

有各种条件密度估计任务可以看作是两节点图模型(GMs)(参见补充部分的示例)。许多是广义线性模型的实例。

广义线性模型（GLM）是普通线性回归的灵活概括，它允许线性模型通过链接函数(link function)与响应变量( response variables)相关联，这些响应变量具有除正态分布外的误差分布模型。 例如，线性回归和logistic回归都可以用广义线性模型统一起来。  

![_config.yml]({{ site.baseurl }}/images/10708/image103.png)  

$$X$$和$$Y$$是可观测的，与线性回归和(判别)线性分类模型一样，我们专注于$$X$$和$$Y$$之间的条件关系   

## 共性(Commonality)   

线性回归模型和判别线性分类模型的一个共同特点都是是对$$y$$的条件期望的一种特殊的表示   

我们模拟了$$y$$的期望$$E_{p}(\mathbf{y})=\mu=f(\theta^T\mathbf{x})$$  
     
$$p$$是$$y$$的条件概率分布     

$$f$$是响应函数(response function),$$f(\cdot)$$对于线性回归是恒等函数(identity function)$$\theta^T\mathbf{x} = f(\theta^T\mathbf{x})$$,对于线性分类模型，可以有不同的选择，包括逻辑斯蒂函数(用于logisti 回归)和累积高斯（用于概率回归）。   

我们赋予$$y$$一个特定的条件概率分布，以$$\mu$$为参数，GLIM(generalized linear model)对条件概率分布$$p(y\mid x)$$的形式做出三个假设:    

+ 假定观测到的输入$$\mathbf{x}$$通过线性组合$$\xi = \theta^T\mathbf{x}$$进入模型   

+ 条件平均值$$\mu$$表示为线性组合$$\xi$$的函数$$f(\xi)$$，$$f$$被称为$$\xi = \theta^T\mathbf{x}$$的链接函数

+ 假设观测到的输出$$y$$描述为具有条件均值$$\mu$$的指数族分布     

![_config.yml]({{ site.baseurl }}/images/10708/image108.png)  

指数族分布的选择通常受到$$Y$$的性质的限制。 因此，类别标签由伯努利(Bernoulli)或多项式(multinomial)分布表示，计数由泊松(Poisson)分布表示，区间由指数(exponential)或伽马(gamma)分布表示，等等    

这使"选择响应函数"作为GLIM的主要自由度。例如我们通常希望对响应函数施加约束来反映了对条件期望的约束。例如，对于伯努利分布和多项式分布，条件期望值必须介于0和1之间，这表明我们应该选择一个范围为(0,1)的响应函数。同样对于伽马分布。随机变量为非负，应该选择一个范围为$$(0,\infty)$$的响应函数  

这样的约束仅给出粗略的指导，但是，通常对于任何给定的分布，都有许多可能的响应函数选择。 但是，有一个特殊的响应函数，即规范响应函数(canonical response function)，它是唯一与给定的指数族分布存在相关？，并且具有一些吸引人的数学特性。假设$$\xi = \eta$$,则$$f(\cdot)=\psi^{-1}(\cdot)$$,则$$f$$是canonical response function，注意一旦选择了指数族密度函数则$$\psi(\cdot)$$是确定的。因此，如果我们决定使用规范响应函数，指数族密度函数的选择完全决定了GLIM.  

$$f(\eta) = \psi^{-1}(\eta)=a'(\eta)=E[Y\mid \eta]$$

**Example 1: 线性回归 (Linear Regression)**   

让我们快速回顾一下线性回归    

假设目标变量和输入之间的关系如下：   

$$y_{i} = \theta^T\mathbf{x}_{i} + \epsilon_{i}$$   

$$\epsilon$$是未建模效应(unmodeled effects)或随机噪声的误差项   

现在假设$$\epsilon$$服从高斯分布$$N(0,\sigma)$$   

$$p(y_{i}\mid \mathbf{x}_{i};\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(- \frac{(y_{i}- \theta^T\mathbf{x}_{i})^2}{2\sigma^2})$$

我们可以使用LMS算法（一种梯度上升/下降方法）来估算参数。 

**Example 2: 逻辑斯蒂回归 (Logistic Regression (sigmoid classifier, perceptron, etc.))**   

条件分布为伯努利： 

$$p(y\mid x)=\mu(x)^y(1-\mu(x))^{1-y}$$  

$$\mu$$是 logistic函数  

$$\mu(x) = \frac{1}{e^{-\theta^Tx}}$$    

我们可以像LR一样使用蛮力(force)梯度法。但我们也可以通过观察$$p(y∣x)$$是一个指数族函数-一个广义线性模型来应用一般规律,。

## More examples: 参数化图模型  

**马尔可夫随机场(Markov random fields)**   

![_config.yml]({{ site.baseurl }}/images/10708/image104.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image109.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image110.png)  

**受限波尔兹曼机 (Restricted Boltzmann Machines)**   

![_config.yml]({{ site.baseurl }}/images/10708/image105.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image111.png)  

**条件随机场(Conditional Random Fields)**   

![_config.yml]({{ site.baseurl }}/images/10708/image106.png)  

判别模型：  

![_config.yml]({{ site.baseurl }}/images/10708/image112.png)  


$$X$$被假定为相互依赖的特征


## MLE for GLIMs  


![_config.yml]({{ site.baseurl }}/images/10708/image107.png)  


# Global and local parameter independence








# 补充(Supplementary)  

密度估计可以看作是单节点的图模型，它是普遍GMs的构造块(building block)   

密度估计：在有限次观测$$x_{1},\dots,x_{N}$$的前提下，对随机变量$$x$$的概率分布$$P(x)$$进行建模

对于密度估计，我们有：

+ 最大似然估计
+ 贝叶斯估计 

## 离散分布(Discrete Distributions)  

伯努利分布：$$BER(P)$$      

$$P(x)=p^x(1-p)^{1-x}$$   

多项式分布; $$MULTINOMIAL(N,\theta)$$   

它一般与二项式分布( Bernoulli)类似。当参数中的“1”表示将只有一个试验时它类似于伯努利分布(Bernoulli)。现在我们有$$k$$个可能发生的实例，每个实例都有一个概率$$\theta_{i}$$,$$\sum_{i=1}^k \theta_{i}=1$$   

假设$$n$$是观测向量:  

![_config.yml]({{ site.baseurl }}/images/10708/image80.png)   

式子考左分数涉及排列组合   

## MLE：拉格朗日乘子约束优化   

目标函数：

![_config.yml]({{ site.baseurl }}/images/10708/image81.png)  

约束条件：  

![_config.yml]({{ site.baseurl }}/images/10708/image82.png)   

带拉格朗日乘数的约束成本函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image83.png)   

## 贝叶斯估计  

狄利克雷分布(Dirichlet distribution)也称多元Beta分布(multivariate Beta distribution) 

[Dirichlet distribution](https://meixuanzhang.github.io/NLP-LDA/)   



## 序列贝叶斯更新  

## 层次贝叶斯模型(Hierarchical Bayesian Models)  

$$\theta$$ 是似然$$P(x\mid \theta)$$的参数，$$P(\theta\mid x)=P(x\mid \theta)P(\theta ; \alpha) $$   

$$\alpha$$ 是先验$$P(\theta ; \alpha)$$的参数,Dirichlet distribution分布的参数

我们可以有超参数，等等。  

当超参数的选择对边际似然没有影响时，我们停止；通常使超参数成为常数。

## Dirichlet先验的局限   

Dirichlet先验只能对所有坐标或单个坐标进行强调/偏移；例如，它不能强调两个坐标。不同超参数$$\alpha $$对应的先验如下图所示：  

![_config.yml]({{ site.baseurl }}/images/10708/image84.png)  

## The Logistic Normal Prior    

优点：协方差结构
缺点：非共轭

## 连续分布(Continuous Distributions)  

**均匀概率密度函数:**    

![_config.yml]({{ site.baseurl }}/images/10708/image85.png)  

**正态(高斯)概率密度函数:**  

![_config.yml]({{ site.baseurl }}/images/10708/image86.png)  

**多元高斯**

![_config.yml]({{ site.baseurl }}/images/10708/image87.png)  

## 多元高斯分布的极大似然估计  

## 高斯分布的贝叶斯参数估计 

## 两节点完全观测贝叶斯网络(BNs)   

## 分类(Classification)  

## 线性回归(Linear Regression)

参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



