---
layout: post
title: 译Lecture 5 Parameter Estimation in Fully Observed Bayesian Networks
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍全观测贝叶斯网络(fully observed Bayesian Networks)参数估计 。  

# 介绍(Introduction)  

上一课，我们介绍了两种主要的图模型任务类型：推理(Inference)和学习(Learning)，并且我们主要讨论了推理(Inference)。 在本讲座中，我们将开始介绍学习(Learning)任务，并探索可用于全观测贝叶斯网络学习任务的技术。    

# 学习图模型(Learning Graphical Models)   

在开始之前，最好先了解一下什么是学习(Learning)以及实现的目标是什么。

## 目标(The goal)  

目标：给定一组独立样本(随机变量的赋值),找到最佳（或最有可能）贝叶斯网络(DAG和CPD)。 

如下图所示，我们得到了一组独立样本（二进制随机变量的赋值）。假设它是一个DAG，我们将学习节点之间的有向链接(因果关系),这个过程称为结构学习(Structural Learning)。学习条件概率是另一个被称为参数学习(Parameter learning)的任务。


![_config.yml]({{ site.baseurl }}/images/10708/image79.png)   

## 总览(Overview)

如下所示，我们对以下几个学习场景感兴趣：  

1、全观测的图模型(Completely observed GMs)   

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)  

2、部分或未观察到的图模型(Partially or unobserved GMs)  

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)（一个开放的研究课题）   

这里还列出了一些有用的估算原理：  

最大似然估计(MLE):Maximal likelihood estimation
贝叶斯估计:Bayesian estimation
最大条件似然: Maximal conditional likelihood
最大“边际”：Maximal “Margin”
最大熵：Maximum entropy  

我们使用learning作为估计参数这一过程的名字，在某些情况下，使用数据来估计网络的拓扑结构。

# 参数学习(Parameter Learning)  

在本节中，我们将介绍参数学习( Parameter Learning)和一些有用的相关模型。

在进行参数学习时，我们假设图模型$$G$$本身是已知的并且是固定的，$$G$$可能来自专家设计或迭代结构学习的中间结果。  

参数学习的目标是从$$N$$个独立的数据集中估计参数,具有相同分布的（i.i.d.）训练样本$$D=x_{1},...x_{N}$$  

通常，每个训练样本$$\mathbf{x}_{n}=(x_{n,1},..,x_{n,M})$$是一个$$M$$维向量，每一维是一个节点。根据模型，如果模型是完全可观察的，那么$$\mathbf{x}_{n}$$中的元素都是已知的（没有丢失的值，没有隐藏的变量）,如果模型是部分可观测的,那么$$\mathbf{x}_{n}$$中的元素部分可知($$\exists_{i},x_{n,1}$$is not observed)  

在本课程中，我们主要考虑学习在给定结构下完全可观测的$$BN$$的参数。   

# 指数族分布(Exponential Family Distributions)  

有各种密度估计任务可以看作是单节点的GMs（参见补充部分的示例），它们实际上是指数族分布的实例。  

指数族分布是普遍GMs的构造块，具有很好的性质，可以方便地进行MLE和贝叶斯估计。  

## 定义(Definition)  

# 补充(Supplementary)  

密度估计可以看作是单节点的图模型，它是普遍GMs的构造块(building block)   

对于密度估计，我们有：

+ 最大似然估计
+ 贝叶斯估计 

## 离散分布(Discrete Distributions)  

伯努利分布：$$BER(P)$$      

$$P(x)=p^x(1-p)^{1-x}$$   

多项式分布; $$MULTINOMIAL(N,\theta)$$   

它一般与二项式分布( Bernoulli)类似。当参数中的“1”表示将只有一个试验时它类似于伯努利分布(Bernoulli)。现在我们有$$k$$个可能发生的实例，每个实例都有一个概率$$\theta_{i}$$,$$\sum_{i=1}^k \theta_{i}=1$$   

假设$$n$$是观测向量:  

![_config.yml]({{ site.baseurl }}/images/10708/image80.png)   

式子考左分数涉及排列组合   

## MLE：拉格朗日乘子约束优化   

目标函数：

![_config.yml]({{ site.baseurl }}/images/10708/image81.png)  

约束条件：  

![_config.yml]({{ site.baseurl }}/images/10708/image82.png)   

带拉格朗日乘数的约束成本函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image83.png)   

## 贝叶斯估计  

狄利克雷分布(Dirichlet distribution)也称多元Beta分布(multivariate Beta distribution) 

[Dirichlet distribution](https://meixuanzhang.github.io/NLP-LDA/)   



## 序列贝叶斯更新  

## 层次贝叶斯模型(Hierarchical Bayesian Models)  

$$\theta$$ 是似然$$P(x\mid \theta)$$的参数，$$P(\theta\mid x)=P(x\mid \theta)P(\theta ; \alpha) $$   

$$\alpha$$ 是先验$$P(\theta ; \alpha)$$的参数(Dirichlet distribution)分布的参数

我们可以有超参数，等等。  

当超参数的选择对边际似然没有影响时，我们停止；通常使超参数成为常数。

## Dirichlet先验的局限  

## The Logistic Normal Prior   

## 连续分布(Continuous Distributions)  

## 多元高斯分布的极大似然估计  

## 高斯分布的贝叶斯参数估计 

## 两节点完全观测贝叶斯网络(BNs)   

## 分类(Classification)  

## 线性回归(Linear Regression)

参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



