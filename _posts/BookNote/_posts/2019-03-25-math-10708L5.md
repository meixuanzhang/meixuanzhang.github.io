---
layout: post
title: 译Lecture 5 Parameter Estimation in Fully Observed Bayesian Networks
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍全观测贝叶斯网络(fully observed Bayesian Networks)参数估计 。  

# 介绍(Introduction)  

上一课，我们介绍了两种主要的图模型任务类型：推理(Inference)和学习(Learning)，并且我们主要讨论了推理(Inference)。 在本讲座中，我们将开始介绍学习(Learning)任务，并探索可用于全观测贝叶斯网络学习任务的技术。    

# 学习图模型(Learning Graphical Models)   

在开始之前，最好先了解一下什么是学习(Learning)以及实现的目标是什么。

## 目标(The goal)  

目标：给定一组独立样本(随机变量的赋值),找到最佳（或最有可能）贝叶斯网络(DAG和CPD)。 

如下图所示，我们得到了一组独立样本（二进制随机变量的赋值）。假设它是一个DAG，我们将学习节点之间的有向链接(因果关系),这个过程称为结构学习(Structural Learning)。学习条件概率是另一个被称为参数学习(Parameter learning)的任务。


![_config.yml]({{ site.baseurl }}/images/10708/image79.png)   

## 总览(Overview)

如下所示，我们对以下几个学习场景感兴趣：  

1、全观测的图模型(Completely observed GMs)   

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)  

2、部分或未观察到的图模型(Partially or unobserved GMs)  

+ 1.1 有向(directed)  
+ 1.2 无向(undirected)（一个开放的研究课题）   

这里还列出了一些有用的估算原理：  

最大似然估计(MLE):Maximal likelihood estimation
贝叶斯估计:Bayesian estimation
最大条件似然: Maximal conditional likelihood
最大“边际”：Maximal “Margin”
最大熵：Maximum entropy  

我们使用learning作为估计参数这一过程的名字，在某些情况下，使用数据来估计网络的拓扑结构。

# 参数学习(Parameter Learning)  

在本节中，我们将介绍参数学习( Parameter Learning)和一些有用的相关模型。

在进行参数学习时，我们假设图模型$$G$$本身是已知的并且是固定的，$$G$$可能来自专家设计或迭代结构学习的中间结果。  

参数学习的目标是从$$N$$个独立的数据集中估计参数,具有相同分布的（i.i.d.）训练样本$$D=x_{1},...x_{N}$$  

通常，每个训练样本$$\mathbf{x}_{n}=(x_{n,1},..,x_{n,M})$$是一个$$M$$维向量，每一维是一个节点。根据模型，如果模型是完全可观察的，那么$$\mathbf{x}_{n}$$中的元素都是已知的（没有丢失的值，没有隐藏的变量）,如果模型是部分可观测的,那么$$\mathbf{x}_{n}$$中的元素部分可知($$\exists_{i},x_{n,1}$$is not observed)  

在本课程中，我们主要考虑学习在给定结构下完全可观测的$$BN$$的参数。   

# 分布族  

分布族(families of distributions),是一组分布集合，它们共享相同的参数形式，并且仅在选择具体参数时才不同。   

通常,一旦选择了网络的全局结构和局部结构，意味为CPDs的具体选择定义了可以通过不同的参数来获得所有分布的一个族。  

# 指数族分布(Exponential Family Distributions)  

有各种密度估计任务可以看作是单节点的GMs(参见补充部分的示例)，它们实际上是指数族分布的实例。  

指数族分布是普遍GMs的构造块，具有很好的性质，可以方便地进行MLE和贝叶斯估计。  

## 定义(Definition)    

对于数值随机变量$$x$$:   

$$x$$上的指数族由以下四个成分指定：  

+ 从$$x$$的赋值到$$R^K$$的一个充分统计量函数(sufficient statistic function)$$T(x)$$    

+ 参数空间是合法参数(legal parameter)的一个凸集(convex set)$$\theta \subseteq R^M$$构成的   

+ 从$$R^M$$到$$R^K$$的一个自然参数函数(natural parameter function)$$t$$   

+ $$x$$上的辅助测量$$h(x)$$  

函数$$A(\eta)=logZ(\eta)$$是 log normalizer.   

$$t(\theta)=\eta^T$$  

![_config.yml]({{ site.baseurl }}/images/10708/image88.png)  

## 例子   

我们可以看许多概率分布实际上属于该族的(包括伯努利，多项式，高斯，泊松，伽玛)

**Example 1: 多元高斯分布(Multivariate Gaussian Distribution)**  

随机变量为连续向量$$x\in R^K$$的多元高斯分布密度函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image89.png)  

用指数族表示：  

![_config.yml]({{ site.baseurl }}/images/10708/image90.png)   

经过这种转换，我们看到多元高斯分布确实属于指数族。  

请注意，$$K$$维多元高斯分布具有$$K+K^2$$(均值,方差,协方差)维自然参数(和足够的统计量)。但是，由于$$\sum$$必须是对称的并且是半正定矩阵(PSD:positive semidefinite matrix)，因此参数实际上具有较低的自由度。        

**Example 2: 多项式分布(Multinomial Distribution)**  

对于随机变量二进制向量$$x \backsim multinomial(x\mid \pi)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image91.png)   

$$=exp\{\sum_{k=1}^{K-1}x_{k} ln(\frac{\pi_{k}}{1-\sum_{k=1}^{K} \pi_{k}}) + ln(1-\sum_{k=1}^{K} \pi_{k})\}$$  

其中：  

$$
(1- \sum_{k=1}^{K-1} x_{k})ln(1- \sum_{k=1}^{K-1} \pi_{k})=(1- \sum_{k=1}^{K-1} x_{k})ln\pi_{K}\\
=x_{K}ln\pi_{K}
$$

线性约束$$\sum_{k=1}^{K} x_{k}=1$$  

用指数族表示：    

![_config.yml]({{ site.baseurl }}/images/10708/image92.png)   

## 指数族的性质  

**矩生成的属性(Moment generating property)**  

指数族的$$q^{th}$$阶导数给出$$q^{th}$$阶中心矩。   

![_config.yml]({{ site.baseurl }}/images/10708/image93.png)   

因此，我们可以利用这个优势，通过log normalizer $$A(\eta)$$的导数来计算任意指数族分布的中心矩(Central moment)   

注：当充分统计量为叠加向量时，需要考虑偏导数。  

**矩和规范参数(Moment vs. canonical parameters)**   

应用矩生成的属性，可以通过以下方式从自然(规范)参数中得出矩参数$$\mu$$:   

![_config.yml]({{ site.baseurl }}/images/10708/image94.png)   

因此，我们可以反转关系并从矩参数推断出规范参数(1-对-1)  

$$\eta = \psi(\mu)$$  

因此我们可以说，指数族中的分布不仅可以通过$$\eta$$-规范参数化进行参数化，还可以通过$$\mu$$-矩参数化进行参数化。   

## MLE for Exponential Family  

对于独立同分布的数据，我们有对数似然：  

![_config.yml]({{ site.baseurl }}/images/10708/image95.png)   


# 补充(Supplementary)  

密度估计可以看作是单节点的图模型，它是普遍GMs的构造块(building block)   

密度估计：在有限次观测$$x_{1},\dots,x_{N}$$的前提下，对随机变量$$x$$的概率分布$$P(x)$$进行建模

对于密度估计，我们有：

+ 最大似然估计
+ 贝叶斯估计 

## 离散分布(Discrete Distributions)  

伯努利分布：$$BER(P)$$      

$$P(x)=p^x(1-p)^{1-x}$$   

多项式分布; $$MULTINOMIAL(N,\theta)$$   

它一般与二项式分布( Bernoulli)类似。当参数中的“1”表示将只有一个试验时它类似于伯努利分布(Bernoulli)。现在我们有$$k$$个可能发生的实例，每个实例都有一个概率$$\theta_{i}$$,$$\sum_{i=1}^k \theta_{i}=1$$   

假设$$n$$是观测向量:  

![_config.yml]({{ site.baseurl }}/images/10708/image80.png)   

式子考左分数涉及排列组合   

## MLE：拉格朗日乘子约束优化   

目标函数：

![_config.yml]({{ site.baseurl }}/images/10708/image81.png)  

约束条件：  

![_config.yml]({{ site.baseurl }}/images/10708/image82.png)   

带拉格朗日乘数的约束成本函数：  

![_config.yml]({{ site.baseurl }}/images/10708/image83.png)   

## 贝叶斯估计  

狄利克雷分布(Dirichlet distribution)也称多元Beta分布(multivariate Beta distribution) 

[Dirichlet distribution](https://meixuanzhang.github.io/NLP-LDA/)   



## 序列贝叶斯更新  

## 层次贝叶斯模型(Hierarchical Bayesian Models)  

$$\theta$$ 是似然$$P(x\mid \theta)$$的参数，$$P(\theta\mid x)=P(x\mid \theta)P(\theta ; \alpha) $$   

$$\alpha$$ 是先验$$P(\theta ; \alpha)$$的参数,Dirichlet distribution分布的参数

我们可以有超参数，等等。  

当超参数的选择对边际似然没有影响时，我们停止；通常使超参数成为常数。

## Dirichlet先验的局限   

Dirichlet先验只能对所有坐标或单个坐标进行强调/偏移；例如，它不能强调两个坐标。不同超参数$$\alpha $$对应的先验如下图所示：  

![_config.yml]({{ site.baseurl }}/images/10708/image84.png)  

## The Logistic Normal Prior    

优点：协方差结构
缺点：非共轭

## 连续分布(Continuous Distributions)  

**均匀概率密度函数:**    

![_config.yml]({{ site.baseurl }}/images/10708/image85.png)  

**正态(高斯)概率密度函数:**  

![_config.yml]({{ site.baseurl }}/images/10708/image86.png)  

**多元高斯**

![_config.yml]({{ site.baseurl }}/images/10708/image87.png)  

## 多元高斯分布的极大似然估计  

## 高斯分布的贝叶斯参数估计 

## 两节点完全观测贝叶斯网络(BNs)   

## 分类(Classification)  

## 线性回归(Linear Regression)

参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



