---
layout: post
title: LDA(Latent Dirichlet Allocation)
date:   2019-01-01
categories: 
---

# 基础概念

**Beta(贝塔)函数和Gamma(伽玛)函数**  

欧拉积分是由瑞士数学家莱昂哈德·欧拉整理得出的两类特殊的含参变量的积分。由欧拉积分所定义的函数分别称为Beta函数和Gamma函数。——百度百科  

Gamma(伽玛)函数：  

$$\Gamma(s)=\int_{0}^{+\infty} x^{s-1}e^{-x}dx,(s>0)$$

Beta(贝塔)函数:  

$$B(p,q)=\int_{0}^{1} x^{p-1}(1-x)^{q-1}dx,(p>0,q>0)$$  

两者关系：  

$$B(p,q) = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$$

**Beta分布**

(二元)Beta分布的概率密度函数：  

$$
f(x;\alpha,\beta)= \frac{x^{\alpha-1}(1-x)^{\beta-1}} {\int_{0}^{1}\mu^{\alpha-1}(1-\mu)^{\beta-1}d\mu}\\
=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\\
=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}(1-x)^{\beta-1}  
$$    

注意：$$\alpha,\beta$$是参数，x是变量

**Dirichlet分布(多元Beta分布)**    

$$f(x_{1},...x_{K-1};\alpha_{1},...\alpha_{K})=\frac{1}{B(\mathbf{α})}\prod_{i=1}^{K}x_{i}^{\alpha_{i}-1}$$

自由度是K-1  

$$\sum_{i=1}^K x_{i}=1,x_{K}=1-\sum_{i=1}^{K-1}$$   

$$
B(\mathbf{α})=\frac{\prod_{i=1}^{K}\Gamma(\alpha_{i})} {\Gamma(\sum_{i=1}^K \alpha_{i})},\mathbf{α}=(\alpha_{1},...\alpha_{K})
$$

注意：$$\mathbf{α}$$是参数，$$x_{1},...x_{K-1}$$是变量  
 
# LDA模型
**希望解决的问题**   

对文章进行归类，分为K类，获得每类文章的词汇词频分布，确定每类文章关键词。

**Notation**  

K：主题个数（类别数），常量  
V：总词汇量，常量  
M：文章总数，常量  
$$N_{d=1..M}:$$每篇文章的词数，常量  
$$N:$$所有文章词数和$$N=\sum_{d=1}^M N_{d}$$，常量  
$$\theta_{i}:$$第i篇文章的类别分布，K维向量，向量元素和为1。    
例：假设K=3，类别是(娱乐、体育、科技)，文章i类别分布是(0.25,0.25,0.5),元素和为1。每篇文章会有自己类别分布。    
$$\varphi_{k}:$$第k类文章词汇词频分布，V维向量，向量元素和为1。  
$$Z$$:所有文章中每个字(词)类别，N 维向量，向量元素取值范围[1,K]。
$$W$$:所有文章的字(词)，N 维向量，向量元素取值范围[1，V]。
# 模型推导过程

目前已知每篇文章由哪些词构成。现假设每篇文章词是由以下方式生成：  

1、假设每篇文章的类别分布(向量)，服从Dirichlet分布,从Dir分布中获得第i篇类别分布向量$$\theta_{i}$$   

$$\theta_{i}\sim Dir(\mathbf{α}),i\in{1,...,M}$$    

$$\theta_{i}$$是Dir分布的变量，$$\mathbf{α}$$是参数,每一篇文章会有自己参数  

实际上我们不能从分布中获得文章的类别分布，这里只是假设能获得

2、假设不同类别文章词汇词频分布(向量)，服从Dirichlet分布，从Dir分布中获得第k类文章词汇词频分布向量$$\varphi_{k}$$       

$$\varphi_{k} \sim Dir(\mathbf{β}),k\in{1,...,K}$$    

$$\varphi_{k}$$是Dir分布的变量，$$\mathbf{β}$$是参数,   每类文章会有自己参数   

实际上我们不能从分布中获得每类文章的词汇词频分布，这里只是假设能获得   

3、生成第i篇文章，第j个位置的词，$$i\in{1,...,M},j\in{1,...,N_{i}}$$    

+ 按第i篇文章的类别分布，随机选择一个类别作为文章i的类别，标记为$$z_{ij}$$   

+ 按$$z_{ij}$$类词汇词频分布，随机选择一个字作为第i篇文章，第j个位置的词，标记为$$w_{ij}$$

+ 不断循环3,生成每一篇文章的词

**由于现在已经知道每篇文章由哪些词构成，我们希望获得每篇文章每个词的类别标记$$z_{ij}$$，通过计算文章词的类别标记分布，作为此文章类别分布，选择概率最大的类别作为此文章的分类，同时通过计算同类文章词汇词频，作为此类文章的词汇词频分布，所以我们要求解的问题是文章中每个词的类别是什么**  

# 方法一、EM  

1、明确隐变量，写出完全数据的对数似然函数  

$$
logP(W;\mathbf{α},\mathbf{β})=\int_{\theta}\int_{\varphi}\sum_{Z}P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})d_{\theta}d_{\varphi}
$$

令$$H=(Z,\theta,\varphi),P(H \mid W)\approx Q(H)$$，则:  

$$
logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}P(W,H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}Q(H)\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H)}\\
$$

令$$C(H)=\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H)}$$则：  

$$logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}Q(H)C(H)=logE_{H}(C(H))\\
\ge E_{H}log(C(H)) ,jensen \ inequality
$$

损失函数是：  

$$
L= E_{H}log(C(H))\\
=\sum_{H}Q(H)lpg\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H)}\\
=\sum_{H}Q(H)log\frac{P(W, H;\mathbf{α}}{Q(H)}\\
$$  

上式中$$P(H \mid W)\approx Q(H)$$,使用$$D_{KL}$$相对熵来衡量分布相似发现：  

$$
D_{KL}(Q(H) \parallel P(H\mid W))=\sum_{H}Q(H)\frac{Q(H)}{ P(H\mid W)}\\
=\sum_{H}Q(H)[log\frac{Q(H)P(W)}{ P(H,W)}]\\
=\sum_{H}Q(H)[log\frac{Q(H)}{ P(H,W)}+logP(W)]\\
=\sum_{H}Q(H)[log\frac{Q(H)}{ P(H,W)}]+logP(W)\\
$$

$$
logP(W)=D_{KL}(Q(H) \parallel P(H\mid W))-\sum_{H}Q(H)log\frac{Q(H)}{P(H,W)}\\
=D_{KL}(Q(H) \parallel P(H\mid W))+\sum_{H}Q(H)log(\frac{Q(H)}{P(H,W)})^{-1}\\
=D_{KL}(Q(H) \parallel P(H\mid W))+\sum_{H}Q(H)log(\frac{P(H,W)}{Q(H)})
=D_{KL}(Q(H) \parallel P(H\mid W))+L
$$

因为$$D_{KL}(Q(H) \parallel P(H\mid W))/ge 0$$,所以$$L$$是$$logP(W)$$下限，对于事实$$P(W)$$是不变的，因此增大$$L$$，同时会减少$$D_{KL},P(H \mid W)$$和$$Q(H)$$越相似。









# 方法二、Gibbs
