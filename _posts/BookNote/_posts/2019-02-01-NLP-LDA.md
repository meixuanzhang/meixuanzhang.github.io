---
layout: post
title: LDA(Latent Dirichlet Allocation)
date:   2019-01-01
categories: 
---

# 基础概念

**Beta(贝塔)函数和Gamma(伽玛)函数**  

欧拉积分是由瑞士数学家莱昂哈德·欧拉整理得出的两类特殊的含参变量的积分。由欧拉积分所定义的函数分别称为Beta函数和Gamma函数。——百度百科  

Gamma(伽玛)函数：  

$$\Gamma(s)=\int_{0}^{+\infty} x^{s-1}e^{-x}dx,(s>0)$$

Beta(贝塔)函数:  

$$B(p,q)=\int_{0}^{1} x^{p-1}(1-x)^{q-1}dx,(p>0,q>0)$$  

两者关系：  

$$B(p,q) = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$$

**Beta分布**

(二元)Beta分布的概率密度函数：  

$$
f(x;\alpha,\beta)= \frac{x^{\alpha-1}(1-x)^{\beta-1}} {\int_{0}^{1}\mu^{\alpha-1}(1-\mu)^{\beta-1}d\mu}\\
=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\\
=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}(1-x)^{\beta-1}  
$$    

注意：$$\alpha,\beta$$是参数，x是变量

**Dirichlet分布(多元Beta分布)**    

$$f(x_{1},...x_{K-1};\alpha_{1},...\alpha_{K})=\frac{1}{B(\mathbf{α})}\prod_{k=1}^{K}x_{k}^{\alpha_{k}-1}$$

自由度是K-1  

$$\sum_{i=1}^K x_{i}=1,x_{K}=1-\sum_{k=1}^{K-1}$$   

$$
B(\mathbf{α})=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})} {\Gamma(\sum_{k=1}^K \alpha_{k})},\mathbf{α}=(\alpha_{1},...\alpha_{K})
$$

注意：$$\mathbf{α}$$是参数，$$x_{1},...x_{K-1}$$是变量  

**指数族分布**  

如果一个分布的概率密度函数可以写成以下形式，则称这一类分布为exponential family distributions：   

$$
f_{x}(x;\theta)=h(x)exp(\eta (\theta) \centerdot T(x)-A(\theta))\\
f_{x}(x;\theta)=exp(\eta (\theta) \centerdot T(x)-A(\theta)+B(x))\\
f_{x}(x;\theta)=h(x)g(x)exp(\eta (\theta) \centerdot T(x))\\
$$

Dirichlet distribution是指数族分布，且具有以下性质:   

$$
E(T(x_{k}))=\frac{dA(\alpha)}{d\alpha_{k}}
$$

参考：   
[Exponential family-wiki](https://en.wikipedia.org/wiki/Exponential_family)    
[Exponential Family Distributions](http://www.johnwinn.org/Publications/thesis/Winn03_appendices.pdf)    

# LDA模型
**希望解决的问题**   

对文章进行归类，分为K类，获得每类文章的词汇词频分布，确定每类文章关键词。

**Notation**  

K：主题个数（类别数），常量  
V：总词汇量，常量  
M：文章总数，常量  
$$N_{d=1..M}:$$每篇文章的词数，常量  
$$N:$$所有文章词数和$$N=\sum_{d=1}^M N_{d}$$，常量  
$$\theta_{i}:$$第i篇文章的类别分布，K维向量，向量元素和为1。    
例：假设K=3，类别是(娱乐、体育、科技)，文章i类别分布是(0.25,0.25,0.5),元素和为1。每篇文章会有自己类别分布。    
$$\varphi_{k}:$$第k类文章词汇词频分布，V维向量，向量元素和为1。  
$$Z$$:所有文章中每个字(词)类别，N 维向量，向量元素取值范围[1,K]。  
$$W$$:所有文章的字(词)，N 维向量，向量元素取值范围[1，V]。  

# 模型推导过程

目前已知每篇文章由哪些词构成。现假设每篇文章词是由以下方式生成：  

1、假设每篇文章的类别分布(向量)，服从Dirichlet分布,从Dir分布中获得第i篇类别分布向量$$\theta_{d}$$   

$$\theta_{d}\sim Dir(\mathbf{α}),d\in{1,...,M}$$    

$$\theta_{d}$$是Dir分布的K维变量，$$\mathbf{α}$$是参数

实际上我们不能从分布中获得文章的类别分布，这里只是假设能获得

2、假设不同类别文章词汇词频分布(向量)，服从Dirichlet分布，从Dir分布中获得第k类文章词汇词频分布向量$$\varphi_{k}$$       

$$\varphi_{k} \sim Dir(\mathbf{β}),k\in{1,...,K}$$    

$$\varphi_{k}$$是Dir分布的V维变量，$$\mathbf{β}$$是参数

实际上我们不能从分布中获得每类文章的词汇词频分布，这里只是假设能获得   

3、生成第d篇文章，第j个位置的词，$$d\in{1,...,M},j\in{1,...,N_{d}}$$    

+ 按第d篇文章的类别分布，随机选择一个类别作为文章 d 第j个位置的词的类别，标记为$$z_{dj}\sim Multinomial(\theta_{d})$$   

+ 按$$z_{dj}$$类词汇词频分布，随机选择一个字作为第d篇文章，第j个位置的词，标记为$$w_{dj}\sim Multinomial(\varphi_{z_{dj}})$$

+ 不断循环3,生成每一篇文章的词

**目前已知每篇文章由哪些词构成，我们希望获得每篇文章类别分布$$\theta_{d}$$，以及每类文章的词频分布$$\varphi_{k}$$**  

# 方法一、EM  

1、明确隐变量，写出完全数据的对数似然函数  


$$
W=(W_1,W_2...W_M)\\  
Z=(Z_1,Z_2...Z_M)\\  
\theta=(\theta_1,\theta_2...\theta_M) \\ 
\mathbf{α}=(α_1,α_2...α_K)\\
\varphi=(\varphi_1,\varphi_2...\varphi_K)\\
\mathbf{β}=(β_1,β_2...β_V)\\
$$

$$W,Z,\theta$$下标表示第m篇文章的所有字，所有字类别     
$$\varphi$$下标表示第k类文章的字频数分布    
$$\mathbf{β},\mathbf{α}$$是参数    

$$
logP(W;\mathbf{α},\mathbf{β})=\int_{\theta}\int_{\varphi}\sum_{Z}P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})d_{\theta}d_{\varphi}
$$

令$$H=(Z,\theta,\varphi)$$

$$
logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}P(W,H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}P(W \mid H;\mathbf{α^{(i)}},\mathbf{β^{(i)}})P(H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}P(H \mid W;\mathbf{α^{(i)}},\mathbf{β^{(i)}})\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{P(H \mid W;\mathbf{α^{(i)}},\mathbf{β^{(i)}})}\\
$$

令$$C(H)=\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{P(H \mid W;\mathbf{α},\mathbf{β})}$$则：  

$$logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}P(W \mid H;\mathbf{α},\mathbf{β})C(H)=logE_{H_{(p)}}(C(H))\\
\ge E_{H_{(p)}}log(C(H)) ,jensen \ inequality
$$

$$L=\sum\limits_{H}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta)$$

令P(H \mid W;\mathbf{α},\mathbf{β})\approx Q(H;\gamma,\pi,\phi)$$，根据平均场理论：    

$$Q(H;\gamma,\pi,\phi)=Q(\theta;\gamma)Q(Z;\pi)Q(\varphi;\phi)$$

损失函数是：  

$$
L= E_{H}log(C(H))\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W, H;\mathbf{α})}{Q(H;\gamma,\pi,\phi)}\\
$$  

上式中$$P(H \mid W)\approx Q(H)$$,使用$$D_{KL}$$相对熵来衡量分布相似发现：  

$$
D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))=\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{Q(H;\gamma,\pi,\phi)}{ P(H\mid W;\mathbf{α},\mathbf{β})}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)P(W;\mathbf{α},\mathbf{β})}{ P(H,W;\mathbf{α},\mathbf{β})}]\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)}{ P(H,W;\mathbf{α},\mathbf{β})}+logP(W;\mathbf{α},\mathbf{β})]\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)}{ P(H,W;\mathbf{α},\mathbf{β})}]+logP(W;\mathbf{α},\mathbf{β})\\
$$

$$
logP(W;\mathbf{α},\mathbf{β})=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))-\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{Q(H;\gamma,\pi,\phi)}{P(H,W;\mathbf{α},\mathbf{β})}\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi,\phi)log(\frac{Q(H;\gamma,\pi,\phi)}{P(H,W;\mathbf{α},\mathbf{β})})^{-1}\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi,\phi)log(\frac{P(H,W;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)})\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+L
$$

因为$$D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))\ge 0$$,所以$$L$$是$$logP(W)$$下限，对于事实$$P(W)$$是不变的，因此增大$$L$$，同时会减少$$D_{KL},P(H \mid W)$$和$$Q(H)$$越相似。

$$
L= \sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W,H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log P(W,H;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log [P(W\mid Z,\varphi)P(Z\mid theta)P(\theta;\mathbf{α})P(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[logP(W\mid Z,\varphi)+logP(Z\mid  \theta)+logP(\theta;\mathbf{α})+logP(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\ 
=E_{H}[log P(W\mid Z,\varphi)]+E_{H}[logP(Z\mid \theta)]+E_{H}[logP(\theta;\mathbf{α})]+E_{H}[logP(\varphi;\mathbf{β})]\\
-E_{H}[logQ(Z;\pi)]-E_{H}[logQ(\theta;\gamma)]-E_{H}[logQ(\varphi;\phi)]
$$


$$
E_{H}[logP(\theta;\mathbf{α})]=E_{Q}[logP(\theta_1,\theta_2...\theta_M;\mathbf{α})]\\
=E_{H}[log(P(\theta_1;\mathbf{α})P(\theta_2;\mathbf{α})...P(\theta_M;\mathbf{α}))]\\
=E_{H}[logP(\theta_1;\mathbf{α})+logP(\theta_2;\mathbf{α})+...+logP(\theta_M;\mathbf{α})]\\
$$

$$
E_{H}[logP(\varphi;\mathbf{β})]=E_{Q}[logP(\varphi_1,\varphi_2...\varphi_K;\mathbf{β})]\\
=E_{H}[log(P(\varphi_1;\mathbf{β})P(\varphi_2;\mathbf{β})...P(\varphi_K;\mathbf{β}))]\\
=E_{H}[logP(\varphi_1;\mathbf{β})+logP(\varphi_2;\mathbf{β})+...+logP(\varphi_K;\mathbf{β})]\\
$$ 


$$
E_{H}[logP(Z\mid \theta)]=E_{Q}[logP(Z_1,Z_2...Z_M\mid \theta_1,\theta_2...\theta_M)]\\
=E_{H}[log(P(Z_1\mid \theta_1)P(,Z_2\mid \theta_2)... P(Z^M\mid \theta_M))]\\
=E_{H}[logP(Z_1\mid \theta_1)+logP(Z_2\mid \theta_2)+...+logP(Z_M\mid \theta_M)]\\

$$   


$$
E_{H}[log P(W\mid Z,\varphi)]=E_{Q}[log P(W_1,W_2...W_M;Z_1,Z_2...Z_M\mid \varphi_1,\varphi_2...\varphi_K)]\\
=E_{H}[log(P(W_1\mid \varphi_{Z_1})P(W_2\mid \varphi_{Z_2})...P(W_M\mid \varphi_{Z_M}))]\\
=E_{H}[logP(W_1\mid \varphi_{Z_1})+logP(W^2\mid \varphi_{Z_2})+...+logP(W_M\mid \varphi_{Z_M})]\\
$$  

$$\varphi_{Z_M}:Z_M$$相应类别下，对应的词汇词频分布    
 
同理：  

$$
E_{H}[logQ(\theta;\gamma)]=E_{H}[logQ(\theta_1;\gamma_1)+logQ(\theta_2;\gamma_2)+...+logQ(\theta_M;\gamma_M)]\\
E_{H}[logQ(\varphi;\phi)]=E_{H}[logQ(\varphi_1;\phi_1)+logQ(\varphi_2;\phi_2)+...+logQ(\varphi_K;\phi_K)]\\
E_{H}[logQ(Z;\pi)]=E_{H}[logQ(Z_1;\pi_1)+logQ(,Z_2;\pi_2)+...+logQ(Z_M;\pi_M)]\\

$$   

只关注第d篇文章则：  

$$
E_{H}[logP(\theta_d;\mathbf{α})]=E_{H}[log(\frac {\Gamma(\sum_{k=1}^K \alpha_{k})}  {\prod_{k=1}^{K}\Gamma(\alpha_{k})}   \prod_{k=1}^{K}\theta_{d,k}^{\alpha_{k}-1})]\\

=E_{H}[log\Gamma(\sum_{k=1}^K \alpha_{k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{k}) + \sum_{k=1}^{K}(\alpha_{k}-1)log\theta_{d,k}]\\
=log\Gamma(\sum_{k=1}^K \alpha_{k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{k}) + \sum_{k=1}^{K}(\alpha_{k}-1)E_{H}[log\theta_{d,k}]\\

$$

式子中$$H \sim Q(H;\gamma,\pi,\phi),E_{H}[log\theta_{d,k}]$$中$$\theta_{d}\sim Dirichlet(\gamma_{d})$$,根据指数族分布性质：  

$$
E_{H}[log\theta_{d,k}]=\frac{dA(\gamma_{d})}{d\gamma_{d,k}}\\
=\frac{d}{d\gamma_{d,k}}\sum_{k=1}^{K}log\Gamma(\gamma_{d,k})-log\Gamma(\sum_{k=1}^K \gamma_{d,k}) \\
=\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k})
$$

$$\Psi():$$ the first derivative of the log Gamma function    

$$
E_{H}[logP(\theta_d;\mathbf{α})]=log\Gamma(\sum_{k=1}^K \alpha_{k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{k}) + \sum_{k=1}^{K}(\alpha_{k}-1)(\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k}))
$$

同理：  

$$
E_{H}[logQ(\theta_d;\gamma_d)]=log\Gamma(\sum_{k=1}^K \gamma_{d,k}) -\sum_{k=1}^{K}log\Gamma(\gamma_{d,k}) + \sum_{k=1}^{K}(\gamma_{d,k}-1)(\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k}))
$$   

$$
E_{H}[logP(Z_d\mid \theta_d)]=E_{H}[\sum_{t=1}^{N_{d}}logP(z_{d,t};\theta_d)]\\
=E_{H}[\sum_{t=1}^{N_{d}}\sum_{k=1}^Klog\theta_{d,k}[z_{d,t}==k]]\\
=\sum_{t=1}^{N_{d}}\sum_{k=1}^K  E_{H}[log\theta_{d,k}]E_{H}[[z_{d,t}==k]]\\
=\sum_{t=1}^{N_{d}}\sum_{k=1}^K (\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k}))\pi_{d,t,k} \\
$$

$$\pi_{d,t,k} $$第d篇第t个字是k类的概率   
同理：  

$$
E_{H}[logQ(Z_d;\pi_d)]=\sum_{t=1}^{N_{d}}\sum_{k=1}^K  E_{H}[log\pi_{d,t,k}]E_{H}[[z_{d,t}==k]]\\

\sum_{t=1}^{N_{d}}\sum_{k=1}^K (log\pi_{d,t,k})\pi_{d,t,k} \\
$$


$$
E_{H}[logP(W_d\mid \varphi_{Z_d})]=E_{H}[\sum_{t=1}^N logP(W_{d,t}\mid \varphi_{Z_d})]\\
=E_{H}[\sum_{t=1}^N \sum_{k=1}^K \sum_{v=1}^V [Z_{d,t}==k][W_{d,t}==v] log\varphi_{k,v}]\\
=\sum_{t=1}^N \sum_{k=1}^K \sum_{v=1}^VE_{H}[[Z_{d,t}==k][W_{d,t}==v] log\varphi_{k,v}]\\
=\sum_{t=1}^N \sum_{k=1}^K \sum_{v=1}^V\pi_{d,t,k}[W_{d,t}==v] log\varphi_{k,v}
$$  

 
与类别有关的项：  

$$
E_{H}[logP(\varphi_k;\mathbf{β})]=log\Gamma(\sum_{v=1}^V \beta_{v}) -\sum_{v=1}^{V}log\Gamma(\beta_{v}) + \sum_{v=1}^{V}(\beta_{v}-1)(\Psi(\phi_{k,v})-\Psi(\sum_{v=1}^V \phi_{k,v}))
$$

$$
E_{H}[logQ(\varphi_k;\phi_k)=log\Gamma(\sum_{v=1}^V \phi_{k,v}) -\sum_{v=1}^{V}log\Gamma(\phi_{k,v}) + \sum_{v=1}^{V}(\phi_{k,v}-1)(\Psi(\phi_{k,v})-\Psi(\sum_{v=1}^V \phi_{k,v}))
$$   



初始化参数$$\mathbf{α},\mathbf{β},\gamma,\pi,\phi$$

**E-step：**为了减少$$D_{KL}$$，我们更新$$\gamma,\pi,\phi$$：  
分别对$$\gamma,\pi,\phi$$求导=0，则：  

$$
\pi_{d,t,k} \propto \varphi_{d,t,k}exp\{\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k})\}\\
\gamma_{d,k}=\alpha_{k}+\sum_{t=1}^{N_{j}}\pi_{d,t,k}\\
\phi_{k,v}= \varphi_{k,v}
$$


对$$\pi_{d,t,k}$$求导时，使用了拉格朗日乘数因为$$\sum_{k=1}^K\pi_{d,t,k}=1$$   

$$\varphi_{d,t,k}$$第k个类别，第v个字的概率，d,t 决定了v是哪个字$$\varphi_{v,k}$$    

当$$D_{KL}$$极小时，迭代则结束   

**M-step:** 使用Q,来重新估计参数$$\varphi$$,因为$$\pi_{d,t,k}$$代表了字$$w_{d,t}$$是类别 k 的概率。

$$
\varphi_{k,v} \propto \sum_{d=1}^M\sum_{t=1}^{N_{d}}\pi_{d,t,k}[w_{d,t}==v]
$$  

最后求得$$\varphi_{k}$$每个类型的字词频数分布以及计算每篇文章类别分布所需的参数$$\gamma_{d,k}$$

参考：  
[Varitional Methods for Latent Dirichlet Allocation](http://net.pku.edu.cn/~zhaoxin/vEMLDA.pdf)   
[Inference Methods for Latent Dirichlet Allocation](http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf)   
[Variational inferencefor LDA](https://courses.engr.illinois.edu/cs598jhm/sp2010/Slides/Lecture07HO.pdf)  
[Topic Models and LDA](https://www.utdallas.edu/~nrr150130/cs6347/2015sp/lects/Lecture_17_LDA.pdf)  


# 方法二、Gibbs
