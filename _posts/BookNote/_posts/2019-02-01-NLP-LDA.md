---
layout: post
title: LDA(Latent Dirichlet Allocation)
date:   2019-01-01
categories: 
---

# 基础概念

**Beta(贝塔)函数和Gamma(伽玛)函数**  

欧拉积分是由瑞士数学家莱昂哈德·欧拉整理得出的两类特殊的含参变量的积分。由欧拉积分所定义的函数分别称为Beta函数和Gamma函数。——百度百科  

Gamma(伽玛)函数：  

$$\Gamma(s)=\int_{0}^{+\infty} x^{s-1}e^{-x}dx,(s>0)$$

Beta(贝塔)函数:  

$$B(p,q)=\int_{0}^{1} x^{p-1}(1-x)^{q-1}dx,(p>0,q>0)$$  

两者关系：  

$$B(p,q) = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$$

**Beta分布**

(二元)Beta分布的概率密度函数：  

$$
f(x;\alpha,\beta)= \frac{x^{\alpha-1}(1-x)^{\beta-1}} {\int_{0}^{1}\mu^{\alpha-1}(1-\mu)^{\beta-1}d\mu}\\
=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\\
=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}(1-x)^{\beta-1}  
$$    

注意：$$\alpha,\beta$$是参数，x是变量

**Dirichlet分布(多元Beta分布)**    

$$f(x_{1},...x_{K-1};\alpha_{1},...\alpha_{K})=\frac{1}{B(\mathbf{α})}\prod_{i=1}^{K}x_{i}^{\alpha_{i}-1}$$

自由度是K-1  

$$\sum_{i=1}^K x_{i}=1,x_{K}=1-\sum_{i=1}^{K-1}$$   

$$
B(\mathbf{α})=\frac{\prod_{i=1}^{K}\Gamma(\alpha_{i})} {\Gamma(\sum_{i=1}^K \alpha_{i})},\mathbf{α}=(\alpha_{1},...\alpha_{K})
$$

注意：$$\mathbf{α}$$是参数，$$x_{1},...x_{K-1}$$是变量  
 
# LDA模型
**希望解决的问题**   

对文章进行归类，分为K类，获得每类文章的词汇词频分布，确定每类文章关键词。

**Notation**  

K：主题个数（类别数），常量  
V：总词汇量，常量  
M：文章总数，常量  
$$N_{d=1..M}:$$每篇文章的词数，常量  
$$N:$$所有文章词数和$$N=\sum_{d=1}^M N_{d}$$，常量  
$$\theta_{i}:$$第i篇文章的类别分布，K维向量，向量元素和为1。    
例：假设K=3，类别是(娱乐、体育、科技)，文章i类别分布是(0.25,0.25,0.5),元素和为1。每篇文章会有自己类别分布。    
$$\varphi_{k}:$$第k类文章词汇词频分布，V维向量，向量元素和为1。  
$$Z$$:所有文章中每个字(词)类别，N 维向量，向量元素取值范围[1,K]。  
$$W$$:所有文章的字(词)，N 维向量，向量元素取值范围[1，V]。
# 模型推导过程

目前已知每篇文章由哪些词构成。现假设每篇文章词是由以下方式生成：  

1、假设每篇文章的类别分布(向量)，服从Dirichlet分布,从Dir分布中获得第i篇类别分布向量$$\theta_{i}$$   

$$\theta_{d}\sim Dir(\mathbf{α_{(d)}}),d\in{1,...,M}$$    

$$\theta_{d}$$是Dir分布的变量，$$\mathbf{α_{(d)}}$$是参数,每一篇文章会有自己参数

实际上我们不能从分布中获得文章的类别分布，这里只是假设能获得

2、假设不同类别文章词汇词频分布(向量)，服从Dirichlet分布，从Dir分布中获得第k类文章词汇词频分布向量$$\varphi_{k}$$       

$$\varphi_{k} \sim Dir(\mathbf{β_{(k)}}),k\in{1,...,K}$$    

$$\varphi_{k}$$是Dir分布的变量，$$\mathbf{β_{(k)}}$$是参数,,每一类文章会有自己参数

实际上我们不能从分布中获得每类文章的词汇词频分布，这里只是假设能获得   

3、生成第d篇文章，第j个位置的词，$$d\in{1,...,M},j\in{1,...,N_{d}}$$    

+ 按第d篇文章的类别分布，随机选择一个类别作为文章 d 第j个位置的词的类别，标记为$$z_{dj}\sim Multinomial(\theta_{d})$$   

+ 按$$z_{dj}$$类词汇词频分布，随机选择一个字作为第d篇文章，第j个位置的词，标记为$$w_{dj}\sim Multinomial(\varphi_{z_{dj}})$$

+ 不断循环3,生成每一篇文章的词

**目前已知每篇文章由哪些词构成，我们希望获得每篇文章类别分布$$\theta_{d}$$，以及每类文章的词频分布$$\varphi_{k}$$**  

# 方法一、EM  

1、明确隐变量，写出完全数据的对数似然函数  


$$
W=(W_1,W_2...W_M)\\  
Z=(Z_1,Z_2...Z_M)\\  
\theta=(\theta_1,\theta_2...\theta_M) \\ 
\mathbf{α}=(\mathbf{α_1},\mathbf{α_2}...\mathbf{α_M})\\
\varphi=(\varphi_1,\varphi_2...\varphi_K)\\
\mathbf{β}=(\mathbf{β_1},\mathbf{β_2}...\mathbf{β_K})\\
$$

$$W,Z,\theta,\mathbf{α}$$下标表示第m篇文章的所有字，所有字类别，类别分布及对对应参数     
$$\varphi,\mathbf{β}$$下标表示第k类文章的字频数分布及对应参数   

$$
logP(W;\mathbf{α},\mathbf{β})=\int_{\theta}\int_{\varphi}\sum_{Z}P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})d_{\theta}d_{\varphi}
$$

令$$H=(Z,\theta,\varphi),P(H \mid W;\mathbf{α},\mathbf{β})\approx Q(H;\gamma,\pi,\phi)$$，根据平均场理论：    

$$Q(H;\gamma,\pi,\phi)=Q(\theta;\gamma)Q(Z;\pi)Q(\varphi;\phi)$$   

则:  

$$
logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}P(W,H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}Q(H;\gamma,\pi,\phi)\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}\\
$$

令$$C(H)=\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}$$则：  

$$logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}Q(H;\gamma,\pi,\phi)C(H)=logE_{H}(C(H))\\
\ge E_{H}log(C(H)) ,jensen \ inequality
$$

损失函数是：  

$$
L= E_{H}log(C(H))\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W, H;\mathbf{α})}{Q(H;\gamma,\pi,\phi)}\\
$$  

上式中$$P(H \mid W)\approx Q(H)$$,使用$$D_{KL}$$相对熵来衡量分布相似发现：  

$$
D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))=\sum_{H}Q(H;\gamma,\pi,\phi)\frac{Q(H;\gamma,\pi,\phi)}{ P(H\mid W;\mathbf{α},\mathbf{β})}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)P(W;\mathbf{α},\mathbf{β})}{ P(H,W;\mathbf{α},\mathbf{β})}]\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)}{ P(H,W;\mathbf{α},\mathbf{β})}+logP(W;\mathbf{α},\mathbf{β})]\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[log\frac{Q(H;\gamma,\pi,\phi)}{ P(H,W;\mathbf{α},\mathbf{β})}]+logP(W;\mathbf{α},\mathbf{β})\\
$$

$$
logP(W;\mathbf{α},\mathbf{β})=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))-\sum_{H}Q(H;\gamma,\pi,\phi)log\frac{Q(H;\gamma,\pi,\phi)}{P(H,W;\mathbf{α},\mathbf{β})}\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi,\phi)log(\frac{Q(H;\gamma,\pi,\phi)}{P(H,W;\mathbf{α},\mathbf{β})})^{-1}\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi,\phi)log(\frac{P(H,W;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)})\\
=D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+L
$$

因为$$D_{KL}(Q(H;\gamma,\pi,\phi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))\ge 0$$,所以$$L$$是$$logP(W)$$下限，对于事实$$P(W)$$是不变的，因此增大$$L$$，同时会减少$$D_{KL},P(H \mid W)$$和$$Q(H)$$越相似。

$$
L= \sum_{H}Q(H;\gamma,\pi,\phi)log\frac{P(W,H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi,\phi)}\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log P(W,H;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi,\phi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)log [P(W\mid Z)P(Z\mid \varphi)P(\theta;\mathbf{α})P(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi,\phi)log Q(H;\gamma,\pi)\\
=\sum_{H}Q(H;\gamma,\pi,\phi)[logP(W\mid Z)+logP(Z\mid \varphi)+logP(\theta;\mathbf{α})+logP(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi)log Q(H;\gamma,\pi)\\
=E_{H}[log P(W;Z,\varphi)]+E_{H}[logP(Z;\theta)]+E_{H}[logP(\theta;\mathbf{α})]+E_{H}[logP(\varphi;\mathbf{β})]\\
-E_{H}[logQ(Z;\pi)]-E_{H}[logQ(\theta;\gamma)]-E_{H}[logQ(\varphi;\phi)]
$$


$$
E_{H}[logP(\theta;\mathbf{α})]=E_{Q}[logP(\theta_1,\theta_2...\theta^M;\mathbf{α_1},\mathbf{α_2}...\mathbf{α_M})]\\
=E_{H}[log(P(\theta_1;\mathbf{α_1})P(\theta_2;\mathbf{α_2})...P(\theta_M;\mathbf{α_M}))]\\
=E_{H}[logP(\theta_1;\mathbf{α_1})+logP(\theta_2;\mathbf{α_2})+...+logP(\theta_M;\mathbf{α_M})]\\
$$

$$
E_{H}[logP(\varphi;\mathbf{β})]=E_{Q}[logP(\varphi_1,\varphi_2...\varphi_K;\mathbf{β_1},\mathbf{β_2}...\mathbf{β_K})]\\
=E_{H}[log(P(\varphi_1;\mathbf{β_1})P(\varphi_2;\mathbf{β_2})...P(\varphi_K;\mathbf{β_K}))]\\
=E_{H}[logP(\varphi_1;\mathbf{β_1})+logP(\varphi_2;\mathbf{β_2})+...+logP(\varphi_K;\mathbf{β_K})]\\
$$ 


$$
E_{H}[logP(Z;\theta)]=E_{Q}[logP(Z_1,Z_2...Z_M;\theta^1,\theta_2...\theta_M)]\\
=E_{H}[log(P(Z_1;\theta_1)P(,Z_2;\theta_2)... P(Z^M;\theta_M))]\\
=E_{H}[logP(Z_1;\theta_1)+logP(,Z_2;\theta_2)+...+logP(Z_M;\theta_M)]\\

$$   


$$
E_{H}[log P(W;Z,\varphi)]=E_{Q}[log P(W_1,W_2...W_M;Z_1,Z_2...Z_M,\varphi_1,\varphi_2...\varphi_K)]\\
=E_{H}[log(P(W_1;\varphi_{Z_1})P(W_2;\varphi_{Z_2})...P(W_M;\varphi_{Z_M}))]\\
=E_{H}[logP(W_1;\varphi_{Z_1})+logP(W^2;\varphi_{Z_2})+...+logP(W_M;\varphi_{Z_M})]\\
$$  

$$\varphi_{Z_M}:Z_M$$相应类别下，对应的词汇词频分布    

同理：  

$$
E_{H}[logQ(\theta;\gamma)]=E_{H}[logQ(\theta_1;\gamma_1)+logQ(\theta_2;\gamma_2)+...+logQ(\theta_M;\gamma_M)]\\
E_{H}[logQ(\varphi;\phi)]=E_{H}[logQ(\varphi_1;\phi_1)+logQ(\varphi_2;\phi_2)+...+logQ(\varphi_K;\phi_K)]\\
E_{H}[logQ(Z;\pi)]=E_{H}[logQ(Z_1;\pi_1)+logQ(,Z_2;\pi_2)+...+logQ(Z_M;\pi_M)]\\

$$   

只关注文章d则：  

$$
E_{H}[logP(\theta_d;\mathbf{α_d})]=E_{H}[log(\frac {\Gamma(\sum_{k=1}^K \alpha_{d,k})}  {\prod_{k=1}^{K}\Gamma(\alpha_{d,k})}   \prod_{k=1}^{K}\theta_{d,k}^{\alpha_{d,k}-1})]\\

=E_{H}[log\Gamma(\sum_{k=1}^K \alpha_{d,k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{d,k}) + \sum_{k=1}^{K}(\alpha_{d,k}-1)log\theta_{d,k}]\\
=log\Gamma(\sum_{k=1}^K \alpha_{d,k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{d,k}) + \sum_{k=1}^{K}(\alpha_{d,k}-1)E_{H}[log\theta_{d,k}]\\

$$

式子中$$H \sim Q(H;\gamma,\pi,\phi),E_{H}[log\theta_{d,k}]$$中$$\theta_{d}\sim Dirichlet(\gamma_{d})$$    

$$
E_{H}[log\theta_{d,k}]=\frac{dA(\gamma_{d})}{d\gamma_{d,k}}\\
=\frac{d}{d\gamma_{d,k}}\sum_{k=1}^{K}log\Gamma(\alpha_{d,k})-log\Gamma(\sum_{k=1}^K \alpha_{d,k}) \\
=\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k})
$$

$$\Psi():$$ the first derivative of the log Gamma function    

$$
E_{H}[logP(\theta_d;\mathbf{α_d})]=log\Gamma(\sum_{k=1}^K \alpha_{d,k}) -\sum_{k=1}^{K}log\Gamma(\alpha_{d,k}) + \sum_{k=1}^{K}(\alpha_{d,k}-1)(\Psi(\gamma_{d,k})-\Psi(\sum_{k=1}^K \gamma_{d,k}))
$$


与文章无关的项：  

$$
E_{H}[logP(\varphi_k;\mathbf{β_k}]=log\Gamma(\sum_{v=1}^V \beta_{k,v}) -\sum_{v=1}^{V}log\Gamma(\beta_{k,v}) + \sum_{v=1}^{V}(\beta_{k,v}-1)(\Psi(\phi_{d,k})-\Psi(\sum_{v=1}^V \phi_{k,v}))
$$



# 方法二、Gibbs
