---
layout: post
title: LDA(Latent Dirichlet Allocation)
date:   2019-01-01
categories: 
---

# 基础概念

**Beta(贝塔)函数和Gamma(伽玛)函数**  

欧拉积分是由瑞士数学家莱昂哈德·欧拉整理得出的两类特殊的含参变量的积分。由欧拉积分所定义的函数分别称为Beta函数和Gamma函数。——百度百科  

Gamma(伽玛)函数：  

$$\Gamma(s)=\int_{0}^{+\infty} x^{s-1}e^{-x}dx,(s>0)$$

Beta(贝塔)函数:  

$$B(p,q)=\int_{0}^{1} x^{p-1}(1-x)^{q-1}dx,(p>0,q>0)$$  

两者关系：  

$$B(p,q) = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$$

**Beta分布**

(二元)Beta分布的概率密度函数：  

$$
f(x;\alpha,\beta)= \frac{x^{\alpha-1}(1-x)^{\beta-1}} {\int_{0}^{1}\mu^{\alpha-1}(1-\mu)^{\beta-1}d\mu}\\
=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\\
=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}(1-x)^{\beta-1}  
$$    

注意：$$\alpha,\beta$$是参数，x是变量

**Dirichlet分布(多元Beta分布)**    

$$f(x_{1},...x_{K-1};\alpha_{1},...\alpha_{K})=\frac{1}{B(\mathbf{α})}\prod_{i=1}^{K}x_{i}^{\alpha_{i}-1}$$

自由度是K-1  

$$\sum_{i=1}^K x_{i}=1,x_{K}=1-\sum_{i=1}^{K-1}$$   

$$
B(\mathbf{α})=\frac{\prod_{i=1}^{K}\Gamma(\alpha_{i})} {\Gamma(\sum_{i=1}^K \alpha_{i})},\mathbf{α}=(\alpha_{1},...\alpha_{K})
$$

注意：$$\mathbf{α}$$是参数，$$x_{1},...x_{K-1}$$是变量  
 
# LDA模型
**希望解决的问题**   

对文章进行归类，分为K类，获得每类文章的词汇词频分布，确定每类文章关键词。

**Notation**  

K：主题个数（类别数），常量  
V：总词汇量，常量  
M：文章总数，常量  
$$N_{d=1..M}:$$每篇文章的词数，常量  
$$N:$$所有文章词数和$$N=\sum_{d=1}^M N_{d}$$，常量  
$$\theta_{i}:$$第i篇文章的类别分布，K维向量，向量元素和为1。    
例：假设K=3，类别是(娱乐、体育、科技)，文章i类别分布是(0.25,0.25,0.5),元素和为1。每篇文章会有自己类别分布。    
$$\varphi_{k}:$$第k类文章词汇词频分布，V维向量，向量元素和为1。  
$$Z$$:所有文章中每个字(词)类别，N 维向量，向量元素取值范围[1,K]。  
$$W$$:所有文章的字(词)，N 维向量，向量元素取值范围[1，V]。
# 模型推导过程

目前已知每篇文章由哪些词构成。现假设每篇文章词是由以下方式生成：  

1、假设每篇文章的类别分布(向量)，服从Dirichlet分布,从Dir分布中获得第i篇类别分布向量$$\theta_{i}$$   

$$\theta_{d}\sim Dir(\mathbf{α_{(d)}}),d\in{1,...,M}$$    

$$\theta_{d}$$是Dir分布的变量，$$\mathbf{α_{(d)}}$$是参数,每一篇文章会有自己参数

实际上我们不能从分布中获得文章的类别分布，这里只是假设能获得

2、假设不同类别文章词汇词频分布(向量)，服从Dirichlet分布，从Dir分布中获得第k类文章词汇词频分布向量$$\varphi_{k}$$       

$$\varphi_{k} \sim Dir(\mathbf{β_{(k)}}),k\in{1,...,K}$$    

$$\varphi_{k}$$是Dir分布的变量，$$\mathbf{β_{(k)}}$$是参数,,每一类文章会有自己参数

实际上我们不能从分布中获得每类文章的词汇词频分布，这里只是假设能获得   

3、生成第d篇文章，第j个位置的词，$$d\in{1,...,M},j\in{1,...,N_{d}}$$    

+ 按第d篇文章的类别分布，随机选择一个类别作为文章 d 第j个位置的词的类别，标记为$$z_{dj}\sim Multinomial(\theta_{d})$$   

+ 按$$z_{dj}$$类词汇词频分布，随机选择一个字作为第d篇文章，第j个位置的词，标记为$$w_{dj}\sim Multinomial(\varphi_{z_{dj}})$$

+ 不断循环3,生成每一篇文章的词

**目前已知每篇文章由哪些词构成，我们希望获得每篇文章类别分布$$\theta_{d}$$，以及每类文章的词频分布$$\varphi_{k}$$**  

# 方法一、EM  

1、明确隐变量，写出完全数据的对数似然函数  


$$
W=(W^1,W^2...W^M)\\  
Z=(Z^1,Z^2...Z^M)\\  
\theta=(\theta^1,\theta^2...\theta^M) \\ 
\mathbf{α}=(\mathbf{α^1},\mathbf{α^2}...\mathbf{α^M})\\
\varphi=(\varphi^1,\varphi^2...\varphi^K)\\
\mathbf{β}=(\mathbf{β^1},\mathbf{β^2}...\mathbf{β^K})\\
$$

$$W,Z,\theta,\mathbf{α}$$上标表示第m篇文章的所有字，所有字类别，类别分布及对对应参数     
$$\varphi,\mathbf{β}$$上标表示第k类文章的字频数分布及对应参数   

$$
logP(W;\mathbf{α},\mathbf{β})=\int_{\theta}\int_{\varphi}\sum_{Z}P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})d_{\theta}d_{\varphi}
$$

令$$H=(Z,\theta,\varphi),P(H \mid W;\mathbf{α},\mathbf{β})\approx Q(H;\gamma,\pi)$$，则:  

$$
logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}P(W,H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})\\
=log\sum_{H}Q(H;\gamma,\pi)\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi)}\\
$$

令$$C(H)=\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi)}$$则：  

$$logP(W;\mathbf{α},\mathbf{β})=log\sum_{H}Q(H;\gamma,\pi)C(H)=logE_{H}(C(H))\\
\ge E_{H}log(C(H)) ,jensen \ inequality
$$

损失函数是：  

$$
L= E_{H}log(C(H))\\
=\sum_{H}Q(H;\gamma,\pi)log\frac{P(W \mid H;\mathbf{α},\mathbf{β})P(H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi)}\\
=\sum_{H}Q(H;\gamma,\pi)log\frac{P(W, H;\mathbf{α})}{Q(H;\gamma,\pi)}\\
$$  

上式中$$P(H \mid W)\approx Q(H)$$,使用$$D_{KL}$$相对熵来衡量分布相似发现：  

$$
D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))=\sum_{H}Q(H;\gamma,\pi)\frac{Q(H)}{ P(H\mid W;\mathbf{α},\mathbf{β})}\\
=\sum_{H}Q(H;\gamma,\pi)[log\frac{Q(H;\gamma,\pi)P(W;\mathbf{α},\mathbf{β})}{ P(H,W;\mathbf{α},\mathbf{β})}]\\
=\sum_{H}Q(H;\gamma,\pi)[log\frac{Q(H;\gamma,\pi)}{ P(H,W;\mathbf{α},\mathbf{β})}+logP(W;\mathbf{α},\mathbf{β})]\\
=\sum_{H}Q(H;\gamma,\pi)[log\frac{Q(H;\gamma,\pi)}{ P(H,W;\mathbf{α},\mathbf{β})}]+logP(W;\mathbf{α},\mathbf{β})\\
$$

$$
logP(W;\mathbf{α},\mathbf{β})=D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W))-\sum_{H}Q(H)log\frac{Q(H)}{P(H,W;\mathbf{α},\mathbf{β})}\\
=D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi)log(\frac{Q(H;\gamma,\pi)}{P(H,W;\mathbf{α},\mathbf{β})})^{-1}\\
=D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+\sum_{H}Q(H;\gamma,\pi)log(\frac{P(H,W;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi)})\\
=D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))+L
$$

因为$$D_{KL}(Q(H;\gamma,\pi) \parallel P(H\mid W;\mathbf{α},\mathbf{β}))\ge 0$$,所以$$L$$是$$logP(W)$$下限，对于事实$$P(W)$$是不变的，因此增大$$L$$，同时会减少$$D_{KL},P(H \mid W)$$和$$Q(H)$$越相似。

$$
L= \sum_{H}Q(H;\gamma,\pi)log\frac{P(W,H;\mathbf{α},\mathbf{β})}{Q(H;\gamma,\pi)}\\
=\sum_{H}Q(H;\gamma,\pi)log P(W,H;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi)log Q(H;\gamma,\pi)\\
=\sum_{H}Q(H;\gamma,\pi)log P(W,Z,\theta,\varphi;\mathbf{α},\mathbf{β})-\sum_{H}Q(H;\gamma,\pi)log Q(H;\gamma,\pi)\\
=\sum_{H}Q(H;\gamma,\pi)log [P(W\mid Z)P(Z\mid \varphi)P(\theta;\mathbf{α})P(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi)log Q(H;\gamma,\pi)\\
=\sum_{H}Q(H;\gamma,\pi)[logP(W\mid Z)+logP(Z\mid \varphi)+logP(\theta;\mathbf{α})+logP(\varphi;\mathbf{β})]-\sum_{H}Q(H;\gamma,\pi)log Q(H;\gamma,\pi)\\
=E_{H}[log P(W;Z,\varphi)]+E_{H}[logP(Z;\theta)]+E_{H}[logP(\theta;\mathbf{α})]+E_{H}[logP(\varphi;\mathbf{β})]\\
-E_{H}[logQ(Z;\theta)]-E_{H}[logQ(\theta;\gamma)]-E_{H}[logQ(\varphi;\pi)]
$$


$$
E_{H}[logP(\theta;\mathbf{α})]=E_{Q}[logP(\theta^1,\theta^2...\theta^M;\mathbf{α^1},\mathbf{α^2}...\mathbf{α^M})]\\
=E_{H}[log(P(\theta^1;\mathbf{α^1})P(\theta^2;\mathbf{α^2})...P(\theta^M;\mathbf{α^M}))]\\
=E_{H}[logP(\theta^1;\mathbf{α^1})+logP(\theta^2;\mathbf{α^2})+...+logP(\theta^M;\mathbf{α^M})]\\
$$

$$
E_{H}[logP(\varphi;\mathbf{β})]=E_{Q}[logP(\varphi^1,\varphi^2...\varphi^K;\mathbf{β^1},\mathbf{β^2}...\mathbf{β^K})]\\
=E_{H}[log(P(\varphi^1;\mathbf{β^1})P(\varphi^2;\mathbf{β^2})...P(\varphi^K;\mathbf{β^K}))]\\
=E_{H}[logP(\varphi^1;\mathbf{β^1})+logP(\varphi^2;\mathbf{β^2})+...+logP(\varphi^K;\mathbf{β^K})]\\
$$ 


$$
E_{H}[logP(Z;\theta)]=E_{Q}[logP(Z^1,Z^2...Z^M;\theta^1,\theta^2...\theta^M)]\\
=E_{H}[log(P(Z^1;\theta^1)P(,Z^2;\theta^2)... P(Z^M;\theta^M))]\\
=E_{H}[logP(Z^1;\theta^1)+logP(,Z^2;\theta^2)+...+logP(Z^M;\theta^M)]\\

$$   


$$
E_{H}[log P(W;Z,\varphi)]=E_{Q}[log P(W^1,W^2...W^M;Z^1,Z^2...Z^M,\varphi^1,\varphi^2...\varphi^K)]\\
=E_{H}[log(P(W^1;\varphi_{Z^1})P(W^2;\varphi_{Z^2})...P(W^M;\varphi_{Z^M}))]\\
=E_{H}[logP(W^1;\varphi_{Z^1})+logP(W^2;\varphi_{Z^2})+...+logP(W^M;\varphi_{Z^M})]\\
$$  

$$\varphi_{Z^M}:Z^M$$相应类别下，对应的词汇词频分布    

同理：  

$$
E_{H}[logQ(\theta;\gamma)]=E_{H}[logQ(\theta^1;\gamma^1)+logQ(\theta^2;\gamma^2)+...+logQ(\theta^M;\gamma^M)]\\
E_{H}[logQ(\varphi;\pi)]=E_{H}[logQ(\varphi^1;\pi^1)+logQ(\varphi^2;\pi^2)+...+logQ(\varphi^K;\pi^K)]\\
E_{H}[logQ(Z;\theta)]=E_{H}[logQ(Z^1;\theta^1)+logQ(,Z^2;\theta^2)+...+logQ(Z^M;\theta^M)]\\

$$   

# 方法二、Gibbs
