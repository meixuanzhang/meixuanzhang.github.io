---
layout: post
title: 译Lecture 3 Undirected Graphical Models
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

无向图形模型简介。  

# 回顾

除了上节课中介绍的I-map概念外，今天的课还包括最小的I-map。  

**最小I-maps**   

如果DAG $$g$$是分布$$P$$的I-map，并且如果从$$g$$中移除任意一条边它将不再是I-map，则它是最小I-map。 

+ 一个分布可以有几个最小I-map，每个对应于一个特定的节点顺序。  

+ $$g$$是$$P$$的最小I-map，这远远不能保证$$g$$捕获$$P$$中的独立结构。


# “Bayes-ball”算法     

我们可以应用“Bayes-ball”算法直接从图模型中检索独立性。我们说给定Y时，X与Z是d-separated的，如果我们不能把球从X中的任何节点发送到Z中的任何节点。条件概率语句（“给定Y”）由图中节点的阴影表示。下面是三种基本有向图结构的示例。    

![_config.yml]({{ site.baseurl }}/images/10708/image31.png)   

+ 在(a)和(b)中，阴影$$Y$$节点阻止球在节点$$X$$和$$Z$$之间移动。这给出了上节课中介绍的独立关系：$$X\bot Z\mid Y$$   

+ (c)又称“V-structure”，是一个特例。与前两个示例相反，如果节点$$Y$$为阴影，则球可以在$$X$$和$$Z$$之间移动，否则将被阻止。因此，右边的图$$X\bot Z$$。   

有了这些基本结构，我们可以对DAG应用规则。例如，让我们试着找出在给定$$X_{1},X_{6}$$时，$$X_{2},X_{3}$$相互独立。  

![_config.yml]({{ site.baseurl }}/images/10708/image32.png)   

阴影$$X_{1},X_{6}$$,球不能从$$X_{2}$$经过$$X_{1}$$到$$X_{3}$$,因为它被阻塞了。然而$$X_{2},X_{6},X_{5}$$是“V-structure”，球可以沿着这条路走$$X_{2},X_{6},X_{5},X_{3}$$.因此，独立声明无效。

# 有向和无向GMs的限制   

从表示( representational)的角度，我们的目标是找到一个图G，精确地捕捉给定分布$$P$$中的独立性。学习GMs的这个目标激发了下面的定义。 

**Perfect Maps**  

我们说$$g$$是独立集$$I$$的 perfect map (P-map) ，当$$I(g)=I$$。我们说$$g$$是分布$$P$$的 perfect map (P-map) ，当$$I(g)=I(P)$$。即$$sep_{g}(X;Z\mid Y)\Leftrightarrow P\models (X\bot Z\mid Y)$$

分布的P-map是唯一符合网络之间I-equivalent关系的图结构。也就是说，一个分布$$P$$可以有许多P-map，所有P-map都是I-equivalent。

(关于变量$$\chi$$的两个图结构$$K_{1},K_{2}$$,如果$$T(K_{1})=I(K_{2})$$则两个图结构是I-equivalent)

然而，任意分布$$P$不一定要得到perfect maps,无论是无向或有向GMs的。下面是两个这样的例子。  

图模型族的维恩图图示，其中D是有向GM的族，而U是无向GM的族。  

![_config.yml]({{ site.baseurl }}/images/10708/image33.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image34.png)  

左边:分布无法使用DGM表示，它需要$$A\bot C\mid \{B,D\}$$和$$B\bot D\mid \{A,C\}$$    
右边:这个 v-structure分布无法使用UGM表示。   


# 无向图形模型(Undirected Graphical Models)概述

+ 一对节点（随机变量）之间只能有对称关系。换句话说，从一个随机变量到另一个随机变量没有因果关系。
+ 模型可以表示分布的属性和配置，但不能显式地生成样本。
+ 每个节点都与其相邻节点有很强的相关性。

**示例**  

![_config.yml]({{ site.baseurl }}/images/10708/image35.png)  

# 贝叶斯网络: 因子分解定理(Factorization Theorem)    

将第$$i$$个节点的父节点定义为$$Pa_{X_{i}}$$,非其后代节点定义为$$NonDescendants_{X_{i}}$$。  

有向拓扑结构图的独立一组条件独立性声明：  

$$\{X_{i} \bot NonDescendants_{X_{i}} \mid Pa_{X_{i}} \}$$

![_config.yml]({{ site.baseurl }}/images/10708/image18.png)   

因此，上述有向图的联合概率可以写成如下：  

$$
P(X_{1},..,X_{8})=\prod_{i=1}^8 P(X_{i}\mid Pa_{X_{i}} )
=P(X_{1})P(X_{2})P(X_{3}\mid X_{1})P(X_{4}\mid X_{2})P(X_{5}\mid X_{2})P(X_{6}\mid X_{3},X_{4}))P(X_{7}\mid X_{6})P(X_{8}\mid X_{5},X_{6}))
$$

# 有向图模型的设置     

**任何图模型都有两个组成部分：**     

+ 定性（拓扑结构）  
+ 定量（与每个条件分布相关的数字）  

**定性设置的假来源：**    

+ 因果关系的先验知识
+ 模块化关系的先验知识
+ 专家评估
+ 从数据中学习
+ 我们只是喜欢某种架构（例如分层图）
…  

# 局部结构和独立性(Local Structures & Independencies)   

+ Common parent (also called ‘common cause’ )  

固定$$B$$解耦$$A$$和$$C$$       

给定$$B$$的情况下，$$A$$和$$C$$相互独立   

$$A \bot C \mid B \Rightarrow P(A,C \mid B)=P(A \mid B)P(C \mid B)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image19.png)    

+ Cascade (also called ‘causal/evidential trail’ )   

已知$$B$$解耦$$A$$和$$C$$   

给定$$B$$的情况下，$$A$$对于预测$$C$$没有提供额外价值    

![_config.yml]({{ site.baseurl }}/images/10708/image20.png)   

+ V-structure (also called ‘common effect’)   

已知$$C$$,则$$A$$和$$B$$关联,$$A$$可以“解释” $$B$$通过确定的$$C$$   

如果$$A$$与$$C$$相关，那么$$B$$也与$$C$$相关的机会将会减少”   

![_config.yml]({{ site.baseurl }}/images/10708/image21.png)    

**v-结构示例**  

时钟显示时间慢了（事件A）和交通阻塞（事件B）是相互独立事件，都可能导致上课迟到（事件C）。出席课堂将这两个事件关联在一起。如果我的时钟是正常，它可能会“解释”我的迟到是由于交通阻塞造成。换句话说，如果知道C，则A和B会耦合，但$$P(A,B\mid C)$$无法解耦为两个事件。C未知则$$P(A,B)=P(A)P(B)=P(A)P(B)$$   
 
# I-maps   

将$$P$$定义为$$\mathbf{X}$$的概率分布，$$I(P)$$表示分布$$P$$中蕴含独立关系集 $$(X \bot Y\mid Z)$$    

定义$$K$$为图结构，图结构中蕴含独立关系集为$$I(K)$$,如果图结构$$K$$对于分布$$P$$有$$I(K)\subseteq I(P)$$，则称$$K$$为$$P$$的I-map

# 关于I-map的事实   

要使$$g$$作为$$P$$的I-map，$$g$$声称的任何独立性也必须在$$P$$中持有。相反，$$P$$可能有其他独立性，而这些独立性并没有在$$G$$中反映出来。   

**例子**:   

![_config.yml]({{ site.baseurl }}/images/10708/image22.png)    

只有在图1中$$X$$和$$Y$$是相互独立的。   

下面我们有两张表显示边际分布。查找I-maps：  

$$P_{1}:I(P_{1})=X \bot Y$$(from inspection i.e,$$P(X,Y)=P(X)P(Y),$$0.48=0.6*0.8)    

![_config.yml]({{ site.baseurl }}/images/10708/image23.png)  

Solution: Graph 1.  

$$P_{2}:I(P_{2})=\phi$$   

![_config.yml]({{ site.baseurl }}/images/10708/image24.png)  

Solution: Both graph 2 and graph 3, 因为它们独立集都为为空。因此，它们在独立集上是等价的。$$P_{2}$$独立集不空,图1独立集不为空。因此它不是$$P_{2}$$的I-map.    

# 什么是$$I(G)$$   

**贝叶斯网络的局部马尔可夫假设**   

叶斯网络结构$$g$$是一个有向无环图，其节点表示随机变量$$X_{1},..X_{n}$$    

**局部马尔可夫假设**  

定义:  

图$$g$$中$$X_{i}$$节点的父节点定义为$$Pa_{X_{i}}$$,非其后代节点定义为$$NonDescendants_{X_{i}}$$。  

通过$$g$$对以下局部条件独立性假设$$I_{l}(g)$$编码:  

$$I_{l}(g):\{X_{i}\bot NonDescendants_{X_{i}} \mid Pa_{X_{i}}: \forall_{i}  \}$$   

换句话说每个节点在父节点的条件下独立于其非后代节点。  

# 贝叶斯网络的D-separation 准则   


定义一：  

在给定$$Z$$时,变量$$X$$和$$Y$$在正则祖先图中分离(moralized ancestral graph)，则$$X$$和$$Y$$给定$$Z$$时是D-separated。(D代表有向边)    

示例: 当$$X \bot Y \mid Z$$，我们说$$Z$$分离了$$X,Y$$   

![_config.yml]({{ site.baseurl }}/images/10708/image25.png)   

祖先图(Ancestral graph只关注感兴趣的节点):只保留节点本身+与问题有关的节点的祖先。   

正则祖先(Moral ancestral)：在Ancestral graph中，将有向边改无向边，v结构的两个父节点加一条边，这是一幅无向图。     

如果给定条件下存在任何路径从一个节点到另一个节点的路径，则这两个节点不是给定条件下独立(示例不是条件独立的)。   

# $$I(g)$$的实际定义   

贝叶斯网络的全局马尔可夫性质

$$X$$与$$Z$$是 d-separated (directed-separated)，在给定$$Y$$时。如果我们不能使用下面所示的“Bayes-ball”算法将球从$$X$$中的任何节点发送到$$Z$$中的任何节点.  

![_config.yml]({{ site.baseurl }}/images/10708/image26.png)   

定义：$$I(G)$$= 与 d-separation相对应的所有独立项:    

$$I(G):\{X_{i} \bot Z\mid Y:dsepg(X;Z \mid Y)\}$$  

# $$I(G)$$ 示例   

![_config.yml]({{ site.baseurl }}/images/10708/image27.png)   

在这个图中有两种类型的有效迹结构：  

+ common cause:$$X_{3}\gets X_{1}\to X_{4}$$。如果不给定条件$$X_{1}$$,这条迹是有效的。  

+ common effect:$$X_{2}\to X_{3}\gets X_{1}$$。如果给定条件$$X_{3}$$,这条迹是有效的。 

为了找到独立项，应考虑长度大于1的所有迹（因为节点不能独立于其父节点）。

**长度为2的迹:**  

+ $$X_{2}\rightleftharpoons X_{3}\rightleftharpoons X_{1}$$由于'common effect'结构，只要不给定条件$$X_{3}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{1}),(X_{2}\bot X_{1}\mid X_{4})$$   

+ $$X_{3}\rightleftharpoons X_{1}\rightleftharpoons X_{4}$$由于'common  cause'结构,只要给定条件$$X_{1}$$,这条路径就会阻塞,所以有$$(X_{3}\bot X_{4}\mid X_{1}),(X_{3}\bot X_{4}\mid \{X_{1},X_{2}\})$$   


**长度为3的迹(only $$X_{2}\rightleftharpoons X_{3}\rightleftharpoons X_{1}\rightleftharpoons X_{4}$$):**  

+ 由于'common effect'结构$$X_{2}\to X_{3}\gets  X_{1}$$,只要不给定条件$$X_{3}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{4}),(X_{2}\bot X_{4}\mid X_{1})$$     

+ 由于'common cause'结构$$X_{3}\gets X_{1}\to X_{4}$$,只要给定条件$$X_{1}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{4}\mid X_{1}),(X_{2}\bot X_{4}\mid \{X_{1},X_{3}\})$$   

**节点集之间的轨迹**  

+ 根据d-separation 有$$X_{2}\bot X_{1},X_{4}$$，$$X_{2}$$和$$X_{1}$$，$$X_{2}$$和$$X_{4}$$之间任意路径是阻塞的.

**Full $$I(g)$$**  

把上面这些放在一起，我们有以下独立项。  

$$
I(g)=\{(X_{2}\bot X_{1}),(X_{2}\bot X_{1}\mid X_{4}),\\
(X_{3}\bot X_{4}\mid X_{1}),(X_{3}\bot X_{4}\mid \{X_{1},X_{2}\}),\\
(X_{2}\bot X_{4}),(X_{2}\bot X_{4}\mid X_{1}),(X_{2}\bot X_{4}\mid \{X_{1},X_{3}\}),\\
(X_{2}\bot \{X_{1},X_{4}\})\}

$$   

# 等价定理  

对于图$$G$$,定义满足$$I(G)$$的所有分布族为$$D_{1}$$,定义根据图$$G$$进行因子分解的所有分布族为$$D_{2}$$.$$D_{1}=D_{2}$$,分布可以表达为：   

$$P(X)=\prod_{i=1:d}P(X_{i}\mid X_{\pi_{i}})$$  

这意味着图中的分离(separation)性质暗含了相关变量的独立性。

# 条件概率表(CPTs)  

这是一个离散概率的例子。 

![_config.yml]({{ site.baseurl }}/images/10708/image28.png)   

# 条件概率密度(CPDs)  

概率也可以是连续函数，例如高斯分布。  

![_config.yml]({{ site.baseurl }}/images/10708/image29.png)   

# BN总结  

+ 条件独立意味能因子分解  

+ 根据G进行因子分解暗指了相关的条件独立性。

# D-separation的可靠性和完备性  

可靠性和完整性是D-separation的两个理想性质   

+ 可靠性(Soundness):如果分布$$P$$根据$$G$$进行因子分解，则$$I(G)\subseteq I(P)$$(D-separation断言的所有独立性其潜在分布满足)    

+ 完整性(Completeness):对于根据$$G$$分解的任意分布$$P$$,如果$$(X\bot Y\mid Z)\in I(P)$$,则$$d-sep_{g}(X;Y\mid Z)$$(D-separation可以检测出所有的独立性)   

+ 事实上，“完整性”并不总是成立的。即使分布根据$$G$$因子分解，它仍然可以包含未反映在图结构中的其他独立性。  

**示例**  

考虑涉及变量$$A$$和$$B$$的分布$$P$$.从I-map可以观察到的每一个独立项在$$P$$中的定义(编码),$$P$$的其中之一I-map是DAG$$A\to B$$,由于它没有定义独立项，因为$$\phi$$是任何集合的子集。

但是，我们可以使用条件概率表，使独立项包含在P中，而不依赖于D-separation。其中一个条件概率表是：

![_config.yml]({{ site.baseurl }}/images/10708/image30.png)   

在此表中，$$B$$独立于$$A$$。 但这没有反映在I-map $$ A \to B$$中。  

定理:  

设$$G$$为BN图。如果$$X$$和$$Y$$在给定$$Z$$的$$G$$中不是D-separation的，根据$$G$$因子分解的$$P$$中$$X$$和$$Y$$是相互依赖的。  

定理:   

对于根据$$G$$因子分解的几乎所有分布$$P$$,也就是说对于除CPD参数化空间中的一组“measure zero”之外的所有分布有$$I(P)=I(G)$$.  

# Readings  

## The Bayesian network representation   

### 3.1.1利用独立性   

独立变量的标准VS紧凑参数化   

给定随机变量$$X_{i}$$，每个变量代表独立抛硬币的结果，其联合分布的标准参数化如下：

$$P(X_{1},..,X_{n})=P(X_{1}=x_{1},..,X_{n}=x_{n})\\
=P(X_{1}=x_{1})P(X_{2}=x_{2})..P(X_{n}=x_{n})$$   

对于每个结果$$x_{i}$$有两种取值可能，表示(representation)$$P$$需要$$2^n$$个参数。然而只有$$2^{n}-1$$个独立参数，由于最后一个参数结果取决于$$2^n$$前面的参数（概率总和为1）。   

减少所需参数数量的一种简单方法是将每个硬币投掷为正的概率表示为$$\theta_{1},..,\theta_{n}$$,则每个硬币投掷为负的概率$$1-\theta_{i}$$，然后我们得到以下仅需要n参数的紧凑表示形式：   

$$P(X_{1},..,X_{n})=\prod_{i}\theta_{X_{i}}$$   

### 3.1.3 朴素贝叶斯(Naive Bayes)   

我们可以根据条件概率进一步表示联合分布。 这是通过朴素贝叶斯模型中完成的。  

此模型的实例包括：  

+ 类(class):一些$$C\in c^1,..c^k$$类,它给出每个特征值的先验值。   

+ 特征(features): 观测特性$$X_{1},..,X_{k}$$   

该模型提出了强“朴素”条件独立假设：  

$$P(x_{i}\mid C,x_{1},..,x_{n})=P(x_{i}\mid C)$$ 

换句话说，给定模型的实例类，特征在条件上是独立的。朴素贝叶斯模型因子分解$$P(C,X_{1},..X_{n})=P(C)\prod_{i} P(X_{i}\mid C)$$

### 3.2.1 贝叶斯网络  

贝叶斯网络使用一个图，其节点是域中的随机变量，其边表示条件概率语句。与朴素贝叶斯模型不同，贝叶斯网络也可以表示不满足朴素条件独立假设的分布。

定义3.1：贝叶斯网络(BN)  

贝叶斯网络$$g$$是一个有向无环图，节点表示随机变量$$X_{1},..X_{n}$$,$$Pa_{X_{i}}^g$$定义为节点$$X_{i}\in g$$的父节点，$$NonDescendants_{X_{i}}$$为节点$$X_{i}$$的非后代节点。通过图$$g$$对以下一组条件独立性假设进行编码，也称为“‘local independencies”，定义为$$I_{l}(g)$$:   

对于每个变量$$X_{i}:(X_{i} \bot NonDescendants_{X_{i}} \mid Pa_{X_{i}}^g )$$

### 3.2.3 图和分布   

在这一节中，我们证明了分布$$P$$满足与$$g$$相关联的局部独立性，$$P$$表示为与$$g$$相关联的一组条件概率分布(CPDs)。

**定义3.2-3.3: I-MAP**     

设$$P$$是关于变量$$X$$的分布，定义$$I(P)$$为$$P$$中持有的形式为$$X\bot Y\mid Z$$的独立性断言集。  

设$$K$$是与一组独立集$$I(K)$$相关的任何图。如果$$I(K)\subseteq I$$,我们说$$K$$是$$I$$的I-map。   

如果$$g$$是$$I(P)$$的I-map，则$$g$$是$$P$$的I-map.   

**I-MAP TO FACTORIZATION**   

在本节中，证明（见正文）BN结构$$g$$所隐含的条件独立性假设允许我们通过$$g$$将分布$$P$$因子分解成条件概率分布，$$g$$是$$P$$的I-map。  

定义3.4 因子分解(Factorization)  

定义$$P$$为分布，$$g$$是变量$$X_{1},..X_{n}$$的BN图.如果$$P$$可以表示为乘积,我们称$$P$$可以根据$$g$$进行因子分解。  

$$P(X_{1},..X_{n})=\prod_{i}P(X_{i}\mid Pa_{X_{i}}^g)$$   

减少参数数量  
在n个二进制变量上的分布中,明确联合分布需要$$2^n -1$$个独立参数。如果根据$$g$$因子分解分布，当每个节点最多有$$k$$个父节点，则计算每个节点条件概率独立参数的数目小于$$2^k$$,由于k通常很小，这表示参数数量呈指数减少。

**FACTORIZATION TO I-MAP**  

反之亦然，如下所示：  

定理3.2  

定义$$g$$是随机变量$$X$$的BN图，$$P$$是相同变量空间的分布。如果可以根据$$g$$因子分解$$P$$,然后$$g$$是$$P$$的I-map.   

### Box 3.C Knowledge engineering

在现实世界中构建BN需要许多步骤，包括：  

+ 精确地选择变量：我们需要一些可以观察到的变量，并且应该指定它们的域。 有时引入隐藏变量将有助于使观察到的变量有条件地独立。

+ 选择与因果顺序一致的结构：选择足够的变量以近似因果关系。  

+ 为网络中的事件选择概率估计：数据收集可能很困难，但是很小的错误影响不大（尽管网络对分配为零概率的事件以及事件概率的相对大小都很敏感）

正确完成后，该模型将非常有用，并且使用起来不会太复杂


### 3.3.1 D-separation  

目标：确定每一个根据$$g$$因子分解的分布$$P$$有哪些独立项。  

**定义 2.16 迹**  

如果对于每一个$$i=1,..,k-1$$,我们有$$X_{i}\rightleftharpoons X_{i+1}$$,则说$$X_{1},..X_{k}$$在图$$K=(\chi,\varepsilon)$$形成了一条迹.   

**有效的2-edge迹**  

通过检查节点$$X,Y,Z$$之间的2-edge连接，可以看到四种类型的有效迹，即影响可以从$$X$$流向$$Z$$的迹： 

1、Causal trail $$X \to Z \to Y$$:active if $$Z$$ is not observed  
2、Evidential trail $$X \gets Z \gets Y$$:active if $$Z$$ is not observed   
3、Common cause  $$X \gets Z \to Y$$:active if $$Z$$ is not observed   
4、Common effect $$X  \to Z \gets Y$$: active if $$Z$$ or one of its descendants is observed

influence可以流过一个较长的迹$$X_{1}\rightleftharpoons ... \rightleftharpoons X_{n}$$，每一个2-edge迹必须允许influence流动。  

**定义3.6有效迹**  

$$g$$表示BN结构，$$X_{1}\rightleftharpoons ... \rightleftharpoons X_{n}$$是$$g$$的一条迹。$$Z$$是观测变量的子集。在给定$$Z$$条件下，迹$$X_{1}\rightleftharpoons ... \rightleftharpoons X_{n}$$是有效的，当满足：  

+ 每当我们有v-structure$$X_{i-1}  \to X_{i} \gets X_{i+1}$$,则$$X_{i}$$或其后代之一应包含在$$Z$$中。  

+ 沿着迹没有其他节点在$$Z$$中   

此外，对于有向图节点之间可能存在多条迹，“directed separation”（d-separation）给出了节点之间分离的概念。

**定义 3.7 D-SEPARATION**  

定义$$X,Y,Z$$是图$$g$$的三个集合,如果在给定$$Z$$,任意节点$$x \in X$$和$$y \in Y$$之间没有有效迹，我们称给定$$Z$$时，$$X$$和$$Y$$是d-separated(表示为$$d-sep_{g}(X;Y\mid Z)$$)  

### 3.3.2 可靠性和完整性  

可靠性：给定$$Z$$时$$X,Y$$是d-separated $$\Rightarrow X\bot Y \mid Z$$,也就是说独立性由d-separated决定，基础分布满足这些独立性。   
完整性：如果$$X\bot Y \mid Z$$,则它们是d-separated,也就是说d-separated检测出所有可能的独立性。

参考：
[Lecture 2: Bayesian Networks](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-02/)



