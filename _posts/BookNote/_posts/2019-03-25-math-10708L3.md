---
layout: post
title: 译Lecture 3 Undirected Graphical Models
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

无向图形模型简介。  

# 回顾

除了上节课中介绍的I-map概念外，今天的课还包括最小的I-map。  

**最小I-maps**   

如果DAG $$g$$是分布$$P$$的I-map，并且如果从$$g$$中移除任意一条边它将不再是I-map，则它是最小I-map。 

+ 一个分布可以有几个最小I-map，每个对应于一个特定的节点顺序。  

+ $$g$$是$$P$$的最小I-map，这远远不能保证$$g$$捕获$$P$$中的独立结构。


# “Bayes-ball”算法     

我们可以应用“Bayes-ball”算法直接从图模型中检索独立性。我们说给定Y时，X与Z是d-separated的，如果我们不能把球从X中的任何节点发送到Z中的任何节点。条件概率语句（“给定Y”）由图中节点的阴影表示。下面是三种基本有向图结构的示例。    

![_config.yml]({{ site.baseurl }}/images/10708/image31.png)   

+ 在(a)和(b)中，阴影$$Y$$节点阻止球在节点$$X$$和$$Z$$之间移动。这给出了上节课中介绍的独立关系：$$X\bot Z\mid Y$$   

+ (c)又称“V-structure”，是一个特例。与前两个示例相反，如果节点$$Y$$为阴影，则球可以在$$X$$和$$Z$$之间移动，否则将被阻止。因此，右边的图$$X\bot Z$$。   

有了这些基本结构，我们可以对DAG应用规则。例如，让我们试着找出在给定$$X_{1},X_{6}$$时，$$X_{2},X_{3}$$相互独立。  

![_config.yml]({{ site.baseurl }}/images/10708/image32.png)   

阴影$$X_{1},X_{6}$$,球不能从$$X_{2}$$经过$$X_{1}$$到$$X_{3}$$,因为它被阻塞了。然而$$X_{2},X_{6},X_{5}$$是“V-structure”，球可以沿着这条路走$$X_{2},X_{6},X_{5},X_{3}$$.因此，独立声明无效。

# 有向和无向GMs的限制   

从表示( representational)的角度，我们的目标是找到一个图G，精确地捕捉给定分布$$P$$中的独立性。学习GMs的这个目标激发了下面的定义。 

**Perfect Maps**  

我们说$$g$$是独立集$$I$$的 perfect map (P-map) ，当$$I(g)=I$$。我们说$$g$$是分布$$P$$的 perfect map (P-map) ，当$$I(g)=I(P)$$。即$$sep_{g}(X;Z\mid Y)\Leftrightarrow P\models (X\bot Z\mid Y)$$

分布的P-map是唯一符合网络之间I-equivalent关系的图结构。也就是说，一个分布$$P$$可以有许多P-map，所有P-map都是I-equivalent。

(关于变量$$\chi$$的两个图结构$$K_{1},K_{2}$$,如果$$I(K_{1})=I(K_{2})$$则两个图结构是I-equivalent)

然而，任意分布$$P$$不一定能得到perfect maps,无论是无向或有向GMs的。下面是两个这样的例子。  

图模型族的维恩图图示，其中D是有向GM的族，而U是无向GM的族。  

![_config.yml]({{ site.baseurl }}/images/10708/image33.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image34.png)  

左边:分布无法使用DGM表示，它需要$$A\bot C\mid \{B,D\}$$和$$B\bot D\mid \{A,C\}$$    
右边:这个 v-structure分布无法使用UGM表示。   


# 无向图形模型(Undirected Graphical Models)概述

+ 一对节点（随机变量）之间只能有对称关系。换句话说，从一个随机变量到另一个随机变量没有因果关系。
+ 模型可以表示分布的属性和构造，但不能显式地生成样本。
+ 每个节点都与其相邻节点有很强的相关性。

**示例**  

![_config.yml]({{ site.baseurl }}/images/10708/image35.png)  

让每个节点代表一个图像补丁。不可能分辨出这个图像块的内容如果将它与其他图像块隔离开来。然而，当我们观察它的相邻图像块时，我们可以看到它是一个水的图像块。由于相邻图像块之间的关系应该是对称的，因此图像最好用无向图形模型来表示。这种特殊的无向图形模型也称为网格模型。

# 定量(Quantitative Specification)    

## 团(Cliques)  

+ 团是完全连通的子图。(单个节点，有两个节点及以上时任意两两节点连通)   
+ 一个最大团是一个团，使得任何超集（包含这个子图的任何更大子图）不是一个完整的图。(最大团任意增加节点不再是团)  
+ 子团不一定是最大团  

**示例:**  

![_config.yml]({{ site.baseurl }}/images/10708/image36.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image37.png)  

![_config.yml]({{ site.baseurl }}/images/10708/image38.png)  

## 势函数(Potential Functions)  

每个团可以与一个势函数$$\psi$$相关联，该势函数可以理解为其参数的一个临时函数，该函数为其联合分布分配一个预概率分数。这个势函数可以是任意的，但必须是非负的。

(参数，也叫参变量，是一个变量。如果我们引入一个或一些另外的变量来描述自变量与因变量的变化，引入的变量本来并不是当前问题必须研究的变量，我们把这样的变量叫做参变量或参数。)

为什么是团？团的每个组成部分都对势函数整体做出了贡献。 

**示例**  

用于说明势函数的示例。$$X_{1}$$和$$X_{2}$$都是二进制的。  

![_config.yml]({{ site.baseurl }}/images/10708/image39.png)   

对于$$\psi(X_{1},X_{2})$$，此表表示给定随机变量输入组合的势函数的值： 

![_config.yml]({{ site.baseurl }}/images/10708/image40.png)   

势函数不一定是概率，另一个用来说明势函数的例子：   

![_config.yml]({{ site.baseurl }}/images/10708/image41.png) 

这个模型意味着$$X\bot Z \mid Y$$。这个独立性声明意味着（根据定义）联合分布必须分解为： 

$$P(X,Y,Z)=P(Y)P(X\mid Y)P(Z\mid Y)\\
=P(X,Y)P(Z\mid Y)\\
=P(X\mid Y)P(Z,Y)$$   

概率分布可用作势函数。然而，在这种情况下，我们不能让所有的势都是边际概率或条件概率。所以这个图的势函数不能是概率分布。

## 吉布斯分布(Gibbs Distribution)与无向图形模型定义  

给定一个无向图$$H$$和与$$H$$团有关的团势函数$$\psi_{C}$$，我们说$$P$$是$$H$$上的Gibbs分布，当它可以表示为:   

$$P(X_{1},..X_{n})=\frac{1}{Z}\prod_{c\in C}\psi_{c}(\mathbf{x_{c}})$$   

其中$$Z$$又称为配分函数(partition function)。大写$$C$$表示所有团的集合，小写$$c$$表示与一组随机变量$$\mathbf{x}$$关联的团。  

无向图形模型通过无向图$$H$$,一组值为正的势函数$$\psi_{C}$$和$$H$$的相关团表示分布$$P(X_{1},..X_{n})$$：  

$$
P(X_{1},..X_{n})=\frac{1}{Z}\prod_{c\in C}\psi_{c}(\mathbf{x_{c}})\\
Z=\sum_{x_{1},..x_{n}}\prod_{c\in C}\psi_{c}(\mathbf{x_{c}})  
$$  

请注意，此分布是Gibbs分布。

## UGM Models 示例  

用于说明UGM的示例图：  

![_config.yml]({{ site.baseurl }}/images/10708/image42.png)   

**使用最大团(MAX CLIQUES):**      

![_config.yml]({{ site.baseurl }}/images/10708/image43.png)   

$$
P'(A,B,C,D)=\frac{1}{Z}\psi_{c}(A,B,D)\psi_{c}(B,C,D)\\
Z=\sum_{A,B,C,D}\psi_{c}(A,B,D)\psi_{c}(B,C,D)
$$

我们只需要用两个3D表而不是一个4D表来表示离散节点。 

**使用成对的团(PAIRWISE CLIQUES):**     

![_config.yml]({{ site.baseurl }}/images/10708/image44.png)  

$$
P''(A,B,C,D)=\frac{1}{Z}\psi_{c}(A,B)\psi_{c}(A,D)\psi_{c}(B,C)\psi_{c}(B,D)\psi_{c}(C,D)\\
Z=\sum_{A,B,C,D}\psi_{c}(A,B)\psi_{c}(A,D)\psi_{c}(B,C)\psi_{c}(B,D)\psi_{c}(C,D)
$$  

我们只需要用五个2D表而不是一个4D表来表示离散节点。  

**使用标准化表示(CANONICAL REPRESENTATION)**  

即使我们使用细粒度的表示，Markov网络也常常被过度参数化。对于任何给定的分布，模型中都有多种参数可供选择。如上图所示，我们可以选择max-cliques或pairwise-cliques来表示这个模型。此外，在团结构中可能出现歧义。例如，给定一对派系$$ \{A,B\}$$和$$\{B,C\}$$，关于$$B$$的信息可以放在这两个团中的任何一个，导致可以通过多种方式定义相同的分布。  

标准化表示提供了避免此问题的自然方法。它定义在所有非空的团上，如下所示。该公式执行包含排除计算，以确保每个随机变量只对整个函数贡献一次。  

$$
P'''(A,B,C,D)=\frac{1}{Z}\psi_{c}(A,B,D)\psi_{c}(B,C,D)\\
\times \psi_{c}(A,B)\psi_{c}(A,D)\psi_{c}(B,C)\psi_{c}(B,D)\psi_{c}(C,D)\\
\times \psi_{c}(A)\psi_{c}(B)\psi_{c}(C)\psi_{c}(D)
$$

$$
Z=\sum_{A,B,D}\psi_{c}(A,B,D)\psi_{c}(B,C,D)\\
\times \psi_{c}(A,B)\psi_{c}(A,D)\psi_{c}(B,C)\psi_{c}(B,D)\psi_{c}(C,D)\\
\times \psi_{c}(A)\psi_{c}(B)\psi_{c}(C)\psi_{c}(D)
$$

# 定性(Qualitative Specification)   

## 全局马尔可夫独立性  

假设给定以下UGM，用$$H$$表示：  

![_config.yml]({{ site.baseurl }}/images/10708/image45.png)  

如果从$$X$$中的节点到$$Z$$中的节点的每条路径都通过$$Y$$中的节点，则$$Y$$将$$X$$和$$Z$$分开：   

$$sepH(X;Z\mid Y)$$   

如果对于任何不相交的$$X,Y,Z$$使得$$Y$$将$$X$$和$$Z$$分开，在给定$$Y$$的情况下$$X$$独立于$$Z$$,则概率分布满足全局Markov属性$$I(H)$$。

$$I(H)=\{X\bot Z\mid Y:sep(X;Z\mid Y)\}$$  

##  局部马尔可夫独立性  

对于每一个节点$$X_{i}\in V$$,这里有$$X_{i}$$唯一的 Markov blanket，定义为$$MB_{X_{i}}$$,它表示与$$X_{i}$$相邻的节点。  

与$$H$$相关的local Markov independencies $$(I_{l})$$为：  

$$I_{l}(H):\{X_{i}\bot (V-\{X_{i}\}-MB_{X_{i}})\mid MB_{X_{i}}:\forall_{i}\}$$  

换句话说，给定$$X_{i}$$的相邻节点$$MB_{X_{i}}$$，$$X_{i}$$独立于其余节点

# 全局马尔可夫性质的可靠性和完整性  

UGM的全局Markov属性与DGM的变体相似，从某种意义上说，它们都具有相似的可靠性和完整性。

## 可靠性  

定理：设$$P$$是关于$$\chi$$的分布，$$g$$是关于$$\chi$$的马尔可夫网络结构。如何$$P$$是根据$$g$$因子分解的Gibbs分布，$$g$$是$$P$$的 I-map  

证明： $$X,Y,Z$$在$$\chi$$上三个互不相交的子集，在$$g$$上$$Z$$分离了$$X$$和$$Y$$。希望证明$$P\models (X\bot Y\mid Z)$$   

首先，我们观察到$$X$$到$$Y$$没有直接连接边。假设$$(X,Y,Z)$$是$$\chi$$上的分区， $$(X\cup Y\cup Z)=\chi$$，任何在$$g$$上的团可以完全通过$$X\cup Z$$ or $$Y\cup Z$$,$$I_{X}$$表示包含在$$X\cup Z$$一组团,$$I_{Y}$$表示包含在$$Y\cup Z$$一组团,已知：  

$$
P(X_{1},..X_{n})=\frac{1}{Z}\prod_{i\in I_{X}}\phi_{i}(D_{i})\cdot \prod_{i\in I_{Y}}\phi_{i}(D_{i})
$$

第一个乘法项都不包含$$Y$$变量,第二个乘法项都不包含$$X$$变量。因此，我们可以将乘法改写为： 

$$P(X_{1},..X_{n})=\frac{1}{Z}f(X,Z)g(Y,Z)$$  

则$$P\models(X\bot Y \mid Z)$$。 

现考虑$$X\cup Y\cup Z$$是$$\chi$$的严格子集。让$$U=\chi - (X\cup Y\cup Z)$$.将$$U$$分割成互不相交的$$U_{1}$$和$$U_{2}$$,$$Z$$在$$g$$分离了$$X\cup U_{1}$$和$$Y\cup U_{2}$$.利用我们在前面的论点，我们得到了$$((X\cup U_{1})\bot (Y\cup U_{2}))\mid Z$$应用概率的分解性质我们得到$$P\models(X\bot Y \mid Z)$$   

概率的分解性质：$$(X\bot Y,W\mid Z) \Rightarrow (X\bot Y \mid Z)$$   

## 完备性（Hammersley-Clifford定理）  

定理：设$$P$$是$$\chi$$上的正态分布，$$g$$是$$\chi$$上的Markov网络图，如果$$g$$是$$P$$的I-map，则$$P$$根据$$$g$因子分解的Gibbs分布。  

这一结果表明，对于正态分布，全局独立性意味着分布根据网络结构进行因子分解。因此，对于这类分布，我们有:一个分布$$P$$根据Markov网络$$g$$因子分解，当且仅当$$g$$是$$P$$的 I-map。

# 其他马尔可夫性质   

对于UGMs，我们根据全局Markov性质定义了 I-map。我们现在将定义局部独立。直观地说，当两个变量没有直接链接时，必须有某种方式使它们条件独立。特别地，我们可以要求$$X$$和$$Y$$独立于图中的所有其他节点。   

**成对独立(Pairwise Independencies)**  

定义$$g$$为马尔可夫网络。 我们将与$$g$$关联的成对独立性定义为:   

$$I_{P}(g)=\{(X\bot Y\mid \chi-\{X,Y\}):X-Y\notin g\}$$

成对独立的例子:  

![_config.yml]({{ site.baseurl }}/images/10708/image46.png)   

为了说明这个想法，请注意在上图中，给定图中所有其他节点$$\{B,C,D \}$$，变量$$A$$和$$E$$在条件上是独立的。   

成对和局部独立性也相关。 它们的关系在以下命题和定理中描述。  

**命题(Proposition)**  

1、对于任意的马尔可夫网络$$g$$,和任意的分布$$P$$,如果$$P\models I_{l}(g)$$则$$P\models I_{P}(g)$$   
2、对于任意的马尔可夫网络$$g$$,和任意的分布$$P$$,如果$$P\models I(g)$$则$$P\models I_{l}(g)$$
3、定义$$P$$为正态分布，如果P满足$$I_{P}(g)$$则$$P$$满足$$I(g)$$  

**定理**  

以下是正态分布$$P$$的等价项：  

$$
P\models I_{l}(g)\\
P\models I_{P}(g)\\
P\models I(g)
$$

# 指数形式   

由于我们不希望在所有情况下都限制团的势函数为正，所以使用无限制的形式实值“能量”函数$$\phi_{c}(x_{c})$$以指数形式来表示团的势函数$$\Phi_{c}(x_{c})$$,：  

$$\Phi_{c}(x_{c})=exp\{-\phi_{c}(x_{c})\}$$   

这就给了联合概率一个很好的加法结构   

$$p(x)=\frac{1}{Z}exp\{-\sum_{c\in C}\phi_{c}(x_{c})\}=\frac{1}{Z}exp\{-H(x)\}$$  

指数中的和称为“free energy”:  

$$H(x)=\sum_{c\in C}\phi_{c}(x_{c})$$  

这种表示形式在物理学中称为“玻尔兹曼分布”，在统计中称为对数线性模型。  

# 无向图示例   

在本节中，我们将介绍几个著名的无向图形模型：Boltzmann机（BM）、Ising模型、受限Boltzmann机（RBM）和条件随机场（CRF）。 

**Boltzmann Machine (BM)**  

Boltzmann机是一个二值节点上具有成对（边）势的完全连通图。下图显示了一个示例：  

![_config.yml]({{ site.baseurl }}/images/10708/image47.png)   

其概率分布可以写成： 

$$p(x_{1},x_{2},x_{3},x_{4})=\frac{1}{Z}exp\{-\sum_{ij}\phi_{i,j}(x_{i},x_{j})\}$$   

也可以用二次方的形式编写：  

$$p(x_{1},x_{2},x_{3},x_{4})=\frac{1}{Z}exp\{\sum_{ij}\theta_{ij}x_{i}x_{j}+\sum_{i}\alpha_{i}x_{i}+C\}$$   

因此，总体自由能量函数具有以下形式：  

$$H(x)=\sum_{ij}(x_{i}-\mu)\Theta_{ij}(x_{j}-\mu)=(x-\mu)^{\top}\Theta(x-\mu)$$

然后用二次规划法求解。  

**Ising model**  

在Ising模型中，节点被安排在一个规则的拓扑结构中（通常是一个规则的填充网格），并且只与它们的几何邻域相连。它就像一台稀疏的玻尔兹曼机器。还有一种多状态Ising模型（也称为Potts模型），其中节点可以接受多个值，而不仅仅是二进制值。Ising模型的一个例子如下图所示：   

![_config.yml]({{ site.baseurl }}/images/10708/image48.png)   

它的概率分布可以写成:   

$$p(X)=\frac{1}{Z}exp\{\sum_{ij\in N_{i}}\theta_{ij}X_{i}X_{j}+\sum_{i}\alpha_{i}X_{i}\}$$ 

**Restricted Boltzmann Machine (RBM)**  

受限Bolzmann机是一个二部图，它在一层隐藏单元和一层可见单元之间有联系。下图显示了一个示例：  

![_config.yml]({{ site.baseurl }}/images/10708/image49.png)   

它的概率分布可以写成:   

$$p(x,h\mid \theta)=\frac{1}{Z}exp\{\sum_{i}\theta_{i}\phi_{i}(x_{i})+\sum_{j}\theta_{j}\phi_{j}(x_{j})+\sum_{i,j}\theta_{i,j}\phi_{i,j}(x_{j},h_{j})-A(\theta)\}$$

RBM有一些吸引人的特性。例如，因素是边际依赖性的(marginally dependent)，给定可见节点上的观测值，因素是条件独立的。它们使人们能够使用迭代Gibbs抽样对RBM进行推理和学习。如果RBM中的边是有向的，则图中会有大量的V-结构（许多依赖关系）增加推理的难度。

marginal independence:  

![_config.yml]({{ site.baseurl }}/images/10708/image51.png)   

**Conditional Random Field (CRF)**    

条件随机场是无向情形下HMM的一种类似形式。对输入允许任意依赖。例如，当标记$$X_{i}$$时，可以考虑到未来的观察结果。。CRF示例如图所示：

![_config.yml]({{ site.baseurl }}/images/10708/image50.png) 

概率分布可以写成：  

$$p_{\theta}(y\mid x)=\frac{1}{Z}exp\{\sum_{c}\theta_{c}f_{c(x,y_{c})}\}$$   

参考：
[Lecture 2: Bayesian Networks](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-02/)



