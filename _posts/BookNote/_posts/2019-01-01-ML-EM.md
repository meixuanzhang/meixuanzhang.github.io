---
layout: post
title: EM算法
date:   2019-01-01
categories: 统计学习
---
# 隐变量定义
在统计学中，隐变量与可观察变量相对，是不直接观察但是通过观察到的其他变量推断的变量。平常模型中遇见的X(输入)，Y(输出)都是可观察到的，是可观察变量。  
# Soft EM&Hard EM
Nocation:  
$$Y_{j}$$：可观测到的变量,这里的$$Y_{j}=(y_{j1},y_{j2},y_{j3}..y_{js})$$是s维随机变量。  
$$Z_{j}$$：隐变量，是k维随机变量$$Z_{j}=(z_{j1},z_{j2},z_{j3}..z_{jk})$$  是只有一个维度为1，其他为0的变量   
$$\theta$$:参数  
$$Y=Y_{1},Y_{2}...Y_n,Z=Z_{1},Z_{2}...Z_n$$ 有n个样本   

**Soft EM**  

目标函数(未取对数)：   

$$L(\theta)=P(Y;\theta)=\sum\limits_{z_{1}..z_{n}}P(Y,Z; \theta)$$  


$$
\theta^{(*)}=\mathop{\arg\max}_{\theta} \sum\limits_{z_{1}..z_{n}} P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta)\\
=P(Y_{1}...Y_{n})
$$  

流程：  
+ 初始化$$\theta$$
+ 重复下面流程直到$$P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta)$$收敛    
   + $$\rho(z_{1}..z_{n}):=P(Z_{1}...Z_{n} \mid Y_{1}...Y_{n};\theta)$$(The E step)
   + $$\theta :=$$ The best fit of $$\theta$$ to $$Y_{1}...Y_{n}$$ and the distribution $$\rho$$ on $$Z_{1}...Z_{n}$$ (The M step)

**Hard EM**  

目标函数(未取对数)：  

$$L(\theta)=P(Y,Z;\theta)$$

$$
\theta^{(*)}=\mathop{\arg\max}_{\theta} \mathop{\max}_{z_{1}..z_{n}} P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta)
$$  

流程：  
+ 初始化$$\theta$$
+ 重复下面流程直到$$P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta)$$收敛    
   + $$(z_{1}..z_{n}) :=\mathop{\arg\max}_{z_{1}..z_{n}}P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta) $$  
   + $$\theta :=\mathop{\arg\max}_{\theta}P(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta) $$    

**注意两种方法目标函数差异**  
 
## Soft EM算法的推导  

**概率模型假设：**  

$$P(Y_{j};\theta)=\sum\limits_{z_{j}}P(Y_{j},Z_{j}; \theta)=\sum\limits_{z_{j}}P(Z_{j};\theta)P(Y_{j}|Z_{j}; \theta)$$  

+ 公式涉及了贝叶斯定理   
+ 含有隐变量的概率模型我们将$$Y_{j}$$，$$Z_{j}$$称为完全数据，只有$$Y_{j}$$称为不完全数据    
+ 注意这里的 $$\theta$$ 是我们待估计的参数，是常数，如果是随机变量公式应写成  
$$P(Y_{j}| \theta)$$，但有些书本不会加以区别。  

**概率模型目标函数：极大似然函数**  

$$L(\theta)=logP(Y;\theta)=log\sum\limits_{z_{1}..z_{n}}P(Y,Z; \theta)=log(\sum\limits_{z_{1}..z_{n}}P(Z;\theta)P(Y|Z; \theta))$$

注意这里$$Y=Y_{1},Y_{2}...Y_n,Z=Z_{1},Z_{2}...Z_n$$ 有n个样本   
如果不含有隐变量，模型可以直接参用极大似然估计估计参数，含有则需要使用EM算法估计参数,因为式子中含有未观测的数据并有包含和的对数。

**算法推导过程**  

EM算法是通过迭代逐步近似极大化$$L(\theta)$$的，假设在第i次迭代后$$\theta$$的估计值$$\theta^{(i)}$$.我们希望新估计值使$$L(\theta)>L(\theta^{(i)})$$,并逐步达到极大值，为此两者差：    

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z_{1}..z_{n}}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

令：  

$$log(\sum\limits_{z_{1}..z_{n}}P(Y|Z;\theta)P(Z;\theta))=log(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$C(Z)=\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}$$    

则：  

$$log(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$= log( \sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) C(Z))$$  

$$= logE_{z_{1}..z_{n}}(C(Z)|Y;\theta^{(i)})$$  

下面式子会在后面推导用到：  

$$(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}) $$  

$$= (\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})logC(Z))  $$  

$$= E_{z_{1}..z_{n}}(logC(Z)|Y;\theta^{(i)})$$ 

根据jesen不等式：
$$f(E(x)) \ge E(f(x))$$  

设f(x)=log(x)，则:  

$$logE_{z_{1}..z_{n}}(C(Z)|Y;\theta^{(i)}) \ge E_{z_{1}..z_{n}}(logC(Z)|Y;\theta^{(i)})$$  

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z_{1}..z_{n}}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

$$ \ge (\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})-logP(Y;\theta^{(i)})$$  

$$=(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

令：  

$$B( \theta,\theta^{(i)}) = L(\theta^{(i)})+(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})}) $$  

则：  

$$L(\theta) \ge B(\theta,\theta^{(i)}) $$  

函数$$B(\theta,\theta^{(i)})$$是$$L(\theta)$$下界，而且$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$(log1=0)  
因此，任何使$$B(\theta,\theta^{(i)})$$增大的$$\theta$$，也可以使$$L(\theta)$$增大，为了使$$L(\theta)$$尽可能的增大，选择$$\theta^{(i+1)}$$使   
$$B(\theta,\theta^{(i)})$$达到极大值，即：  

$$\theta^{(i+1)}=\mathop{\arg\max}_{\theta}B(\theta,\theta^{(i)})$$

求$$\theta^{(i+1)}$$的表达式，省去对$$\theta$$而言是常数的项  

$$\theta^{(i+1)}=\mathop{\arg\max}_{\theta}(L(\theta^{(i)})+(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

$$=\mathop{\arg\max}_{\theta}(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})logP(Y|Z; \theta)P(Z;\theta)) $$  

$$=\mathop{\arg\max}_{\theta}(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta))$$  

$$=\mathop{\arg\max}_{\theta}E_{z_{1}..z_{n}}(logP(Y,Z; \theta)|Y;\theta^{(i)})$$  

### Soft例子:高斯混合模型GMM

**高斯模型/正态分布模型**  

$$\phi(Y_{j};\theta_{k}) = \frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y_{j}-\mu_{k})^2}{2\sigma_{k}^2})$$

下标k表示第k个模型，Y可以是一维的,这里我们设定Y是多维度变量，$$\mu ,\sigma$$也是多维的，每个维度会有自己的均值和方差, 这样模型应称为多元高斯模型

**高斯混合模型**  

高斯混合模型是指具有如下形式的概率分布模型:(式子中的k代表混合了k个(多元)高斯模型)  

$$P(Y_{j};\theta) = \sum\limits_{k=1}^K \alpha_{k}\phi(Y_{j};\theta_{k})$$  

其中$$\alpha_{k}$$是系数，$$\alpha_{k} \ge 0, \sum\limits_{k=1}^K \alpha_{k}=1;\phi(Y_{j};\theta_{k})$$是高斯分布密度，  
$$\theta_{k}=(\mu_{k},\sigma_{k}^2), \theta=(\alpha_{1},\alpha_{2},...\alpha_{k};\theta_{1},\theta_{2}...\theta_{k})$$  
我们用EM算法估计高斯混合模型的参数$$\theta$$  

**1、明确隐变量，写出完全数据的对数似然函数**  

假设观测数据$$Y_{j},j=1,2,...N$$(j是样本编码),是依据概率$$\alpha_{k}$$选择第k个高斯分布模型 ，  
然后依第k个分模型的概率分布$$\phi(Y;\theta_{k})$$生成的(观测数据$$Y_{j}$$)。  
这时观测数据$$Y_{j}$$是已知的，但其来自第k个分模型的数据是未知的，以隐变量$$Z_{j}$$表示其来自的第k个分模型    
$$Z_{j}=(z_{j1},z_{j2}...z_{jk})$$  

$$
z_{jk} = \left\{ \begin{array}{rl}
1 &\mbox{ 第j个观测来自第k个分模型} \\
0 &\mbox{ 否则}
\end{array} \right.
$$  

写出完全数据$$(Y_{j},Z_{j})$$的似然函数:  

$$
P(Y,Z;\theta)=\prod\limits_{j=1}^N P(Y_{j},Z_{j}; \theta)\\
=\prod\limits_{j=1}^N P(Y_{j},z_{j1},z_{j2}...z_{jk}; \theta)\\
=\prod\limits_{k=1}^K \prod\limits_{j=1}^N[\alpha_{k}\phi(Y_{j};\theta_{k})]^{z_{jk}}\\
= \prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\phi(Y_{j};\theta_{k})]^{z_{jk}}\\
=\prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y_{j}-\mu_{k})^2}{2\sigma_{k}^2})]^{z_{jk}}
$$  

注意式子中的k,因为Z是onehot向量,当$$z_{jk}$$为0时，指数为0，$$[\alpha_{k}\phi(Y_{j};\theta_{k})]^{z_{jk}}$$为1,  

式中，$$n_k=\sum\limits_{j=1}^N z_{jk},\sum\limits_{k=1}^K n_k=N$$  

完全数据的对数似然函数为  

$$logP(Y,Z;\theta)=\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]$$   


**EM算法的E步，确定目标函数Q**  

$$
Q=E_{z_{1}..z_{n}}(logP(Y,Z; \theta)|Y;\theta^{(i)})\\
= \sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta)\\
=E_{z_{1}..z_{n}}\lbrace\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace
$$    

因为$$E_{X,Y}(X+Y)=E_{X}(X)+E_{Y}(Y)$$


$$
Q = \sum\limits_{k=1}^{K}\sum\limits_{j=1}^N {E(z_{jk})} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N {E(z_{jk})}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\\
=\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N \lbrace {E(z_{jk})} log\alpha_{k}+P_{jk}{E(z_{jk})}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace\\
$$  

令：

$$b_{k}=\sum\limits_{j=1}^N {E(z_{jk})}$$    

则：  

$$
Q=\sum\limits_{k=1}^{K}b_{k}log\alpha_{k}+ \sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{E(z_{jk})}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]
$$

为了确定Q，需要计算$$E(z_{jk})$$

$$
E(z_{jk})=P(z_{jk}=1 |Y;\theta^{(i)})\\
=\frac{P(z_{jk}=1,Y ;\theta^{(i)})}{P(Y;\theta^{(i)})}\\
=\frac{P(z_{jk}=1,Y_{j};\theta^{(i)})}{P(Y_{j};\theta^{(i)})}\\
=\frac{\alpha_{k}\phi(Y_{j};\theta_{k})}{\sum\limits_{k=1}^K \alpha_{k}\phi(Y_{j};\theta_{k})}
$$    

因为：      

$$
P(z_{jk}=1,Y;\theta^{(i)})=P(z_{jk}=1,Y_{j},Y_{1}..Y{j-1},Y{j+1}..;\theta^{(i)})\\
=P(z_{jk}=1,Y_{j};\theta^{(i)})P(Y_{1}..Y{j-1},Y{j+1}..;\theta^{(i)})\\
=P(z_{jk}=1,Y_{j}|\theta^{(i)})P(Y_{1};\theta^{(i)})..P(Y_{j-1};\theta^{(i)})P(Y_{j+1};\theta^{(i)})...
$$  


$$
P(Y;\theta^{(i)})=P(Y_{j};\theta^{(i)})P(Y_{1};\theta^{(i)})..P(Y_{j-1};\theta^{(i)})P(Y_{j+1};\theta^{(i)})...\\
$$  

所以：  

$$
\frac{P(z_{jk}=1,Y |;\theta^{(i)})}{P(Y;\theta^{(i)})}=\frac{P(z_{jk}=1,Y_{j} |;\theta^{(i)})}{P(Y_{j};\theta^{(i)})}\\
$$  

**确定EM算法的M步**  

迭代的M步是求函数Q对$$\theta$$的极大值，即求新一轮迭代的模型参数： 

$$\theta^{(i+1)}=\mathop{\arg\max}_{\theta}Q(\theta,\theta^{(i+1)})$$  

用$$\mu_{k},\sigma_{k}^2,\alpha_{k},k=1,2...K$$表示$$\theta^{(i+1)}$$参数，分别对$$\mu_{k},\sigma_{k}^2$$求偏导数并令其为0，  
得到$$\mu_{k},\sigma_{k}^2$$，在$$\sum\limits_{k=1}^K \alpha_{k}=1$$条件下对$$\alpha_{k}$$求偏导数令其为0，得到$$\alpha_{k}$$,结果如下：   

$$\mu_{k}=\frac{\sum\limits_{j=1}^N E(z_{jk}) Y_{j}}{\sum\limits_{j=1}^N E(z_{jk})} ,k=1,2...K$$  

$$\sigma_{k}^2=\frac{\sum\limits_{j=1}^N E(z_{jk})(Y_{j}-\mu_{k})^2}{\sum\limits_{j=1}^N E(z_{jk})} ,k=1,2...K$$  

$$\alpha_{k}=\frac{\sum\limits_{j=1}^N E(z_{jk}) }{N} ,k=1,2...K\\$$  

## Hard EM算法的推导    

**概率模型目标函数：极大似然函数：**  

$$L(\theta)=logP(Y,Z;\theta)\\
=logP(Z \mid Y;\theta)P(Y;\theta)$$    

**算法推到过程**

$$\theta^{(i+1)}=\mathop{\arg\max}_{\theta} logP(Y_{1}...Y_{n},Z_{1}...Z_{n};\theta)$$  

### Hard例子:高斯混合模型GMM

**1、明确隐变量，写出完全数据的对数似然函数**

推导看Soft GMM：  

$$logP(Y,Z;\theta)=\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]$$   


**EM算法的E步，确定目标函数Q**

为了确定Q，需要知道$$z_{jk}$$的值

$$
P(Y,Z; \theta)=P(Z \mid Y;\theta)P(Y;\theta)\\
Z^*=\mathop{\arg\max}_{z_{1}..z_{n}}P(Y,Z; \theta)\\
=\mathop{\arg\max}_{z_{1}..z_{n}}P(Z \mid Y;\theta)P(Y;\theta)\\
=\mathop{\arg\max}_{z_{1}..z_{n}}P(Z \mid Y;\theta)\\
P(Z_{1},Z_{2}..Z_{n} \mid Y;\theta)=P(Z_{1}\mid Y;\theta)P(Z_{2}\mid Y;\theta)..P(Z_{n} \mid Y;\theta)
$$  

注意$$P(Y;\theta)$$初始化参数后是常数。    

$$
Z_{1}^*=\mathop{\arg\max}_{z_{1}}P(Z_{1}\mid Y;\theta)\\
Z_{2}^*=\mathop{\arg\max}_{z_{2}}P(Z_{2}\mid Y;\theta)
....
$$  

由于$$P(Z_{1}\mid Y;\theta)=P(z_{11},z_{12}..z_{1k}\mid Y;\theta)，z_{11},z_{12}..z_{1k}$$是0,1向量，    
$$P(Z_{1}\mid Y;\theta)=P(z_{1k}=1,other=0\mid Y;\theta),k=1,2,...K$$,对于样本1,哪个元素$$(z_{11},z_{12}..z_{1k})$$取1会使概率最大则取1，其他元素取0。  
由此类推计算每个样本j,哪个取1会使概率最大则取1，其他元素取0。确定每个$$z_{jk}$$的值。   

$$N_{k}=\sum\limits_{j=1}^{N}\mid z_{jk}=1\mid   ,k=1,2...K$$  属于k类的样本个数   

**确定EM算法的M步**  

迭代的M步是求函数Q对$$\theta$$的极大值，即求新一轮迭代的模型参数：  
分别对$$\mu_{k},\sigma_{k}^2$$求偏导数并令其为0，  
得到$$\mu_{k},\sigma_{k}^2$$，在$$\sum\limits_{k=1}^K \alpha_{k}=1$$条件下对$$\alpha_{k}$$求偏导数令其为0，得到$$\alpha_{k}$$,结果如下： 

$$\mu_{k}=\frac{\sum\limits_{j:z_{jk}=1} Y_{j}}{N_{k}} ,k=1,2...K$$     

分子是样本为k类的$$Y_{j}$$的和    

$$\sigma_{k}^2=\frac{\sum\limits_{j:z_{jk}=1} (Y_{j}-\mu_{k})^2}{N_{k}} ,k=1,2...K$$  

$$\alpha_{k}=\frac{N_{k}}{N} ,k=1,2...K\\$$    

### Hard例子:k-mean问题

给定一个n个对象的数据集，它可以构建数据的k个划分，每个划分就是一个簇，并且 k ≤ n。同时还需满足：  
每个组至少包含一个对象   
每个对象必须属于且仅属于一个簇。   

**1、明确隐变量，写出目标函数**    

k-mean问题迭代算法中，目标函数与GMM不同，不是似然函数，这里我们使用最小平方误差作为目标函数则：   

$$Z_{j}$$：隐变量，是k维随机变量$$Z_{j}=(z_{j1},z_{j2},z_{j3}..z_{jk})$$  是只有一个维度为1，其他为0的变量，代表样本属于的簇    

$$\theta=<\mu_{1},\mu_{2},...\mu_{k}>$$    

$$Q = \sum\limits_{j=1}^N (Y_{j}-\mu_{Z_{j}})^2$$    

$$\theta^*=\mathop{\arg\min}_{\mu_{1}..\mu_{k}} \mathop{\min}_{z_{1}..z_{n}}\sum\limits_{j=1}^N (Y_{j}-\mu_{Z_{j}})^2$$  

$$\theta^*=\mathop{\arg\min}_{\mu_{1}..\mu_{k}} \mathop{\min}_{z_{1}..z_{n}}\sum\limits_{j=1}^N (Y_{j}-\mu_{k:z_{jk}=1})^2,k=1,2,..K$$  

**EM算法的E步，确定目标函数Q**    

为了确定Q，需要知道$$z_{jk}$$的值   

$$Z^* = \mathop{\min}_{z_{1}..z_{n}}\sum\limits_{j=1}^N (Y_{j}-\mu_{Z_{j}})^2$$   

$$Z_{j}^* = \mathop{\min}_{z_{j}} (Y_{j}-\mu_{Z_{j}})^2 $$       

对于样本j,哪个元素$$(z_{j1},z_{j2}..z_{jk})$$取1会使值式子最小则取1，其他元素取0。  
由此类推计算每个样本,哪个取1会使概率最大则取1，其他元素取0。确定每个$$z_{jk}$$的值。   

**确定EM算法的M步**   

$$\theta^*=\mathop{\arg\min}_{\mu_{1}..\mu_{k}}\sum\limits_{j=1}^N (Y_{j}-\mu_{(k:z_{jk}=1)})^2$$

为了确定Q，需要知道$$z_{jk}$$的值
## HMM
## LDA     

来源：李航《统计学习》
