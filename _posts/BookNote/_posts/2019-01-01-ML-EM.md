---
layout: post
title: EM算法
date:   2019-01-01
categories: 统计学习
---
# 隐变量定义
在统计学中，隐变量与可观察变量相对，是不直接观察但是通过观察到的其他变量推断的变量。平常模型中遇见的X(输入)，Y(输出)都是可观察到的，是可观察变量。  
# EM算法的推导
Nocation:  
$$Y_{j}$$：可观测到的变量,这里的$$Y_{j}=(y_{j1},y_{j2},y_{j3}..y_{js})$$是s维随机变量。  
$$Z_{j}$$：隐变量，是k维随机变量$$Z_{j}=(z_{j1},z_{j2},z_{j3}..z_{jk})$$  
$$\theta$$:参数  
$$Y=Y_{1},Y_{2}...Y_n,Z=Z_{1},Z_{2}...Z_n$$ 有n个样本 
**Soft EM**  
目标函数：   
$$L(\theta)=logP(Y;\theta)=log\sum\limits_{z_{1}..z_{n}}P(Y,Z; \theta)$$


**Hard EM**
目标函数：  
$$L(\theta)=logP(Y,Z;\theta)$$


**概率模型假设：**  

$$P(Y_{j};\theta)=\sum\limits_{z_{j}}P(Y_{j},Z_{j}; \theta)=\sum\limits_{z_{j}}P(Z_{j};\theta)P(Y_{j}|Z_{j}; \theta)$$  

+ 公式涉及了贝叶斯定理   
+ 含有隐变量的概率模型我们将$$Y_{j}$$，$$Z_{j}$$称为完全数据，只有$$Y_{j}$$称为不完全数据    
+ 注意这里的 $$\theta$$ 是我们待估计的参数，是常数，如果是随机变量公式应写成  
$$P(Y_{j}| \theta)$$，但有些书本不会加以区别。  

**概率模型目标函数：极大似然函数**  

$$L(\theta)=logP(Y;\theta)=log\sum\limits_{z_{1}..z_{n}}P(Y,Z; \theta)=log(\sum\limits_{z_{1}..z_{n}}P(Z;\theta)P(Y|Z; \theta))$$

注意这里$$Y=Y_{1},Y_{2}...Y_n,Z=Z_{1},Z_{2}...Z_n$$ 有n个样本   
如果不含有隐变量，模型可以直接参用极大似然估计估计参数，含有则需要使用EM算法估计参数,因为式子中含有未观测的数据并有包含和的对数。

**算法推导过程**  

EM算法是通过迭代逐步近似极大化$$L(\theta)$$的，假设在第i次迭代后$$\theta$$的估计值$$\theta^{(i)}$$.我们希望新估计值使$$L(\theta)>L(\theta^{(i)})$$,并逐步达到极大值，为此两者差：    

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z_{1}..z_{n}}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

令：  

$$log(\sum\limits_{z_{1}..z_{n}}P(Y|Z;\theta)P(Z;\theta))=log(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$C(Z)=\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}$$    

则：  

$$log(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$= log( \sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) C(Z))$$  

$$= logE_{z_{1}..z_{n}}(C(Z)|Y;\theta^{(i)})$$  

下面式子会在后面推导用到：  

$$(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}) $$  

$$= (\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})logC(Z))  $$  

$$= E_{z_{1}..z_{n}}(logC(Z)|Y;\theta^{(i)})$$ 

根据jesen不等式：
$$f(E(x)) \ge E(f(x))$$  

设f(x)=log(x)，则:  

$$logE_{z_{1}..z_{n}}(C(Z)|Y;\theta^{(i)}) \ge E_{z_{1}..z_{n}}(logC(Z)|Y;\theta^{(i)})$$  

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z_{1}..z_{n}}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

$$ \ge (\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})-logP(Y;\theta^{(i)})$$  

$$=(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

令：  

$$B( \theta,\theta^{(i)}) = L(\theta^{(i)})+(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})}) $$  

则：  

$$L(\theta) \ge B(\theta,\theta^{(i)}) $$  

函数$$B(\theta,\theta^{(i)})$$是$$L(\theta)$$下界，而且$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$(log1=0)  
因此，任何使$$B(\theta,\theta^{(i)})$$增大的$$\theta$$，也可以使$$L(\theta)$$增大，为了使$$L(\theta)$$尽可能的增大，选择$$\theta^{(i+1)}$$使   
$$B(\theta,\theta^{(i)})$$达到极大值，即：  

$$\theta^{(i+1)}=\mathop{\arg\min}_{\theta}B(\theta,\theta^{(i)})$$

求$$\theta^{(i+1)}$$的表达式，省去对$$\theta$$而言是常数的项  

$$\theta^{(i+1)}=\mathop{\arg\min}_{\theta}(L(\theta^{(i)})+(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

$$=\mathop{\arg\min}_{\theta}(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)})logP(Y|Z; \theta)P(Z;\theta)) $$  

$$=\mathop{\arg\min}_{\theta}(\sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta))$$  

$$=\mathop{\arg\min}_{\theta}E_{z_{1}..z_{n}}(logP(Y,Z; \theta)|Y;\theta^{(i)})$$  


# 例子
## 高斯混合模型GMM

**高斯模型/正态分布模型**  

$$\phi(Y_{j};\theta_{k}) = \frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y_{j}-\mu_{k})^2}{2\sigma_{k}^2})$$

下标k表示第k个模型，Y可以是一维的,这里我们设定Y是多维度变量，$$\mu ,\sigma$$也是多维的，每个维度会有自己的均值和方差, 这样模型应称为多元高斯模型

**高斯混合模型**  

高斯混合模型是指具有如下形式的概率分布模型:(式子中的k代表混合了k个(多元)高斯模型)  

$$P(Y_{j};\theta) = \sum\limits_{k=1}^K \alpha_{k}\phi(Y_{j};\theta_{k})$$  

其中$$\alpha_{k}$$是系数，$$\alpha_{k} \ge 0, \sum\limits_{k=1}^K \alpha_{k}=1;\phi(Y_{j};\theta_{k})$$是高斯分布密度，  
$$\theta_{k}=(\mu_{k},\sigma_{k}^2), \theta=(\alpha_{1},\alpha_{2},...\alpha_{k};\theta_{1},\theta_{2}...\theta_{k})$$  
我们用EM算法估计高斯混合模型的参数$$\theta$$  

**1、明确隐变量，写出完全数据的对数似然函数**  

假设观测数据$$Y_{j},j=1,2,...N$$(j是样本编码),是依据概率$$\alpha_{k}$$选择第k个高斯分布模型 ，  
然后依第k个分模型的概率分布$$\phi(Y;\theta_{k})$$生成的(观测数据$$Y_{j}$$)。  
这时观测数据$$Y_{j}$$是已知的，但其来自第k个分模型的数据是未知的，以隐变量$$Z_{j}$$表示其来自的第k个分模型    
$$Z_{j}=(z_{j1},z_{j2}...z_{jk})$$  

$$
z_{jk} = \left\{ \begin{array}{rl}
1 &\mbox{ 第j个观测来自第k个分模型} \\
0 &\mbox{ 否则}
\end{array} \right.
$$  

写出完全数据$$(Y_{j},Z_{j})$$的似然函数:  

$$
P(Y,Z;\theta)=\prod\limits_{j=1}^N P(Y_{j},Z_{j}; \theta)\\
=\prod\limits_{j=1}^N P(Y_{j},z_{j1},z_{j2}...z_{jk}; \theta)\\
=\prod\limits_{k=1}^K \prod\limits_{j=1}^N[\alpha_{k}\phi(Y_{j};\theta_{k})]^{z_{jk}}\\
= \prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\phi(Y_{j};\theta_{k})]^{z_{jk}}\\
=\prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y_{j}-\mu_{k})^2}{2\sigma_{k}^2})]^{z_{jk}}
$$  

注意式子中的k,因为Z是onehot向量,当$$z_{jk}$$为0时，指数为0，$$[\alpha_{k}\phi(Y_{j};\theta_{k})]^{z_{jk}}$$为1,  

式中，$$n_k=\sum\limits_{j=1}^N z_{jk},\sum\limits_{k=1}^K n_k=N$$  

完全数据的对数似然函数为  

$$logP(Y,Z;\theta)=\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]$$   


**EM算法的E步，确定目标函数Q**  

$$
Q=E_{z_{1}..z_{n}}(logP(Y,Z; \theta)|Y;\theta^{(i)})\\
= \sum\limits_{z_{1}..z_{n}}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta)\\
=E_{z_{1}..z_{n}}\lbrace\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace
$$    

当$$z_{jk}=0$$时，$$logP(Y,Z; \theta)$$里与$$z_{jk}$$相关的项都为零，

$$logP(Y,Z; \theta)=logP(Y,z_{jk}=1; \theta)$$

令:  

$$P_{jk}=P(z_{jk}=1 |Y;\theta^{(i)})$$  


$$
Q = \sum\limits_{k=1}^{K}\sum\limits_{j=1}^N P_{jk} \lbrace\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}} log\alpha_{k}+\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace\\
=\sum\limits_{k=1}^{K}\sum\limits_{j=1}^N \lbrace P_{jk}{z_{jk}} log\alpha_{k}+P_{jk}{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace\\
$$  

令：

$$b_{k}=\sum\limits_{j=1}^N P_{jk}{z_{jk}}$$    

则：  

$$
Q=\sum\limits_{k=1}^{K}b_{k}log\alpha_{k}+ \sum\limits_{k=1}^{K}\sum\limits_{j=1}^NP_{jk}{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]
$$

为了确定Q，需要计算$$P_{jk}{z_{jk}}$$

$$
P_{jk}z_{jk}=P(z_{jk}=1 |Y;\theta^{(i)})\\
=\frac{P(z_{jk}=1,Y ;\theta^{(i)})}{P(Y;\theta^{(i)})}\\
=\frac{P(z_{jk}=1,Y_{j};\theta^{(i)})}{P(Y_{j};\theta^{(i)})}\\
=\frac{\alpha_{k}\phi(Y_{j};\theta_{k})}{\sum\limits_{k=1}^K \alpha_{k}\phi(Y_{j};\theta_{k})}
$$    

因为：      

$$
P(z_{jk}=1,Y;\theta^{(i)})=P(z_{jk}=1,Y_{j},Y_{1}..Y{j-1},Y{j+1}..;\theta^{(i)})\\
=P(z_{jk}=1,Y_{j};\theta^{(i)})P(Y_{1}..Y{j-1},Y{j+1}..;\theta^{(i)})\\
=P(z_{jk}=1,Y_{j}|\theta^{(i)})P(Y_{1};\theta^{(i)})..P(Y_{j-1};\theta^{(i)})P(Y_{j+1};\theta^{(i)})...
$$  


$$
P(Y;\theta^{(i)})=P(Y_{j};\theta^{(i)})P(Y_{1};\theta^{(i)})..P(Y_{j-1};\theta^{(i)})P(Y_{j+1};\theta^{(i)})...\\
$$  

所以：  

$$
\frac{P(z_{jk}=1,Y |;\theta^{(i)})}{P(Y;\theta^{(i)})}=\frac{P(z_{jk}=1,Y_{j} |;\theta^{(i)})}{P(Y_{j};\theta^{(i)})}\\
$$  

**确定EM算法的M步**  

## k-mean问题
给定一个 n 个对象的数据集，它可以构建数据的 k 个划分，每个划分就是一个簇，并且 k ≤ n。同时还需满足：  
每个组至少包含一个对象。
每个对象必须属于且仅属于一个簇。

## HMM
## LDA     

来源：李航《统计学习》
