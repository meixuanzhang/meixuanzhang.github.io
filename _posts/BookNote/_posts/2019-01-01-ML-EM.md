---
layout: post
title: EM算法
date:   2019-01-01
categories: 统计学习
---
# 隐变量定义
在统计学中，隐变量与可观察变量相对，是不直接观察但是通过观察到的其他变量推断的变量。平常模型中遇见的X(输入)，Y(输出)都是可观察到的，是可观察变量。  
# EM算法的推导
Nocation:  
Y：可观测到的变量,这里的$$Y=(y_{1},y_{2},y_{3}..y_{n})$$是多维随机变量。  
Z：隐变量，也是多维随机变量$$Z=(z_{1},z_{2},z_{3}..z_{n})$$  
$$\theta$$:参数  

**概率模型假设：**  

$$P(Y;\theta)=\sum\limits_{z}P(Y,Z; \theta)=\sum\limits_{z}P(Z;\theta)P(Y|Z; \theta)$$  

+ 公式涉及了贝叶斯定理   
+ 含有隐变量的概率模型我们将Y，Z称为完全数据，只有Y称为不完全数据    
+ 注意这里的 $$\theta$$ 是我们待估计的参数，是常数，如果是随机变量公式应写成  
$$P(Y| \theta)$$，但有些书本不会加以区别。  

**概率模型目标函数：极大似然函数**  

$$L(\theta)=logP(Y;\theta)=log\sum\limits_{z}P(Y,Z; \theta)=log(\sum\limits_{z}P(Z;\theta)P(Y|Z; \theta))$$

如果不含有隐变量，模型可以直接参用极大似然估计估计参数，含有则需要使用EM算法估计参数,因为式子中含有未观测的数据并有包含和的对数。

**算法推导过程**  

EM算法是通过迭代逐步近似极大化$$L(\theta)$$的，假设在第i次迭代后$$\theta$$的估计值$$\theta^{(i)}$$.我们希望新估计值使$$L(\theta)>L(\theta^{(i)})$$,并逐步达到极大值，为此两者差：    

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

令：  

$$log(\sum\limits_{z}P(Y|Z;\theta)P(Z;\theta))=log(\sum\limits_{z}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$C(Z)=\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}$$    

则：  

$$log(\sum\limits_{z}P(Z|Y;\theta^{(i)})\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})$$  

$$= log( \sum\limits_{z}P(Z|Y;\theta^{(i)}) C(Z))$$  

$$= logE_{z}(C(Z)|Y;\theta^{(i)})$$  

下面式子会在后面推导用到：  

$$(\sum\limits_{z}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})}) $$  

$$= (\sum\limits_{z}P(Z|Y;\theta^{(i)})logC(Z))  $$  

$$= E_{z}(logC(Z)|Y;\theta^{(i)})$$ 

根据jesen不等式：
$$f(E(x)) \ge E(f(x))$$  

设f(x)=log(x)，则:  

$$logE_{z}(C(Z)|Y;\theta^{(i)}) \ge E_{z}(logC(Z)|Y;\theta^{(i)})$$  

$$L(\theta)-L(\theta^{(i)})=log(\sum\limits_{z}P(Y|Z; \theta)P(Z;\theta))-logP(Y;\theta^{(i)})$$  

$$ \ge (\sum\limits_{z}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})})-logP(Y;\theta^{(i)})$$  

$$=(\sum\limits_{z}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

令：  

$$B( \theta,\theta^{(i)}) = L(\theta^{(i)})+(\sum\limits_{z}P(Z|Y;\theta^{(i)})log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})}) $$  

则：  

$$L(\theta) \ge B(\theta,\theta^{(i)}) $$  

函数$$B(\theta,\theta^{(i)})$$是$$L(\theta)$$下界，而且$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$(log1=0)  
因此，任何使$$B(\theta,\theta^{(i)})$$增大的$$\theta$$，也可以使$$L(\theta)$$增大，为了使$$L(\theta)$$尽可能的增大，选择$$\theta^{(i+1)}$$使   
$$B(\theta,\theta^{(i)})$$达到极大值，即：  

$$\theta^{(i+1)}=\mathop{\arg\min}_{\theta}B(\theta,\theta^{(i)})$$

求$$\theta^{(i+1)}$$的表达式，省去对$$\theta$$而言是常数的项  

$$\theta^{(i+1)}=\mathop{\arg\min}_{\theta}(L(\theta^{(i)})+(\sum\limits_{z}P(Z|Y;\theta^{(i)}) log\frac{P(Y|Z; \theta)P(Z;\theta)}{P(Z|Y;\theta^{(i)})P(Y;\theta^{(i)})})$$  

$$=\mathop{\arg\min}_{\theta}(\sum\limits_{z}P(Z|Y;\theta^{(i)})logP(Y|Z; \theta)P(Z;\theta)) $$  

$$=\mathop{\arg\min}_{\theta}(\sum\limits_{z}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta))$$  

$$=\mathop{\arg\min}_{\theta}E_{z}(logP(Y,Z; \theta)|Y;\theta^{(i)})$$  


# 例子
## 高斯混合模型GMM

**高斯模型/正态分布模型**  

$$\phi(Y;\theta_{k}) = \frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y-\mu_{k})^2}{2\sigma_{k}^2})$$

下标k表示第k个模型，Y可以是一维的,这里我们设定Y是多维度变量，$$\mu ,\sigma$$也是多维的，每个维度会有自己的均值和方差, 这样模型应称为多元高斯模型

**高斯混合模型**  

高斯混合模型是指具有如下形式的概率分布模型:(式子中的k代表混合了k个(多元)高斯模型)  

$$P(Y;\theta) = \sum\limits_{k=1}^K \alpha_{k}\phi(Y;\theta_{k})$$  

其中$$\alpha_{k}$$是系数，$$\alpha_{k} \ge 0, \sum\limits_{k=1}^K \alpha_{k}=1;\phi(Y;\theta_{k})$$是高斯分布密度，  
$$\theta_{k}=(\mu_{k},\sigma_{k}^2), \theta=(\alpha_{1},\alpha_{2},...\alpha_{k};\theta_{1},\theta_{2}...\theta_{k})$$  
我们用EM算法估计高斯混合模型的参数$$\theta$$  

**1、明确隐变量，写出完全数据的对数似然函数**  

假设观测数据$$Y_{j},j=1,2,...N$$(j是样本编码),是依据概率$$\alpha_{k}$$选择第k个高斯分布模型 ，  
然后依第k个分模型的概率分布$$\phi(Y;\theta_{k})$$生成的(观测数据$$Y_{j}$$)。  
这时观测数据$$Y_{j}$$是已知的，但其来自第k个分模型的数据是未知的，以隐变量$$Z_{j}$$表示其来自的第k个分模型    
$$Z_{j}=(z_{j1},z_{j2}...z_{jk})$$  

$$
z_{jk} = \left\{ \begin{array}{rl}
1 &\mbox{ 第j个观测来自第k个分模型} \\
0 &\mbox{ 否则}
\end{array} \right.
$$  

写出完全数据$$(Y_{j},Z_{j})$$的似然函数:  

$$
P(Y,Z;\theta)=\prod\limits_{j=1}^N P(Y_{j},Z_{j}; \theta)\\
=\prod\limits_{j=1}^N P(Y_{j},z_{j1},z_{j2}...z_{jk}; \theta)\\
=\prod\limits_{k=1}^K \prod\limits_{j=1}^N[\alpha_{k}\phi(Y;\theta_{k})]^{z_{jk}}\\
= \prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\phi(Y;\theta_{k})]^{z_{jk}}\\
=\prod\limits_{k=1}^K\alpha_{k}^{n_k} \prod\limits_{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_{k}}exp(- \frac{(Y-\mu_{k})^2}{2\sigma_{k}^2})]^{z_{jk}}
$$  

注意式子中的k,因为Z是onehot向量,当$$z_{jk}$$为0时，指数为0，$$[\alpha_{k}\phi(Y;\theta_{k})]^{z_{jk}}$$为1,  

可以理解为$$P(Y,Z;\theta)$$是$$z_{jk}$$为1时的概率     

式中，$$n_k=\sum\limits_{j=1}^N z_{jk},\sum\limits_{k=1}^K n_k=N$$  

完全数据的对数似然函数为  

$$logP(Y,Z;\theta)=\sum\limits_{k=1}^{K} n_{k} log\alpha_{k}+\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]$$   


**EM算法的E步，确定目标函数Q**  

$$
Q=E_{z}(logP(Y,Z; \theta)|Y;\theta^{(i)})\\
= \sum\limits_{z}P(Z|Y;\theta^{(i)}) logP(Y,Z; \theta)\\
=E_{z}\lbrace\sum\limits_{k=1}^{K} n_{k} log\alpha_{k}+\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace
$$    

将$$P(Z|Y;\theta^{(i)})$$  

可将$$P(Z|Y;\theta^{(i)})$$  

标记为$$P_{k}$$   

$$P_{k}$$意味j已知情况下，$$z_{jk}=1$$时概率  

因为Z变量是onehot向量，变量取值只有k种不同，并且实际Q函数只包含了$$z_{jk}$$为1的项  



$$
Q = \sum\limits_{k=1}^{K} P_{k} \lbrace\sum\limits_{k=1}^{K} n_{k} log\alpha_{k}+\sum\limits_{j=1}^N{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace\\
=\sum\limits_{k=1}^{K}\lbrace\sum\limits_{k=1}^{K}  P_{k}n_{k} log\alpha_{k}+\sum\limits_{j=1}^N P_{k}{z_{jk}}[log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace\\
$$  

其中$$P_{k}n_{k}=\sum\limits_{j=1}^N P_{k}z_{jk}=\sum\limits_{j=1}^N E_{k}[z_{jk}]$$  因此：  

$$
Q = \sum\limits_{k=1}^{K}\lbrace\sum\limits_{j=1}^N E_{k}[z_{jk}] log\alpha_{k}+\sum\limits_{j=1}^N E_{k}[{z_{jk}}][log(\frac{1}{\sqrt{2\pi}})-log\sigma_{k}-\frac{1}{\sqrt{2\sigma_{k}^2}}(Y_{j}-\mu_{k})^2]\rbrace

$$

为了确定Q，需要计算$$E_{k}[z_{jk}]$$,$$z_{jk}$$取值范围是{0,1}:  

$$
E_{k}[z_{jk}] =P_{k}z_{jk}
$$

## k-mean算法
## HMM
## LDA     

来源：李航《统计学习》
