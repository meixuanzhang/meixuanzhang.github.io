---
layout: post
title: EM算法
date:   2019-01-01
categories: 统计学习
---
# 隐变量定义
在统计学中，隐变量与可观察变量相对，是不直接观察但是通过观察到的其他变量推断的变量。平常模型中遇见的X(输入)，Y(输出)都是可观察到的，是可观察变量。  
# EM算法的推导
Nocation:  
Y：可观测到的变量,这里的$$Y=(y_{1},y_{2},y_{3}..y_{n})$$是多维随机变量。  
Z：隐变量，也是多维随机变量$$Z=(z_{1},z_{2},z_{3}..z_{n})$$  
$$\theta$$:参数  
概率模型假设：
$$P(Y;\theta)=\sum\limits_{z}P(Y,Z; \theta)=\sum\limits_{z}P(Z;\theta)P(Y|Z; \theta)$$  
公式涉及了贝叶斯定理   
含有隐变量的概率模型我们将Y，Z称为完全数据，只有Y称为不完全数据  
注意这里的$$\theta$$是我们待估计的参数，是常数，如果是随机变量公式应写成$$P(Y| \theta)$$，但有些书本不会加以区别。  

概率模型目标函数：极大似然函数
$$L(\theta)=logP(Y;\theta)=log\sum_{z}P(Y,Z; \theta)=log(\sum_{z}P(Z;\theta)P(Y|Z; \theta))$$

如果不含有隐变量，模型可以直接参用极大似然估计估计参数，含有则需要使用EM算法估计参数。 



























来源：李航《统计学习》
