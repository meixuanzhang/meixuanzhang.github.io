---
layout: post
title: 译Lecture 2 Bayesian Networks
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

动机：通过一些随机变量表示联合分布在计算成本上是昂贵的，所以我们需要方法来紧凑地表示联合分布。  

# 两种图模型(Graphical Models)  


**有向图(Directed Graphs)** (Bayesian Networks)  

一个非非环形图$$g$$,由一组节点$$V$$和一组有方向的边$$\varepsilon$$组成，边连接的两个节点具有因果关系(causality relationship),图中的节点代表一组随机变量$$X_{1},X_{2}...X_{N}$$,节点和随机变量之间存在一对一的映射。 

![_config.yml]({{ site.baseurl }}/images/10708/image14.png)  
 

上述有向图的联合概率可以写成：

$$P(X_{1},...,X_{8})=P(X_{1})P(X_{2})P(X_{3}\mid X_{1})P(X_{4}\mid X_{2})P(X_{5}\mid X_{2})P(X_{6}\mid X_{3},X_{4})P(X_{7}\mid X_{6})P(X_{8}\mid X_{5},X_{6})$$   


**无向图(Undirected Graphs)** (Markov Random Fields)  

![_config.yml]({{ site.baseurl }}/images/10708/image15.png)   

无向图包含通过无方向边连接的节点。  


# 标记(Notation)   

**变量(Variable)**: 大写英文字母,下标代表所处维度(i),上标表示索引(j)如：$$ V_{i}^j $$    
**变量值(Values of variables)**: 小写字母表示它是某个随机变量的“观察值”如：$$v_{i}^j$$   
**随机变量(Random variable)**: 具有随机性的变量，根据不同的观察结果之间会发生变化。   
**随机向量(Random vector)**: 大写加粗字母(维度 1 X n).   
**随机矩阵(Random matrix)**:大写加粗字母(维度 n X n).   
**参数(Parameters)**: 希腊字符,可以认为是随机变量。   

# 不诚实的赌场(The Dishonest Casino)  

设X是序列的随机变量，随机变量序列为(random sequence),$$X_{1},..,X_{T}$$。$$X_{t}$$是赌场骰子结果,$$X \in (1,2,3,4,5,6)$$，$$Y$$是随机变量的解析,$$Y_{t}$$表示骰子是公正或有偏差的，$$Y\in (0,1)$$。有偏差的骰子，每个点数发生概率分布如下：  

![_config.yml]({{ site.baseurl }}/images/10708/image16.png)    

我们可能想问的一些问题是：  

**评估(Evaluation):** 给定赌场模型下，序列发生的可能性有多大？  

**解码(Decoding):** 序列中哪些部分是用公平骰子生成的，哪些部分是用有偏差骰子生成的？ 

**学习(Learning):** How “loaded” is the loaded dice? How “fair” is the fair dice?，赌场玩家将公平骰子换成有偏差骰子频率？

Loaded dice:A loaded, weighted, cheat, or crooked die is one that has been tampered with so that it will land with a specific side facing upwards more or less often than a fair die would. 

我们可以对赌场问题进行建模的一种方法是使用隐马尔可夫模型，其中$$X_{t}$$是观测变量(observed variables)，$$Y_{t}$$是隐藏变量(hidden variables)：  

![_config.yml]({{ site.baseurl }}/images/10708/image17.png)    

所有隐藏变量均具有Markov属性，即在给定当前条件下过去有条件地独立于未来：    

$$Y_{t-1} \bot \{Y_{t+1},...,Y_{T}\}\mid Y_{t}$$   

此属性在图的拓扑中也显式突出显示。  

此外，根据我们的HMM序列，我们可以计算可能性，如下所示$$\mathbf{X},\mathbf{Y}$$表示序列：  

$$
P(\mathbf{X},\mathbf{Y})=P(Y_{1})\prod_{t=1}^T P(X_{t}\mid Y_{t})\prod_{t=2}^T P(Y_{t}\mid Y_{t-1})\\
P(\mathbf{X},\mathbf{Y})=P(Y_{1})P(X_{1}\mid Y_{1})P(Y_{2}\mid Y_{1})P(X_{2}\mid Y_{2})...P(Y_{T}\mid Y_{T-1})P(X_{T}\mid Y_{T})\\
P(\mathbf{X},\mathbf{Y})=P(Y_{1})P(Y_{2}\mid Y_{1})..P(Y_{T}\mid Y_{T-1})P(X_{1}\mid Y_{1})P(X_{2}\mid Y_{2})...P(X_{T}\mid Y_{T})\\
P(\mathbf{X},\mathbf{Y})=P(Y_{1}..Y_{T})P(X_{1},..,X_{T}\mid Y_{1},..,Y_{T})
$$

边际和后验分布可以计算如下：   

**Marginal(边际分布):** $$P(\mathbf{X})=\sum_{Y_{1}}..\sum_{Y_{T}}P(\mathbf{X},\mathbf{Y})$$   

**Posterior(后验分布):** $$P(\mathbf{Y} \mid \mathbf{X}) =\frac{P(\mathbf{Y},\mathbf{X})}{P(\mathbf{X})}$$  

# 贝叶斯网络(Bayesian Network)   

+ BN是一个有向图，其节点表示随机变量，其边表示一个变量对另一变量的有向影响。 

+ 它是一种数据结构，提供了通过因子分解方式紧凑地表示联合分布的框架。  

+ 它简洁表示了分布的条件独立性假设。  

+ 可以将该图看作是对生成采样过程的编码，其中每个变量的值由基于父变量的条件分布来选择。换句话说，每个变量都是其父变量的随机函数(其父变量决定了变量分布)。  

# 贝叶斯网络: 因子分解定理(Factorization Theorem)    

将第$$i$$个节点的父节点定义为$$Pa_{X_{i}}$$,非其后代节点定义为$$NonDescendants_{X_{i}}$$。  

有向拓扑结构图的独立一组条件独立性声明：  

$$\{X_{i} \bot NonDescendants_{X_{i}} \mid Pa_{X_{i}} \}$$

![_config.yml]({{ site.baseurl }}/images/10708/image18.png)   

因此，上述有向图的联合概率可以写成如下：  

$$
P(X_{1},..,X_{8})=\prod_{i=1}^8 P(X_{i}\mid Pa_{X_{i}} )
=P(X_{1})P(X_{2})P(X_{3}\mid X_{1})P(X_{4}\mid X_{2})P(X_{5}\mid X_{2})P(X_{6}\mid X_{3},X_{4}))P(X_{7}\mid X_{6})P(X_{8}\mid X_{5},X_{6}))
$$

# 有向图模型的设置     

**任何图模型都有两个组成部分：**     

+ 定性（拓扑结构）  
+ 定量（与每个条件分布相关的数字）  

**定性设置的假来源：**    

+ 因果关系的先验知识
+ 模块化关系的先验知识
+ 专家评估
+ 从数据中学习
+ 我们只是喜欢某种架构（例如分层图）
…  

# 局部结构和独立性(Local Structures & Independencies)   

+ Common parent (also called ‘common cause’ )  

固定$$B$$解耦$$A$$和$$C$$       

给定$$B$$的情况下，$$A$$和$$C$$相互独立   

$$A \bot C \mid B \Rightarrow P(A,C \mid B)=P(A \mid B)P(C \mid B)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image19.png)    

+ Cascade (also called ‘causal/evidential trail’ )   

已知$$B$$解耦$$A$$和$$C$$   

给定$$B$$的情况下，$$A$$对于预测$$C$$没有提供额外价值    

![_config.yml]({{ site.baseurl }}/images/10708/image20.png)   

+ V-structure (also called ‘common effect’)   

已知$$C$$,则$$A$$和$$B$$关联,$$A$$可以“解释” $$B$$通过确定的$$C$$   

如果$$A$$与$$C$$相关，那么$$B$$也与$$C$$相关的机会将会减少”   

![_config.yml]({{ site.baseurl }}/images/10708/image21.png)    

**v-结构示例**  

时钟显示时间慢了（事件A）和交通阻塞（事件B）是相互独立事件，都可能导致上课迟到（事件C）。出席课堂将这两个事件关联在一起。如果我的时钟是正常，它可能会“解释”我的迟到是由于交通阻塞造成。换句话说，如果知道C，则A和B会耦合，但$$P(A,B\mid C)$$无法解耦为两个事件。C未知则$$P(A,B)=P(A)P(B)=P(A)P(B)$$   
 
# I-maps   

将$$P$$定义为$$\mathbf{X}$$的概率分布，$$I(P)$$表示分布$$P$$中蕴含独立关系集 $$(X \bot Y\mid Z)$$    

定义$$K$$为图结构，图结构中蕴含独立关系集为$$I(K)$$,如果图结构$$K$$对于分布$$P$$有$$I(K)\subseteq I(P)$$，则称$$K$$为$$P$$的I-map

# 关于I-map的事实   

要使$$g$$作为$$P$$的I-map，$$g$$声称的任何独立性也必须在$$P$$中持有。相反，$$P$$可能有其他独立性，而这些独立性并没有在$$G$$中反映出来。   

**例子**:   

![_config.yml]({{ site.baseurl }}/images/10708/image22.png)    

只有在图1中$$X$$和$$Y$$是相互独立的。   

下面我们有两张表显示边际分布。查找I-maps：  

$$P_{1}:I(P_{1})=X \bot Y$$(from inspection i.e,$$P(X,Y)=P(X)P(Y),$$0.48=0.6*0.8)    

![_config.yml]({{ site.baseurl }}/images/10708/image23.png)  

Solution: Graph 1.  

$$P_{2}:I(P_{2})=\phi$$   

![_config.yml]({{ site.baseurl }}/images/10708/image24.png)  

Solution: Both graph 2 and graph 3, 因为它们独立集都为为空。因此，它们在独立集上是等价的。$$P_{2}$$独立集不空,图1独立集不为空。因此它不是$$P_{2}$$的I-map.    

# 什么是$$I(G)$$   

**贝叶斯网络的局部马尔可夫假设**   

叶斯网络结构$$g$$是一个有向无环图，其节点表示随机变量$$X_{1},..X_{n}$$    

**局部马尔可夫假设**  

定义:  

图$$g$$中$$X_{i}$$节点的父节点定义为$$Pa_{X_{i}}$$,非其后代节点定义为$$NonDescendants_{X_{i}}$$。  

通过$$g$$对以下局部条件独立性假设$$I_{l}(g)$$编码:  

$$I_{l}(g):\{X_{i}\bot NonDescendants_{X_{i}} \mid Pa_{X_{i}}: \forall_{i}  \}$$   

换句话说每个节点在父节点的条件下独立于其非后代节点。  

# 贝叶斯网络的D-separation 准则   


定义一：  

在给定$$Z$$时,变量$$X$$和$$Y$$在正则祖先图中分离(moralized ancestral graph)，则$$X$$和$$Y$$给定$$Z$$时是D-separated。(D代表有向边)    

示例: 当$$X \bot Y \mid Z$$，我们说$$Z$$分离了$$X,Y$$   

![_config.yml]({{ site.baseurl }}/images/10708/image25.png)   

祖先图(Ancestral graph只关注感兴趣的节点):只保留节点本身+与问题有关的节点的祖先。   

正则祖先(Moral ancestral)：在Ancestral graph中，将有向边改无向边，v结构的两个父节点加一条边，这是一幅无向图。     

如果给定条件下存在任何路径从一个节点到另一个节点的路径，则这两个节点不是给定条件下独立(示例不是条件独立的)。   

# $$I(g)$$的实际定义   

贝叶斯网络的全局马尔可夫性质

$$X$$与$$Z$$是 d-separated (directed-separated)，在给定$$Y$$时。如果我们不能使用下面所示的“Bayes-ball”算法将球从$$X$$中的任何节点发送到$$Z$$中的任何节点.  

![_config.yml]({{ site.baseurl }}/images/10708/image26.png)   

定义：$$I(G)$$= 与 d-separation相对应的所有独立性质:   

$$I(G):\{X_{i} \bot Z\mid Y:dsepg(X;Z \mid Y)\}$$  

# $$I(G)$$ 示例   

![_config.yml]({{ site.baseurl }}/images/10708/image27.png)   

在这个图中有两种类型的有效迹结构：  

+ common cause:$$X_{3}\gets X_{1}\to X_{4}$$。如果不给定条件$$X_{1}$$,这条迹是有效的。  

+ common effect:$$X_{2}\to X_{3}\gets X_{1}$$。如果给定条件$$X_{3}$$,这条迹是有效的。 

为了找到独立项，应考虑长度大于1的所有迹（因为节点不能独立于其父节点）。

**长度为2的迹:**  

+ $$X_{2}\rightleftharpoons X_{3}\rightleftharpoons X_{1}$$由于'common effect'结构，只要不给定条件$$X_{3}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{1}),(X_{2}\bot X_{1}\mid X_{4})$$   

+ $$X_{3}\rightleftharpoons X_{1}\rightleftharpoons X_{4}$$由于'common  cause'结构,只要给定条件$$X_{1}$$,这条路径就会阻塞,所以有$$(X_{3}\bot X_{4}\mid X_{1}),(X_{3}\bot X_{4}\mid \{X_{1},X_{2}\})$$   


**长度为3的迹(only $$X_{2}\rightleftharpoons X_{3}\rightleftharpoons X_{1}}\rightleftharpoons X_{4}$$):**  

+ 由于'common effect'结构$$X_{2}\to X_{3}\gets  X_{1}$$,只要不给定条件$$X_{3}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{4}),(X_{2}\bot X_{4}\mid X_{1})$$     

+ 由于'common cause'结构$$X_{3}\gets X_{1}\to X_{4}$$,只要给定条件$$X_{1}$$,这条路径就会阻塞,所以有$$(X_{2}\bot X_{4}\mid X_{1}),(X_{2}\bot X_{4}\mid \{X_{1},X_{3}\})$$   

**节点集之间的轨迹**  

+ 根据d-separation 有$$X_{2}\bot X_{1},X_{4}$$，$$X_{2}$$和$$X_{1}$$，$$X_{2}$$和$$X_{4}$$之间任意路径是阻塞的.

**Full $$I(g)$$**  

把上面这些放在一起，我们有以下独立项。  

$$
I(g)=\{(X_{2}\bot X_{1}),(X_{2}\bot X_{1}\mid X_{4}),\\
(X_{3}\bot X_{4}\mid X_{1}),(X_{3}\bot X_{4}\mid \{X_{1},X_{2}\}),\\
(X_{2}\bot X_{4}),(X_{2}\bot X_{4}\mid X_{1}),(X_{2}\bot X_{4}\mid \{X_{1},X_{3}\}),\\
(X_{2}\bot \{X_{1},X_{4}\})\}

$$   

# 等价定理

参考：
[Lecture 2: Bayesian Networks](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-02/)



