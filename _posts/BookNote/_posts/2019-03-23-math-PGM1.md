---
layout: post
title: Probabilistic Graphical Models 1:Representation
date:   2019-03-23
categories: Probabilistic Graphical Models
---

# Introduction and Overview
## Motivation and Overview 


**Model**  

模型是对我们认知的事物描述，通过计算机可以捕捉我们对变量（事物）是什么以及它们如何相互作用的理解，模型构造独立于算法，可将模型构造与使用算法分开，通过专家、对历史数据进行统计机器学习分析提出模型  
同一模型对不同的问题有不同算法解决  
模型帮助我们处理大量不确定性问题  
![_config.yml]({{ site.baseurl }}/images/1PGM/image1.png)

**Uncertainty不确定性来源**   

+ Partial knowledge of state of the word(对事物认知不完全)   
+ Noisy observations(噪音)  
+ Phenomena not covered by our model(影响因素考虑不全面)  
+ Inherent stochasticity (固有随机性) 复杂系统的建模局限性使人们可以将世界视为固有的随机性。  

**Probability Theory(概率论)**  

+ Declarative representation with clear semantics 通过概率论能清晰的描述问题，如使用概率分布描述了不同状态下世界不确定性  
+ Powerful reasoning patterns 概率论有强大推理模式，如基于条件下不确定性决策，
+ Established learning methods 概率论与统计使我们可以从历史数据学校模型

**Graphical Models(图模型)**  

贝叶斯网络：图节点代表了随机变量，边代表了变量之间的概率关系，有方向，无环，
马尔可夫网络：无方向
![_config.yml]({{ site.baseurl }}/images/1PGM/image2.png)  
![_config.yml]({{ site.baseurl }}/images/1PGM/image3.png)  

**Graphical Representation**   

+ Intuitive&compact data structure 图描述是一种直观的紧凑的数据结构
+ Efficient reasoning using general-purpose algorithms 它为我们提供了一套有效推理的方法，使用了图形结构的通用算法。
+ Spare parameterization 图描述可以使用非常少量的参数有效地表示这些高维概率分布。  
feasible elicitation 通过专家（启发式）经验学习描述
learning from data 从数据中学习描述

**Overview**  
+ Representation  学习不同的模型
Directed and undirected   
Temporal and plate model   
+ Inference  通过模型进行推理和决策
Exact and approximate   
Decision making  
+ Learning 学习模型参数和架构
Parameters and structure 
With and without conplete data

## Distributions  
**Joint Distribution联合概率**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image4.png)  

**condition**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image5.png)  
![_config.yml]({{ site.baseurl }}/images/1PGM/image6.png)  

**Marginalization**  
![_config.yml]({{ site.baseurl }}/images/1PGM/image7.png)  

## Factors   
**factors**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image8.png) 

factor$$\phi$$类似函数，$$X_{1},X_{2}..,X_{k}$$是变量。  
scope of the factor是$$X_{1},X_{2}..,X_{k}$$取值集合  

**General factors**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image9.png)   

**Factor Product**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image10.png) 

**Factor Marginalization**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image11.png) 

**Factor Reduction**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image12.png) 

**为什么使用factor**   

+ Fundamental building block for defining distributions in high-dimensional spaces  
factor是定义分布和高维空间的基本构建块。  
+ set of basic operations for manipulating these probability distribtions  
通过factor操作构造概率分布 

# Bayesian Network Fundamentals  
## Semantics&Factorization  
贝叶斯网络例子：  

![_config.yml]({{ site.baseurl }}/images/1PGM/image13.png) 

图中显示的是条件概率分布(CPD)

![_config.yml]({{ site.baseurl }}/images/1PGM/image14.png) 

**Chain Rule for Bayesian Network**  
![_config.yml]({{ site.baseurl }}/images/1PGM/image15.png) 

factor product  

![_config.yml]({{ site.baseurl }}/images/1PGM/image16.png) 


**Bayesian Network定义**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image17.png) 

BN is a legal Distribution

$$P\ge 0$$ P is a product of CPDs,cpd are non-negative   
$$\sum P=1$$  

![_config.yml]({{ site.baseurl }}/images/1PGM/image18.png) 


**P Factorizes over G** 

![_config.yml]({{ site.baseurl }}/images/1PGM/image19.png) 


## Resoning Patterns  



