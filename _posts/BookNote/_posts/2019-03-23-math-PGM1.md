---
layout: post
title: PGM:Representation 1 
date:   2019-03-23
categories: ["Probabilistic Graphical Models"]
---

# 一、Introduction and Overview
## 1、Motivation and Overview 


**Model**  

模型是对我们认知的事物描述，通过计算机可以捕捉我们对变量（事物）是什么以及它们如何相互作用的理解，模型构造独立于算法，可将模型构造与使用算法分开，通过专家、对历史数据进行统计机器学习分析提出模型       
模型帮助我们处理大量不确定性问题    

![_config.yml]({{ site.baseurl }}/images/1PGM/image1.png)

**Uncertainty不确定性来源**   

+ Partial knowledge of state of the word(对事物认知不完全)   
+ Noisy observations(噪音)  
+ Phenomena not covered by our model(影响因素考虑不全面)  
+ Inherent stochasticity (固有随机性) 复杂系统的建模局限性使人们可以将世界视为固有的随机性。  

**Probability Theory(概率论)**  

+ Declarative representation with clear semantics 通过概率论能清晰的描述问题，如使用概率分布描述了不同状态下世界不确定性  
+ Powerful reasoning patterns 概率论有强大推理模式，如基于条件下不确定性决策，
+ Established learning methods 概率论与统计使我们可以从历史数据学习模型

**Graphical Models(图模型)**  

贝叶斯网络：图节点代表了随机变量，边代表了变量之间的概率关系，有方向，无环   
马尔可夫网络：无方向

![_config.yml]({{ site.baseurl }}/images/1PGM/image2.png)  
![_config.yml]({{ site.baseurl }}/images/1PGM/image3.png)  

**Graphical Representation**   

+ Intuitive&compact data structure 图描述是一种直观的紧凑的数据结构  
+ Efficient reasoning using general-purpose algorithms 它为我们提供了一套有效推理的方法，使用了图形结构的通用算法。  
+ Spare parameterization 图描述可以使用非常少量的参数有效地表示这些高维概率分布。  
feasible elicitation 通过专家（启发式）经验学习描述  
learning from data 从数据中学习描述  

**Overview**  
+ Representation  学习不同的模型
Directed and undirected   
Temporal and plate model   
+ Inference  通过模型进行推理和决策
Exact and approximate   
Decision making  
+ Learning 学习模型参数和架构
Parameters and structure 
With and without conplete data

## 2、Distributions  
**Joint Distribution联合概率**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image4.png)  

**condition**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image5.png)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image6.png)  

**Marginalization**    
P(I,D)是联合分布  
P(D)是边缘分布  

![_config.yml]({{ site.baseurl }}/images/1PGM/image7.png)  

## 3、Factors   
**factors**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image8.png) 

factor$$\phi$$类似函数，$$X_{1},X_{2}..,X_{k}$$是变量。将高维映射到低维。       
scope of the factor是$$X_{1},X_{2}..,X_{k}$$取值集合    

**General factors**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image9.png)   

**Factor Product**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image10.png) 

**Factor Marginalization**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image11.png) 

**Factor Reduction**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image12.png) 

**为什么使用factor**   

+ Fundamental building block for defining distributions in high-dimensional spaces  
factor是定义分布和高维空间的基本构建块。  
+ set of basic operations for manipulating these probability distribtions  
通过factor操作构造概率分布   

## 4、图(补充)  

**节点和边**  

图是一个包含节点(node)集合边(edge)的数据结构K，假设节点集$$X=\{x_{1},..,x_{n}\}$$    
$$x_{i}\to x_{j}$$表示$$x_{i},x_{j}$$节点连接是有向边    
$$x_{i}-x_{j}$$表示无向边   
$$x_{i}\rightleftharpoons x_{j}$$表示$$x_{i},x_{j}$$节点有边连接(有向或无向)   
$$\varepsilon$$表示成对节点的集合  
有向图：所有边都是有向的，记作g  
无相同：所有边都是无向的，记作H  

在图$$K=(X,\varepsilon)$$中，若$$x_{i}\to x_{j}$$,则$$x_{i}$$是$$x_{j}$$子结点(child),而$$x_{j}$$是$$x_{i}$$父节点(parent)。若$$x_{i}-x_{j}$$，则$$x_{i}$$是$$x_{j}$$的邻节点(neighbor)反之亦然。只要$$x_{i}\rightleftharpoons x_{j}$$，我们则称$$x_{i},x_{j}$$相邻(adjacent)。  

$$Pa_{x}$$表示x的父节点  
$$Ch_{x}$$表示x的子节点  
$$Nb_{x}$$表示x的邻节点  
$$Boundary_{x}=Pa_{x}\cup Nb_{x}$$表示x的边界集  
节点x的度(degree)是与其相连边的个数，indgree是有向边$$y\to x$$的个数  
图的degree是图中节点最大的degree

**子图**  

$$K=(X,\varepsilon),\widehat{X}\subset X$$，则将$$K[\widehat{X}]=(\widehat{X},\varepsilon_{'})$$称为induced subgraph ,$$\varepsilon_{'}$$表示$$x,y\in \widehat{X}$$的所有边$$x\rightleftharpoons y \in X$$   

如果子图$$K[\widehat{X}]$$任意两节点之间是相连的，则称$$K[\widehat{X}]$$是complete，$$\widehat{X}$$称为clique，如果$$\widehat{X} \subset Y,Y$$不是clique，那么$$\widehat{X}$$是最大的clique，maximal clique即现有子图增加新的节点后将不再是clique(任意节点相连)，将现有子图称为maximal clique  

如果子图$$K[\widehat{X}]$$任意节点x,有$$Boundary_{x}\subset \widehat{X}$$，我们称$$\widehat{X}$$是upwardly closed   

如果子图$$K[\widehat{X}]$$所有节点$$Boundary_{x}$$并集等于Y，则称Y是$$\widehat{X}$$的upward closure，标记为$$K^+[X]$$ (未确定理解对不对)

![_config.yml]({{ site.baseurl }}/images/1PGM/image47.png)   

**路径和迹**  

在图$$K=(X,\varepsilon)$$中，如果对于每一个$$i=1,2..,k-1$$有$$x_{i}\to x_{i+1}$$或$$x_{i}-x_{i+1}$$,则$$x_{1}..x_{k}$$形成了一条**路径(path)**,如果至少有一个i满足$$x_{i}\to x_{i+1}$$，我们则说这条路径是有向的。  

对于每一个$$i=1,2..,k-1$$有$$x_{i}\rightleftharpoons x_{i+1}$$，那么$$x_{1}..x_{k}$$在图$$K=(X,\varepsilon)$$中形成一条**迹(trail)** 。
上图中A,C,D,E,I是一条路径，也是一条迹，A,C,F,G,D是一条迹，但不是一条路径。

对于图中任意节点$$x_{i},x_{j}$$之间存在一条迹，那么该图是**连通图**。

如果图$$K=(X,\varepsilon)$$中存在一条使得$$x_{1}=x,x_{k}=y$$的有向路径$$x_{1}...x_{k}$$,那么$$x$$是$$y$$的祖先(ancestor),而$$y$$是$$x$$的后代(descendant)。使用$$Descendants_{x}$$表示$$x$$的后代，$$Ancestors_{x}$$表示$$x$$的祖先，$$NonDescendants$$表示$$x-Descendants_{x}$$中节点的集合。

上图中F,G,I是C的后代。A、B是C的祖先，路径分布是A,C和B,E,D,C   

有向图$$g=\{X,\varepsilon\}$$，如果节点$$x_{1}...x_{n}$$序列中，只要$$x_{i}\to x_{j}\in \varepsilon $$,就有$$i<j$$,节点这种排序称为称为拓扑序(topological ordering)  

**圈与环**
 
图K中有一个圈(cycle)意味存在**有向路径**$$x_{1}...x_{k}$$,其中$$x_{1}=x_{k}$$。如果一个图不包含圈，那么这个图是无圈图(acyclic graph)    

一个包含有向边和无向边的无圈图称为部分有向无圈图(partially directed acyclic graph:PDAG)   

令K是X上的PDAG,按以下条件划分成互不相交的n份$$K_{1}..K_{n}$$:   

+ $$K_{i}$$子图不包含有向边  
+ 对$$i<j$$，任意一对节点$$x\in K_{i},y \in K_{j}$$,如果$$x,y$$之间存在边，边只能是有向边$$x \to y$$。

每一个分支$$K_{i}$$称为链分支(chain component),由于链结构，PDAG也被称为链图(chain graph)  

上图共有六个链分支：{A},{B},{C,D,E},{F,G},{H},{I},这个排序是其中一个合理的排序。  

当PDAG时一个无向图时，整个图形成一个单链分支，如果是有向无圈图，那么每一个节点就是其本身的一个链分支

环(loop)是一条**迹**$$x_{1}...x_{k}$$,其中$$x_{1}=x_{k}$$,**如果一个图不包含任何环**，那么这个图是单连通。如果一个节点在单连通(无环)图中只有一个相邻的节点，那么该节点称为叶结点(leaf)。一个单连通(无环)的有向图称为一棵多重树(ploytree),一个单连通(无环)的无向图称为深林(forest)。一个无向连通图称为树(tree)(不太确定对不对)  

路径是迹，圈是环，但迹不一定是路径，环不一定是圈

如果有向图的每一个节点至多只有一个父节点，这个有向图是森林。有向森林连通图是一颗树  

令$$x_{1}-x_{2}..-{x_{k}-x_{1}}$$是图中一个环，环中两个不相邻的节点$$x_{i},x_{j}$$有相连的边称为弦(chord)，对于$$k>4$$，任意环$$x_{1}-x_{2}..-{x_{k}-x_{1}}$$有一条弦的无向图，称为弦图。

# 二、Bayesian Network Fundamentals  
## 1、Semantics&Factorization  
贝叶斯网络例子：  

![_config.yml]({{ site.baseurl }}/images/1PGM/image13.png) 

图中显示的是条件概率分布(CPD)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image14.png) 

**Chain Rule for Bayesian Network**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image15.png) 

factor product  

![_config.yml]({{ site.baseurl }}/images/1PGM/image16.png) 


**Bayesian Network定义**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image17.png) 

BN is a legal Distribution

$$P\ge 0$$ ,P is a product of CPDs,cpd are non-negative   
$$\sum P=1$$  

![_config.yml]({{ site.baseurl }}/images/1PGM/image18.png) 


**P Factorizes over G**   

![_config.yml]({{ site.baseurl }}/images/1PGM/image19.png) 


## 2、Resoning Patterns   

![_config.yml]({{ site.baseurl }}/images/1PGM/image20.png) 

**Causal Reasoning**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image21.png)   

L是果，i是因，果是条件    
学生获得Letter概率是0.5,获得Letter概率受成绩影响，课程成绩越高$$(g^1>g^2>g^3)$$，获得Letter概率越高。    
已知学生智力低，课程获得高成绩概率降低，获得Letter概率降低（无条件下获得Letter概率，被高智商群体拉高了）。所以式子2在低智力条件下，获得Letter概率降低。   
现已知智力低，课程难度低，获得高成绩概率会提高，式子3在智力低，难度低条件下，获得Letter概率提升。    


**Evidential Reasoning**    

 ![_config.yml]({{ site.baseurl }}/images/1PGM/image28.png)   

d是因，g是果，因是条件    
是难课程的概率是0.4，课程越难，成绩越低，已知课程成绩低，是难的课程的概率提升    


是高智商的概率是0.3，已知学生成绩低，是高智商的概率降低    


**Intercausal Reasoning**    

![_config.yml]({{ site.baseurl }}/images/1PGM/image22.png)    

i是g的因，d是g的因，g是果，两个因之间条件关系      
学生成绩低，是高智商的概率是0.08，成绩低但课程难，是高智商的概率提升   


![_config.yml]({{ site.baseurl }}/images/1PGM/image23.png)   

成绩一般，是高智商的概率是0.175    
成绩一般，课程难，是高智商的概率概率提升    

![_config.yml]({{ site.baseurl }}/images/1PGM/image24.png)   

$$x_{1},x_{2}$$是相互独立，但在条件 y 下是不独立的    
两种可能会使得$$y=1,x_{1}=1$$或$$x_{2}=1$$,如果已经知道$$x_{1}=1$$则$$y=1$$,那$$x_{2}=1$$概率是0.5     

**Example**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image25.png) 

SAT和Grade是同一个因的果   
是难课程的概率是0.4,成绩低，是难课程的概率提升，sat高，智商高，成绩低，难课程概率提升，成绩低的原因很可能是因为课程难    
是高智商的概率是0.4,成绩低，是高智商的概率降低，sat高，是高智商的概率提升    


**总结**  
三种推理模式：Causal Reasoning，Evidential Reasoning，Intercausal Reasoning
 

## 3、Flow of probabilistic influence   

![_config.yml]({{ site.baseurl }}/images/1PGM/image26.png)   

V-structure X和Y相互不影响,X不影响Y  
如果$$X \to Y$$，则不能找到一个变量作为condition使X和Y相互独立，但是如果X和Y相互独立，有可能找到一个变量作为condition使X和Y不相互独立 

**Active Trails**  

 A trail$$ X_{1}-..-X_{n} $$ is active if:  
 it has no v-structrues $$X_{i-1}\to X_{i}\gets X_{i+1}$$   
 
**When can X influence Y Given evidence about Z**    

确定Z情况下，X什么时候influence Y

![_config.yml]({{ site.baseurl }}/images/1PGM/image27.png)   

$$X\to W\to Y ,W\in Z$$  X not influence Y 因为W已知情况下，X不能影响W，不能影响Y   
$$X\to W\to Y ,W\notin Z$$  X influence Y 因为W未知，X变化能影响W，能影响Y   
$$X\to W\gets Y $$ 当W及其子变量时$$\in W$$ X influence Y,因为W已知情况下，X能影响Y  
$$X\to W\gets Y $$ 当W及其子变量时$$\notin W$$ X influence Y,因为W未知，X不能影响Y  
Letter是Grade的descendants 子变量    

**Active Trails**  

 A trail$$ X_{1}-..-X_{n} $$ is active given Z if:  
 for any v-structure$$X_{i-1} \to X_{i}\gets X_{i+1}$$ we have that $$X_{i}$$ or one of its descendants $$\in Z$$  
no other $$X_{i}$$ in Z  

EX:  
S-I-G-D   
S: SAT   
I:Intelligence  
G:Grade  
D:Difficulty 

当G已知时，S-I-G-D是Active Trails

# 三、Bayesian Networks: Independencies
## 1、Conditional Independence  

![_config.yml]({{ site.baseurl }}/images/1PGM/image29.png)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image30.png)   

$$
P(X,Y,Z)=P(X,Y\mid Z)P(Z)\\
=P(X\mid Z)P(Y\mid Z)P(Z)\\
=\frac{P(X,Z)}{P(Z)}\frac{P(Y,Z)}{P(Z)}P(Z)
\propto P(X,Z)P(Y,Z)
$$


例子，有两枚硬币A,B A是头的概率是90%，B是头的概率是50%   
如果随机选一枚硬币，第一次扔结果是头，第二次扔是头的概率提升，两次扔硬币并不互相独立   
如果被告知选了硬币A或B，则两次扔硬币是独立的    

![_config.yml]({{ site.baseurl }}/images/1PGM/image31.png)  

V-structure condition on G,X、Y则不独立了  

![_config.yml]({{ site.baseurl }}/images/1PGM/image32.png)  



## 2、Independencies in Bayesian Network   

**1、Independence & Factorization**

通过图因子分解P

![_config.yml]({{ site.baseurl }}/images/1PGM/image33.png)   

**2、d-separation**

X和Y在条件Z下，没有active trail,则称 X和Y d-separated   
![_config.yml]({{ site.baseurl }}/images/1PGM/image34.png)    

![_config.yml]({{ site.baseurl }}/images/1PGM/image35.png)    


对于Letter,Grade是它的parents,Job、Happy是它的descendants,Coherence、Diffculty、Intelligece、SAT是non-descendants
在给定parents情况下，Letter和它的non-descendants都是d-separated

![_config.yml]({{ site.baseurl }}/images/1PGM/image36.png)   

**I-maps** 

如果P满足I(G),我们说G是P的I-map，I(G)描述了G中的独立假设  
![_config.yml]({{ site.baseurl }}/images/1PGM/image37.png)   

![_config.yml]({{ site.baseurl }}/images/1PGM/image38.png)  

$$\phi$$空集 
$$
P(\phi)=P(\phi A)=P(\phi)P(A)=0=P(\phi)\\
P(A)=P(\Omega A)=P(\Omega)P(A)=P(A)
$$

$$I(G1)={I\bot D},I(G2)=\{\phi\}$$
P1、P2 satisfy the empty set of independence assumptions空集和全集与任意事件独立

G1假设I、D相互独立，G2则没有假设I、D独立    
P1可以通过G1和G2进行因式分解，满足I(G1)和I(G2)，P2只能通过G2进行因式分解  
G2是P1和P2的I-map  
G1是P1的I-map  

![_config.yml]({{ site.baseurl }}/images/1PGM/image39.png)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image40.png)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image41.png)

如果G是P的I-map,则P分解过程可以使用G中独立假设  

## Naive Bayes  

Given the class variable, each observed variable is independent of the other observed variables.  

![_config.yml]({{ site.baseurl }}/images/1PGM/image42.png)  

![_config.yml]({{ site.baseurl }}/images/1PGM/image43.png)  

**Bernouli Naive Bayes**    

V=(dog,cat,....)   
$$cat\in \{0,1\} ,dog \in \{0,1\}$$    
P(cat=1|Label)~Bernoulli     
P(dog=1|Label)~Bernoulli     
....

![_config.yml]({{ site.baseurl }}/images/1PGM/image44.png)    

**Multinomial Naive Bayes**    
n是文本长度  

$$W=(W_{1},W_{2}....W_{n})$$     

$$W_{i} \in \{"cat","dog"...\}$$    

$$
P(W_{i} |Label)\sim Multinomial\\
\sum_{W_{i}}P(W_{i}\mid Label)= 1
$$

![_config.yml]({{ site.baseurl }}/images/1PGM/image45.png)   

**Summary**  

![_config.yml]({{ site.baseurl }}/images/1PGM/image46.png)   

## Application Medical Diagbosis    
Medical Diagbosis:Pathfinder 的发展（略）  

In reality, events can have 0 probabilities; the problem is that it is very difficult to be sure of which events truly have 0 probabilities and which events have small but non-zero probabilities. Wrongly ascribing a 0 probability to an event means that no amount of evidence to the contrary can change things, which is dangerous; on the other hand, if we assign an event a low but non-zero probability and it turns out that the event actually has a 0 probability in reality, not much harm is done because the event is improbable anyway.
