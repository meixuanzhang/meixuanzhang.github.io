---
layout: post
title: 译Lecture 4 Exact Inference
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍推理问题，并在图模型中找到确切的解决方案。

# 介绍  

在之前的讲座中，我们介绍了图模型的概念及其数学公式。现在我们知道我们可以使用图模型$$M$$（贝叶斯网络或无向图模型）来描述满足某些条件独立性的概率分布$$P_{M}$$。在本课中，我们将学习如何使用图模型。给定GM$$M$$，我们通常有两类任务.   

**推理**：回答与由$$M$$定义的概率分布$$P_{M}$$有关的询问，例如$$P_{M}(X\mid Y)$$,$$X$$和$$Y$$是GM $$M$$的变量子集

**学习**：从数据$$D$$中估计一个可信的模型$$D$$,我们将获得$$M$$的点估计的过程称为学习.但是对于贝叶斯，他们寻求$$p(M\mid D)$$的后验分布,这实际上是一个推理问题。学习任务与推理任务密切相关。当我们要计算$$M$$的点估计时，如果不是所有的变量都是可观测的，我们需要进行推断来估算缺失的数据。因此学习算法通常使用推理作为子程序.  

(设总体$$X$$的分布函数形式已知,但它的一个或多个参数为未知,借助于总体$$X$$的一个样本来估计总体未知参数的值的问题称为点估计问题).  

## 推理问题  

在这里，我们将研究与由GM $$M$$定义的概率分布$$P$$相关的不同类型的问题。

**可能性(LIKELIHOOD)**   

一个人可能会问的大多数问题都涉及到证据(evidence).所以我们首先介绍证据的定义。证据$$\mathbf{e}$$是一组变量$$\mathbf{E}$$的赋值. 在不失一般性的前提下，我们假设$$\mathbf{E}=X_{k+1},..,X_{k}$$.   

最简单的问题是证据$$\mathbf{e}$$的概率：   

$$P(\mathbf{e})=\sum_{x_{1}}...\sum_{x_{k}}P(x_{1},..,x_{k},\mathbf{e})$$  

这通常称为计算$$\mathbf{e}$$的可能性。  

**条件概率(CONDITIONAL PROBABILITY)**   

在给定证据$$\mathbf{e}$$的情况下，我们通常对变量$$X$$的条件概率感兴趣

$$P(X\mid \mathbf{e})=\frac{P(X, \mathbf{e})}{P(\mathbf{e})}=\frac{P(X, \mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}$$  

这是给定证据$$\mathbf{e}$$,$$X$$的后验信心(概率)。通常，我们只询问所有域变量$$X = Y,Z$$中的变量$$Y$$子集，而对其余的变量$$Z$$“不在乎”:  

$$P(Y\mid \mathbf{e})=\sum_{\mathbf{z}}P(Y,Z=\mathbf{z} \mid \mathbf{e})$$   

对“不关心”变量$$Z$$求和过程称为边际化，得到的$$P(Y\mid  \mathbf{e})$$称为边际概率。  

后验信心(posteriori belief)非常有用。这里我们展示了后验信心的一些应用：


## 推理方法  

![_config.yml]({{ site.baseurl }}/images/10708/image50.png) 

# 消元(Elimination)算法及实例  

## Elimination on Chains  

## Elimination in Hidden Markov Models  

## Takeaways from Examples 

## Variable Elimination Algorithm  

# Graph Elimination   

# Message Passing Algorithms

$$p_{\theta}(y\mid x)=\frac{1}{Z}exp\{\sum_{c}\theta_{c}f_{c(x,y_{c})}\}$$   



参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



