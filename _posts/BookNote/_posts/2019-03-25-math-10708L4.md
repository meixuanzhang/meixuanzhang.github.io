---
layout: post
title: 译Lecture 4 Exact Inference
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍推理问题，并在图模型中找到确切的解决方案。

# 介绍  

在之前的讲座中，我们介绍了图模型的概念及其数学公式。现在我们知道我们可以使用图模型$$M$$（贝叶斯网络或无向图模型）来描述满足某些条件独立性的概率分布$$P_{M}$$。在本课中，我们将学习如何使用图模型。给定GM$$M$$，我们通常有两类任务.   

**推理**：回答与由$$M$$定义的概率分布$$P_{M}$$有关的询问，例如$$P_{M}(X\mid Y)$$,$$X$$和$$Y$$是GM $$M$$的变量子集

**学习**：从数据$$D$$中估计一个可信的模型$$D$$,我们将获得$$M$$的点估计的过程称为学习.但是对于贝叶斯，他们寻求$$p(M\mid D)$$的后验分布,这实际上是一个推理问题。学习任务与推理任务密切相关。当我们要计算$$M$$的点估计时，如果不是所有的变量都是可观测的，我们需要进行推断来估算缺失的数据。因此学习算法通常使用推理作为子程序.  

(设总体$$X$$的分布函数形式已知,但它的一个或多个参数为未知,借助于总体$$X$$的一个样本来估计总体未知参数的值的问题称为点估计问题).  

## 推理问题  

在这里，我们将研究与由GM $$M$$定义的概率分布$$P$$相关的不同类型的问题。

**可能性(LIKELIHOOD)**   

一个人可能会问的大多数问题都涉及到证据(evidence).所以我们首先介绍证据的定义。证据$$\mathbf{e}$$是一组变量$$\mathbf{E}$$的赋值. 在不失一般性的前提下，我们假设$$\mathbf{E}=X_{k+1},..,X_{k}$$.   

最简单的问题是证据$$\mathbf{e}$$的概率：   

$$P(\mathbf{e})=\sum_{x_{1}}...\sum_{x_{k}}P(x_{1},..,x_{k},\mathbf{e})$$  

这通常称为计算$$\mathbf{e}$$的可能性。  

**条件概率(CONDITIONAL PROBABILITY)**   

在给定证据$$\mathbf{e}$$的情况下，我们通常对变量$$X$$的条件概率感兴趣

$$P(X\mid \mathbf{e})=\frac{P(X, \mathbf{e})}{P(\mathbf{e})}=\frac{P(X, \mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}$$  

这是给定证据$$\mathbf{e}$$,$$X$$的后验信心(概率)。通常，我们只询问所有域变量$$X = Y,Z$$中的变量$$Y$$子集，而对其余的变量$$Z$$“不在乎”:  

$$P(Y\mid \mathbf{e})=\sum_{\mathbf{z}}P(Y,Z=\mathbf{z} \mid \mathbf{e})$$   

对“不关心”变量$$Z$$求和过程称为边际化，得到的$$P(Y\mid  \mathbf{e})$$称为边际概率。  

后验信心(posteriori belief)非常有用。这里我们展示了后验信心的一些应用：

+ 预测：在给定起始条件的情况下计算结果的可能性。 

链模型中的预测示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image52.png)   

在这种类型的问题中，问题节点是证据的后代。如果我们已知变量$$A,B$$取值，输出的概率是后验信心$$P(C\mid A,B)$$.使用图中的条件独立性$$C\bot A\mid B$$，可以将其简化为$$P(C\mid A,B)=P(C\mid B)$$

+ 诊断(Diagnosis)：给定症状计算疾病/故障的概率。 

链模型中的诊断示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image53.png)   

在这种类型的问题中，问题节点是证据的祖先。在GM $$M$$中，如果我们知道变量$$B,C$$取值，因(cause)的概率是一种后验信心$$P(A\mid B,C)$$。再次使用条件概率，可以将其简化为$$P(A\mid B,C)=P(A\mid B)$$

+ 学习：在对变量进行部分观察的情况下进行学习时，我们需要在学习算法中计算后验信心。 在EM算法中，我们将使用后验信心填充未观察到的变量作为算法的一部分。 稍后我们将介绍有关学习算法的更多详细信息。

变量之间的信息流不受GM中边的方向性限制。 实际上，我们可以结合网络各个部分的证据来进行概率推断。Deep Belief Network(DBN)是一个示例.  DBN是具有多层的生成模型或受限玻尔兹曼机(RBM)。该模型成功解决了诸如识别手写数字，学习运动捕捉数据，协作过滤之类的任务。下图显示了具有3个隐藏层的DBN。 我们可以从数据$$V$$中推断出隐藏单元$$H_{1},H_{2},H_{3}$$,可以通过在相反方向上对隐藏单元$$H$$进行采样来生成数据$$V$$.  

一个有三层隐藏的DBN，用于图像处理。  

![_config.yml]({{ site.baseurl }}/images/10708/image54.png) 

**MOST PROBABLE ASSIGNMENT**   

另一个有趣的问题是为感兴趣变量的找到最可能联合赋值(most probable joint assignment MPA)。这种推理通常在某些证据下进行，忽略一些“不关心”的变量$$Z$$  

$$MPA(Y\mid e)=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y}\mid \mathbf{e})=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y},\mathbf{z}\mid \mathbf{e})$$   

从方程中可以看出，MPA是$$Y$$的最大后验概率。  

给定GM$$M$$时，此问题通常对预测有用。  

+ 分类：根据证据找出最可能的标签    

+ 解释(Explanation)：根据证据找到最可能的情况。  

重要提示：变量的MPA取决于问题的“上下文”——变量集被联合询问。例如，$$y_{1}$$和$$y_{2}$$概率分布如下表所示.当我们计算$$y_{1}$$的MPA时,首先计算边际概率$$p(y_{1}=0)=0.4,p(y_{1}=1)=0.6$$,MPA 是$$\mathop{\arg\max}_{y_{1}}p(y_{1})=1$$.另一方面，$$y_{1},y_{2}$$的MPA是$$\mathop{\arg\max}_{y_{1}}p(y_{1},y_{2})=(0,0)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image55.png) 

## 推理方法  

推理通常是一个难题。实际上，有一个定理表明，在GM中计算$$P(X= \mathbf{x}\mid \mathbf{e})$$是NP-hard。但是，NP-hardness并不意味着我们无法解决推理。 该定理意味着我们找不到适用于任意GM的通用推理过程。 我们仍然有机会为某些特定的GM找到可证明有效的算法。  

有许多推理方法可以用于GM中。它们可以分为两类：  

+ 精确推理算法(Exact inference algorithms). Including the elimination algorithm, message-passing algorithm (sum-product, belief propagation), the junction tree algorithms.  这些算法可以对问题给出精确的结果。 本讲座的主要主题是精确推理算法。    

+ 近似推理技术(Approximate inference techniques). Including stochastic simulation / sampling methods, Markov chain Monte Carlo (MCMC) methods, variational algorithms.这些算法只给出了推理问题的近似答案。我们将在以后的讲座中介绍这些方法。   

# 消除(Elimination)算法及实例    

既然我们理解了推理的问题，我们将研究一些简单的案例，以为直观的通用方法建立直觉。  

## Elimination on Chains    

考虑一个关于变量$$A,B,C,D,E$$的简单链，如下所示:   

Chain PGM.  

![_config.yml]({{ site.baseurl }}/images/10708/image56.png) 

想象一下，无论$$A,B,C,D$$的值如何，我们都希望计算$$E=e$$的概率。我们可以把联合概率加起来：  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}\sum_{a}P(a,b,c,d,e)$$   

这将需要指数形式的项。幸运的是,我们可以使用贝叶斯网络的属性来减少此计算成本。 由于贝叶斯网络对条件独立性进行编码，因此可以将联合概率分解为：   

$$P(e)=\sum_{d}\sum_{c}\sum_{b}\sum_{a}P(a)P(b\mid a)P(c\mid b)P(d\mid c)P(e\mid d)$$  

这种分解允许我们分离条件独立变量，因此我们可以插入和分离求和，如下所示：  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}P(c\mid b)P(d\mid c)P(e\mid d)\sum_{a}P(a)P(b\mid a)$$    

聚焦于最后一个项$$\sum_{a}P(a)P(b\mid a)$$,我们看到通过$$a$$进行边际化，只剩下$$b$$的函数.我们通常将其称为$$\phi(b)$$,但在语义上等效于$$P(b)$$。对于$$e$$的边际概率,我们剩下以下表达式:  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}P(c\mid b)P(d\mid c)P(e\mid d)P(b)$$

注意，由于变量$$a$$不再是这个表达式的一部分，我们将说$$a$$已被消除。因此，我们可以根据情况使用新的图模型： 

消除$$A$$后的图模型。  

![_config.yml]({{ site.baseurl }}/images/10708/image57.png)   

重复此过程，我们得到下列序列：  

$$
P(e)=\sum_{d}\sum_{c}P(d\mid c)P(e\mid d)\sum_{b}P(c\mid b)P(b)\\
=\sum_{d}\sum_{c}P(d\mid c)P(e\mid d)P(c)\\
=\sum_{d}P(e\mid d)\sum_{c}P(d\mid c)P(c)\\
=\sum_{d}P(e\mid d)P(d)
$$ 

由于每个消除步骤的成本为$$O(Val\mid X_{i}\mid \times \mid X_{i+1}\mid)$$，因此总体复杂度为$$O(nk^2)$$，比起联合概率朴素求和的指数运行时间，整体上有很大改进。  

## Elimination in Hidden Markov Models    

现在，我们将考虑一个经常在时间序列分析和自然语言处理中使用的模型，称为隐马尔可夫模型。

![_config.yml]({{ site.baseurl }}/images/10708/image58.png)   

在给定观测序列的情况下，我们可以很天真地找到$$y_{i}$$的条件概率，但是使用消除法，我们可以获得与链式(Elimination on Chains)例子中类似的计算优势。 

在这个模型中，我们有两个直观的选择来消除变量的顺序。我们可以从第一个时间步骤（称为向前算法）开始，也可以从最后一个时间步骤（称为向后算法）开始。  

注意，求和符号将表示对所有随机变量$$y$$求和，除了第$$i$$个变量$$y_{i}$$  

**FORWARD ALGORITHM**   

$$
P(y_{i}\mid x_{1},..,x_{T})=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\sum_{y_{1}}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})\\
=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\phi(x_{1},y_{2})\\
=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})P(x_{1},y_{2})

$$

我们可以继续这种模式，每个中间项$$\phi(\cdot)$$代表一个联合概率。  

**BACKWARD ALGORITHM**  

如果我们选择从链的开始消除变量，我们将首先按如下方式对因素进行分组：  

$$
P(y_{i}\mid x_{1},..,x_{T})=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) \sum_{y_{T}}P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\\
=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) \phi(x_{T},y_{T-1})\\
=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) P(x_{T}\mid y_{T-1})
$$  

我们可以继续这种模式，每个中间项$$\phi(\cdot)$$代表一个条件概率。  

## Takeaways from Examples  

我们的探索的主要成果是消除提供了一种有效进行精确推理的系统方法，虽然我们通常可以创建中间因子，但中间因子因素的语义可能会有所不同。

## Variable Elimination Algorithm   

从这些例子中，我们可以将上面例子中使用的技术合并为一个更通用的算法，称为变量消除(Variable Elimination)。  

注意，上面例子中的一个常见操作是取因子的乘积，然后对变量值求和。这是一般和积运算(Sum-Product)。  

$$\mathbf{X}$$：变量集   
$$F$$:因子集，$$\phi \in F$$,$$Scope[\phi] \in \mathbf{X}$$     
$$\mathbf{Y} \subset \mathbf{X} $$: query variables   
$$\mathbf{Z}=\mathbf{X}-\mathbf{Y}$$ 待消除的变量集      

$$\tau(\mathbf{Y})=\sum_{\mathbf{z}}\prod_{\phi \in F} \phi$$  

$$\mathbf{z}$$是要消除的变量  

此外，我们希望采用一种合并证据$$E=e$$的方法，以推广我们对上述因子的使用。 因为我们将维护一个因子列表并对其进行遍历，所以定义新因子明确包含证据是有利的，而不是在原始因子中包含固定变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image71.png)   

$$\mathbf{e}$$是$$\mathbf{E}$$，$$\mathbf{\bar{e}}$$是已知取值    

掌握了这些概念后，我们可以概述新算法。  

查询$$P(X_{1}\mid e)$$,我们首先关注联合概率$$P(X_{1},e)$$,$$P(X_{1},e)=\sum_{x_{n}}..\sum_{x_{2}}\prod_{i}P(x_{i}\mid pa_{i})$$.这表明变量具有隐式的“消除顺序”. 

按照上述规定的顺序：   

+ 将所有相关项移到innermost sum，然后移出所有不相关的项。   

+ 对innermost sum里项执行求和积运算( Sum-Product)，生成一个新的因子$$\phi$$ 。   

+ 重复直到计算出整个联合结果。  

计算所需的查询：    

$$P(X_{1}\mid e)=\frac{\phi(X_{1},e)}{\sum_{X_{1}}\phi(X_{1},e)}$$  

![_config.yml]({{ site.baseurl }}/images/10708/image72.png)   


# Graph Elimination    

在本节中，我们将分析Variable Elimination (VE)算法的复杂性。我们首先根据算法流程进行了基本分析，这可以让我们了解复杂性的瓶颈。然后我们表明，VE的每个步骤都可以看作是一个图形转换步骤，这可以让我们从图形角度更清晰地分析算法的复杂性。 我们还将VA的图形透视图形式化为图形消除算法。   

## Basic Complexity Analysis   

从上一节我们知道VE可以大大降低推理复杂度。设$$n$$为完全联合概率中变量的个数，$$m$$为c初始因子( initial factors)的数量包括原始因素(original factors)和证据势(evidence potentials)。 我们已经看到，VE是一种n步迭代消除算法。   

在每个步骤中，算法都会选择一个变量$$X_{i}$$，仅对涉及$$X_{i}$$的因子的子集的乘积执行汇总操作。$$N_{i}$$表示为因子子集中的变量数$$(y_{1},y_{2},..,y_{N_{i}})$$。我们可以将步骤正式写为：   

$$m_{i}(y_{1},...,y_{N_{i}}) = \sum_{x_{i}\in Val(X_{i})}\prod_{j=1}^{k_{i}}m(x_{i},y_{ci})$$  

$$k_{i}$$表示包含$$X_{i}$$的因子个数    

假设每个变量取值个数不超过$$v$$,对于$$(y_{1},..y_{N_{i}})$$可取值数为$$v^{N_{i}}$$(涉及排列组合)，每个取值有$$Val(X_{i})k_{i}\leqslant vN_{i}$$个乘积运算和$$Val(X_{i})\leqslant v$$个加法运算。考虑到算法有n个步骤,使$$N_{max}=max_{i}N_{i}$$我们可以写下的复杂度是$$O(nv^{N_{max}})$$,因此，我们发现VE算法的计算成本主要取决于以指数增长的中间因子的最大值。

## VE to Graph Elimination: Example    

我们已经看到VE算法的瓶颈是中间因子的最大值。它受消除顺序的影响。看一个示例，该示例将VE中的迭代消除步骤与一系列图结构转换联系在一起。提供了一种基于图消除的可视化方式来分析复杂性。关于VE的计算复杂度的问题可以简化为纯粹的图论来考虑。给定贝叶斯网络分解如下图所示，我们将通过VE推理$$P(A\mid h)$$,初始因子为：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)P(f\mid a)P(g\mid e)P(h\mid e,f)$$在执行VE之前，我们选择消除顺序为$$H,G,F,E,D,C,B$$:  

![_config.yml]({{ site.baseurl }}/images/10708/image59.png)   

Step 1: to handle conditioning h  

$$H$$变量节点是观察节点，我们可以添加额外的证据指标(evidence indicator)因子，将对观察证据的处理化作为边缘化步骤:  

$$m_{h}(e,f)=P(h=\bar{h}\mid e,f)=\sum_{h}P(h\mid e,f)\delta(h=\bar{h})$$

因子乘积变成：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)P(f\mid a)P(g\mid e)m_{h}(e,f)$$  

**Graph transformation:**对$$h$$处理后，去掉旧因子中的$$h$$，生成新因子$$m_{h}(e,f)$$.从图演化的角度来看，如下所示，删除节点$$h$$这对应于在我们消除旧因子中的$$h$$,由于生成的新因子取决于E和F，因此在节点E和F之间也增加了一条边。简化后的因子分解是根据新图因子分解的gibbs分布

![_config.yml]({{ site.baseurl }}/images/10708/image60.png)   

Step 2: eliminate G    



![_config.yml]({{ site.baseurl }}/images/10708/image61.png)   

![_config.yml]({{ site.baseurl }}/images/10708/image62.png)   

![_config.yml]({{ site.baseurl }}/images/10708/image63.png)   

![_config.yml]({{ site.baseurl }}/images/10708/image74.png)   

![_config.yml]({{ site.baseurl }}/images/10708/image75.png)   

## VE to Graph Elimination (GE): Formal Connection  

**Graph Elimination Algorithm :**  

## Complexity Analysis in Graph Perspective  

## Elimination Ordering

# Message Passing Algorithms

$$p_{\theta}(y\mid x)=\frac{1}{Z}exp\{\sum_{c}\theta_{c}f_{c(x,y_{c})}\}$$   



参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



