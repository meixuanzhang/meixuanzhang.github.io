---
layout: post
title: 译Lecture 4 Exact Inference
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍推理问题，并在图模型中找到确切的解决方案。

# 介绍  

在之前的讲座中，我们介绍了图模型的概念及其数学公式。现在我们知道我们可以使用图模型$$M$$（贝叶斯网络或无向图模型）来描述满足某些条件独立性的概率分布$$P_{M}$$。在本课中，我们将学习如何使用图模型。给定GM$$M$$，我们通常有两类任务.   

**推理**：回答与由$$M$$定义的概率分布$$P_{M}$$有关的询问，例如$$P_{M}(X\mid Y)$$,$$X$$和$$Y$$是GM $$M$$的变量子集

**学习**：从数据$$D$$中估计一个可信的模型$$D$$,我们将获得$$M$$的点估计的过程称为学习.但是对于贝叶斯，他们寻求$$p(M\mid D)$$的后验分布,这实际上是一个推理问题。学习任务与推理任务密切相关。当我们要计算$$M$$的点估计时，如果不是所有的变量都是可观测的，我们需要进行推断来估算缺失的数据。因此学习算法通常使用推理作为子程序.  

(设总体$$X$$的分布函数形式已知,但它的一个或多个参数为未知,借助于总体$$X$$的一个样本来估计总体未知参数的值的问题称为点估计问题).  

## 推理问题  

在这里，我们将研究与由GM $$M$$定义的概率分布$$P$$相关的不同类型的问题。

**可能性(LIKELIHOOD)**   

一个人可能会问的大多数问题都涉及到证据(evidence).所以我们首先介绍证据的定义。证据$$\mathbf{e}$$是一组变量$$\mathbf{E}$$的赋值. 在不失一般性的前提下，我们假设$$\mathbf{E}=X_{k+1},..,X_{k}$$.   

最简单的问题是证据$$\mathbf{e}$$的概率：   

$$P(\mathbf{e})=\sum_{x_{1}}...\sum_{x_{k}}P(x_{1},..,x_{k},\mathbf{e})$$  

这通常称为计算$$\mathbf{e}$$的可能性。  

**条件概率(CONDITIONAL PROBABILITY)**   

在给定证据$$\mathbf{e}$$的情况下，我们通常对变量$$X$$的条件概率感兴趣

$$P(X\mid \mathbf{e})=\frac{P(X, \mathbf{e})}{P(\mathbf{e})}=\frac{P(X, \mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}$$  

这是给定证据$$\mathbf{e}$$,$$X$$的后验信心(概率)。通常，我们只询问所有域变量$$X = Y,Z$$中的变量$$Y$$子集，而对其余的变量$$Z$$“不在乎”:  

$$P(Y\mid \mathbf{e})=\sum_{\mathbf{z}}P(Y,Z=\mathbf{z} \mid \mathbf{e})$$   

对“不关心”变量$$Z$$求和过程称为边际化，得到的$$P(Y\mid  \mathbf{e})$$称为边际概率。  

后验信心(posteriori belief)非常有用。这里我们展示了后验信心的一些应用：

+ 预测：在给定起始条件的情况下计算结果的可能性。 

链模型中的预测示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image52.png)   

在这种类型的问题中，问题节点是证据的后代。如果我们已知变量$$A,B$$取值，输出的概率是后验信心$$P(C\mid A,B)$$.使用图中的条件独立性$$C\bot A\mid B$$，可以将其简化为$$P(C\mid A,B)=P(C\mid B)$$

+ 诊断(Diagnosis)：给定症状计算疾病/故障的概率。 

链模型中的诊断示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image53.png)   

在这种类型的问题中，问题节点是证据的祖先。在GM $$M$$中，如果我们知道变量$$B,C$$取值，因(cause)的概率是一种后验信心$$P(A\mid B,C)$$。再次使用条件概率，可以将其简化为$$P(A\mid B,C)=P(A\mid B)$$

+ 学习：在对变量进行部分观察的情况下进行学习时，我们需要在学习算法中计算后验信心。 在EM算法中，我们将使用后验信心填充未观察到的变量作为算法的一部分。 稍后我们将介绍有关学习算法的更多详细信息。

变量之间的信息流不受GM中边的方向性限制。 实际上，我们可以结合网络各个部分的证据来进行概率推断。Deep Belief Network(DBN)是一个示例.  DBN是具有多层的生成模型或受限玻尔兹曼机(RBM)。该模型成功解决了诸如识别手写数字，学习运动捕捉数据，协作过滤之类的任务。下图显示了具有3个隐藏层的DBN。 我们可以从数据$$V$$中推断出隐藏单元$$H_{1},H_{2},H_{3}$$,可以通过在相反方向上对隐藏单元$$H$4进行采样来生成数据$$V$$.  

一个有三层隐藏的DBN，用于图像处理。  

![_config.yml]({{ site.baseurl }}/images/10708/image54.png) 

**MOST PROBABLE ASSIGNMENT**   

另一个有趣的问题是为感兴趣变量的找到最可能联合赋值(most probable joint assignment MPA)。这种推理通常在某些证据下进行，忽略一些“不关心”的变量$$Z$$  

$$MPA(Y\mid e)=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y}\mid \mathbf{e})=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y},\mathbf{z}\mid \mathbf{e})$$   

从方程中可以看出，MPA是$$Y$$的最大后验概率。  

给定GM$$M$$时，此问题通常对预测有用。  

+ 分类：根据证据找出最可能的标签    

+ 解释(Explanation)：根据证据找到最可能的情况。  

重要提示：变量的MPA取决于问题的“上下文”——变量集被联合询问。例如，$$y_{1}$$和$$y_{2}$$概率分布如下表所示.当我们计算$$y_{1}$$的MPA时,首先计算边际概率$$p(y_{1}=0)=0.4,p(y_{1}=1)=0.6$$,MPA 是$$\mathop{\arg\max}_{y_{1}}p(y_{1})=1$$.另一方面，$$y_{1},y_{2}$$的MPA是$$\mathop{\arg\max}_{y_{1}}p(y_{1},y_{2})=(0,0)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image55.png) 

## 推理方法  

![_config.yml]({{ site.baseurl }}/images/10708/image50.png) 

# 消元(Elimination)算法及实例  

## Elimination on Chains  

## Elimination in Hidden Markov Models  

## Takeaways from Examples 

## Variable Elimination Algorithm  

# Graph Elimination   

# Message Passing Algorithms

$$p_{\theta}(y\mid x)=\frac{1}{Z}exp\{\sum_{c}\theta_{c}f_{c(x,y_{c})}\}$$   



参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



