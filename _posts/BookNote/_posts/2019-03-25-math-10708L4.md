---
layout: post
title: 译Lecture 4 Exact Inference
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

介绍推理问题，并在图模型中找到确切的解决方案。

# 介绍  

在之前的讲座中，我们介绍了图模型的概念及其数学公式。现在我们知道我们可以使用图模型$$M$$（贝叶斯网络或无向图模型）来描述满足某些条件独立性的概率分布$$P_{M}$$。在本课中，我们将学习如何使用图模型。给定GM$$M$$，我们通常有两类任务.   

**推理**：回答与由$$M$$定义的概率分布$$P_{M}$$有关的询问，例如$$P_{M}(X\mid Y)$$,$$X$$和$$Y$$是GM $$M$$的变量子集

**学习**：从数据$$D$$中估计一个可信的模型$$D$$,我们将获得$$M$$的点估计的过程称为学习.但是对于贝叶斯，他们寻求$$p(M\mid D)$$的后验分布,这实际上是一个推理问题。学习任务与推理任务密切相关。当我们要计算$$M$$的点估计时，如果不是所有的变量都是可观测的，我们需要进行推断来估算缺失的数据。因此学习算法通常使用推理作为子程序.  

(设总体$$X$$的分布函数形式已知,但它的一个或多个参数为未知,借助于总体$$X$$的一个样本来估计总体未知参数的值的问题称为点估计问题).  

## 推理问题  

在这里，我们将研究与由GM $$M$$定义的概率分布$$P$$相关的不同类型的问题。

**可能性(LIKELIHOOD)**   

一个人可能会问的大多数问题都涉及到证据(evidence).所以我们首先介绍证据的定义。证据$$\mathbf{e}$$是一组变量$$\mathbf{E}$$的赋值. 在不失一般性的前提下，我们假设$$\mathbf{E}=X_{k+1},..,X_{k}$$.   

最简单的问题是证据$$\mathbf{e}$$的概率：   

$$P(\mathbf{e})=\sum_{x_{1}}...\sum_{x_{k}}P(x_{1},..,x_{k},\mathbf{e})$$  

这通常称为计算$$\mathbf{e}$$的可能性。  

**条件概率(CONDITIONAL PROBABILITY)**   

在给定证据$$\mathbf{e}$$的情况下，我们通常对变量$$X$$的条件概率感兴趣

$$P(X\mid \mathbf{e})=\frac{P(X, \mathbf{e})}{P(\mathbf{e})}=\frac{P(X, \mathbf{e})}{\sum_{x}P(X=x,\mathbf{e})}$$  

这是给定证据$$\mathbf{e}$$,$$X$$的后验信心(概率)。通常，我们只询问所有域变量$$X = Y,Z$$中的变量$$Y$$子集，而对其余的变量$$Z$$“不在乎”:  

$$P(Y\mid \mathbf{e})=\sum_{\mathbf{z}}P(Y,Z=\mathbf{z} \mid \mathbf{e})$$   

对“不关心”变量$$Z$$求和过程称为边际化，得到的$$P(Y\mid  \mathbf{e})$$称为边际概率。  

后验信心(posteriori belief)非常有用。这里我们展示了后验信心的一些应用：

+ 预测：在给定起始条件的情况下计算结果的可能性。 

链模型中的预测示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image52.png)   

在这种类型的问题中，问题节点是证据的后代。如果我们已知变量$$A,B$$取值，输出的概率是后验信心$$P(C\mid A,B)$$.使用图中的条件独立性$$C\bot A\mid B$$，可以将其简化为$$P(C\mid A,B)=P(C\mid B)$$

+ 诊断(Diagnosis)：给定症状计算疾病/故障的概率。 

链模型中的诊断示例。 绿色节点是可观察到的变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image53.png)   

在这种类型的问题中，问题节点是证据的祖先。在GM $$M$$中，如果我们知道变量$$B,C$$取值，因(cause)的概率是一种后验信心$$P(A\mid B,C)$$。再次使用条件概率，可以将其简化为$$P(A\mid B,C)=P(A\mid B)$$

+ 学习：在对变量进行部分观察的情况下进行学习时，我们需要在学习算法中计算后验信心。 在EM算法中，我们将使用后验信心填充未观察到的变量作为算法的一部分。 稍后我们将介绍有关学习算法的更多详细信息。

变量之间的信息流不受GM中边的方向性限制。 实际上，我们可以结合网络各个部分的证据来进行概率推断。Deep Belief Network(DBN)是一个示例.  DBN是具有多层的生成模型或受限玻尔兹曼机(RBM)。该模型成功解决了诸如识别手写数字，学习运动捕捉数据，协作过滤之类的任务。下图显示了具有3个隐藏层的DBN。 我们可以从数据$$V$$中推断出隐藏单元$$H_{1},H_{2},H_{3}$$,可以通过在相反方向上对隐藏单元$$H$$进行采样来生成数据$$V$$.  

一个有三层隐藏的DBN，用于图像处理。  

![_config.yml]({{ site.baseurl }}/images/10708/image54.png) 

**MOST PROBABLE ASSIGNMENT**   

另一个有趣的问题是为感兴趣变量的找到最可能联合赋值(most probable joint assignment MPA)。这种推理通常在某些证据下进行，忽略一些“不关心”的变量$$Z$$  

$$MPA(Y\mid e)=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y}\mid \mathbf{e})=\mathop{\arg\max}_{ \mathbf{y}\in Y}P(\mathbf{y},\mathbf{z}\mid \mathbf{e})$$   

从方程中可以看出，MPA是$$Y$$的最大后验概率。  

给定GM$$M$$时，此问题通常对预测有用。  

+ 分类：根据证据找出最可能的标签    

+ 解释(Explanation)：根据证据找到最可能的情况。  

重要提示：变量的MPA取决于问题的“上下文”——变量集被联合询问。例如，$$y_{1}$$和$$y_{2}$$概率分布如下表所示.当我们计算$$y_{1}$$的MPA时,首先计算边际概率$$p(y_{1}=0)=0.4,p(y_{1}=1)=0.6$$,MPA 是$$\mathop{\arg\max}_{y_{1}}p(y_{1})=1$$.另一方面，$$y_{1},y_{2}$$的MPA是$$\mathop{\arg\max}_{y_{1}}p(y_{1},y_{2})=(0,0)$$  

![_config.yml]({{ site.baseurl }}/images/10708/image55.png) 

## 推理方法  

推理通常是一个难题。实际上，有一个定理表明，在GM中计算$$P(X= \mathbf{x}\mid \mathbf{e})$$是NP-hard。但是，NP-hardness并不意味着我们无法解决推理。 该定理意味着我们找不到适用于任意GM的通用推理过程。 我们仍然有机会为某些特定的GM找到可证明有效的算法。  

有许多推理方法可以用于GM中。它们可以分为两类：  

+ 精确推理算法(Exact inference algorithms). Including the elimination algorithm, message-passing algorithm (sum-product, belief propagation), the junction tree algorithms.  这些算法可以对问题给出精确的结果。 本讲座的主要主题是精确推理算法。    

+ 近似推理技术(Approximate inference techniques). Including stochastic simulation / sampling methods, Markov chain Monte Carlo (MCMC) methods, variational algorithms.这些算法只给出了推理问题的近似答案。我们将在以后的讲座中介绍这些方法。   

# 消除(Elimination)算法及实例    

既然我们理解了推理的问题，我们将研究一些简单的案例，以为直观的通用方法建立直觉。  

## Elimination on Chains    

考虑一个关于变量$$A,B,C,D,E$$的简单链，如下所示:   

Chain PGM.  

![_config.yml]({{ site.baseurl }}/images/10708/image56.png) 

想象一下，无论$$A,B,C,D$$的值如何，我们都希望计算$$E=e$$的概率。我们可以把联合概率加起来：  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}\sum_{a}P(a,b,c,d,e)$$   

这将需要指数形式的项。幸运的是,我们可以使用贝叶斯网络的属性来减少此计算成本。 由于贝叶斯网络对条件独立性进行编码，因此可以将联合概率分解为：   

$$P(e)=\sum_{d}\sum_{c}\sum_{b}\sum_{a}P(a)P(b\mid a)P(c\mid b)P(d\mid c)P(e\mid d)$$  

这种分解允许我们分离条件独立变量，因此我们可以插入和分离求和，如下所示：  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}P(c\mid b)P(d\mid c)P(e\mid d)\sum_{a}P(a)P(b\mid a)$$    

聚焦于最后一个项$$\sum_{a}P(a)P(b\mid a)$$,我们看到通过$$a$$进行边际化，只剩下$$b$$的函数.我们通常将其称为$$\phi(b)$$,但在语义上等效于$$P(b)$$。对于$$e$$的边际概率,我们剩下以下表达式:  

$$P(e)=\sum_{d}\sum_{c}\sum_{b}P(c\mid b)P(d\mid c)P(e\mid d)P(b)$$

注意，由于变量$$a$$不再是这个表达式的一部分，我们将说$$a$$已被消除。因此，我们可以根据情况使用新的图模型： 

消除$$A$$后的图模型。  

![_config.yml]({{ site.baseurl }}/images/10708/image57.png)   

重复此过程，我们得到下列序列：  

$$
P(e)=\sum_{d}\sum_{c}P(d\mid c)P(e\mid d)\sum_{b}P(c\mid b)P(b)\\
=\sum_{d}\sum_{c}P(d\mid c)P(e\mid d)P(c)\\
=\sum_{d}P(e\mid d)\sum_{c}P(d\mid c)P(c)\\
=\sum_{d}P(e\mid d)P(d)
$$ 

由于每个消除步骤的成本为$$O(Val\mid X_{i}\mid \times \mid X_{i+1}\mid)$$，因此总体复杂度为$$O(nk^2)$$，比起联合概率朴素求和的指数运行时间，整体上有很大改进。  

## Elimination in Hidden Markov Models    

现在，我们将考虑一个经常在时间序列分析和自然语言处理中使用的模型，称为隐马尔可夫模型。

![_config.yml]({{ site.baseurl }}/images/10708/image58.png)   

在给定观测序列的情况下，我们可以很天真地找到$$y_{i}$$的条件概率，但是使用消除法，我们可以获得与链式(Elimination on Chains)例子中类似的计算优势。 

在这个模型中，我们有两个直观的选择来消除变量的顺序。我们可以从第一个时间步骤（称为向前算法）开始，也可以从最后一个时间步骤（称为向后算法）开始。  

注意，求和符号将表示对所有随机变量$$y$$求和，除了第$$i$$个变量$$y_{i}$$  

**FORWARD ALGORITHM**   

$$
P(y_{i}, x_{1},..,x_{T})=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\sum_{y_{1}}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})\\
=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\phi(x_{1},y_{2})\\
=\sum_{y-1,-i}P(x_{2}\mid y_{2})P(y_{3}\mid y_{2})...P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})P(x_{1},y_{2})

$$

我们可以继续这种模式，每个中间项$$\phi(\cdot)$$代表一个联合概率。  

**BACKWARD ALGORITHM**  

如果我们选择从链的开始消除变量，我们将首先按如下方式对因素进行分组：  

$$
P(y_{i}, x_{1},..,x_{T})=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) \sum_{y_{T}}P(y_{T}\mid y_{T-1})P(x_{T}\mid y_{T})\\
=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) \phi(x_{T},y_{T-1})\\
=\sum_{y-T,-i}P(y_{1})P(x_{1}\mid y_{1})P(y_{2}\mid y_{1})...P(x_{T-1}\mid y_{T-1})P(y_{T-1}\mid y_{T-2}) P(x_{T}\mid y_{T-1})
$$  

我们可以继续这种模式，每个中间项$$\phi(\cdot)$$代表一个条件概率。  

## Takeaways from Examples  

我们的探索的主要成果是消除提供了一种有效进行精确推理的系统方法，虽然我们通常可以创建中间因子，但中间因子因素的语义可能会有所不同。

## Variable Elimination Algorithm   

从这些例子中，我们可以将上面例子中使用的技术合并为一个更通用的算法，称为变量消除(Variable Elimination)。  

注意，上面例子中的一个常见操作是取因子的乘积，然后对变量值求和。这是一般和积运算(Sum-Product)。  

$$\mathbf{X}$$：变量集   
$$F$$:因子集，$$\phi \in F$$,$$Scope[\phi] \in \mathbf{X}$$     
$$\mathbf{Y} \subset \mathbf{X} $$: query variables   
$$\mathbf{Z}=\mathbf{X}-\mathbf{Y}$$ 待消除的变量集      

$$\tau(\mathbf{Y})=\sum_{\mathbf{z}}\prod_{\phi \in F} \phi$$  

$$\mathbf{z}$$是要消除的变量  

此外，我们希望采用一种合并证据$$E=e$$的方法，以推广我们对上述因子的使用。 因为我们将维护一个因子列表并对其进行遍历，所以定义新因子明确包含证据是有利的，而不是在原始因子中包含固定变量。  

![_config.yml]({{ site.baseurl }}/images/10708/image71.png)   

$$\mathbf{e}$$是$$\mathbf{E}$$，$$\mathbf{\bar{e}}$$是已知取值    

掌握了这些概念后，我们可以概述新算法。  

查询$$P(X_{1}\mid e)$$,我们首先关注联合概率$$P(X_{1},e)$$,$$P(X_{1},e)=\sum_{x_{n}}..\sum_{x_{2}}\prod_{i}P(x_{i}\mid pa_{i})$$.这表明变量具有隐式的“消除顺序”. 

按照上述规定的顺序：   

+ 将所有相关项移到innermost sum，然后移出所有不相关的项。   

+ 对innermost sum里项执行求和积运算( Sum-Product)，生成一个新的因子$$\phi$$ 。   

+ 重复直到计算出整个联合结果。  

计算所需的查询：    

$$P(X_{1}\mid e)=\frac{\phi(X_{1},e)}{\sum_{X_{1}}\phi(X_{1},e)}$$  

![_config.yml]({{ site.baseurl }}/images/10708/image72.png)   


# Graph Elimination    

在本节中，我们将分析Variable Elimination (VE)算法的复杂性。我们首先根据算法流程进行了基本分析，这可以让我们了解复杂性的瓶颈。然后我们表明，VE的每个步骤都可以看作是一个图形转换步骤，这可以让我们从图形角度更清晰地分析算法的复杂性。 我们还将VA的图形透视图形式化为图形消除算法。   

## Basic Complexity Analysis   

从上一节我们知道VE可以大大降低推理复杂度。设$$n$$为完全联合概率中变量的个数，$$m$$为c初始因子( initial factors)的数量包括原始因素(original factors)和证据势(evidence potentials)。 我们已经看到，VE是一种n步迭代消除算法。   

在每个步骤中，算法都会选择一个变量$$X_{i}$$，仅对涉及$$X_{i}$$的因子的子集的乘积执行汇总操作。$$N_{i}$$表示为因子子集中的变量数$$(y_{1},y_{2},..,y_{N_{i}})$$。我们可以将步骤正式写为：   

$$m_{i}(y_{1},...,y_{N_{i}}) = \sum_{x_{i}\in Val(X_{i})}\prod_{j=1}^{k_{i}}m(x_{i},y_{ci})$$  

$$k_{i}$$表示包含$$X_{i}$$的因子个数    

假设每个变量取值个数不超过$$v$$,对于$$(y_{1},..y_{N_{i}})$$可取值数为$$v^{N_{i}}$$(涉及排列组合)，每个取值有$$Val(X_{i})k_{i}\leqslant vN_{i}$$个乘积运算和$$Val(X_{i})\leqslant v$$个加法运算。考虑到算法有n个步骤,使$$N_{max}=max_{i}N_{i}$$我们可以写下的复杂度是$$O(nv^{N_{max}})$$,因此，我们发现VE算法的计算成本主要取决于以指数增长的中间因子的最大辖域。    

定义：假定$$\mathbf{D}$$表示随机变量的集合，因子$$\phi$$定义为$$Val(\mathbf{D})$$映射到实数域$$\mathbf{R}$$的一个函数。变量$$\mathbf{D}$$称为因子的辖域，记为$$Scope[\phi]$$      

## VE to Graph Elimination: Example    

我们已经看到VE算法的瓶颈是中间因子的最大值。它受消除顺序的影响。看一个示例，该示例将VE中的迭代消除步骤与一系列图结构转换联系在一起。提供了一种基于图消除的可视化方式来分析复杂性。关于VE的计算复杂度的问题可以简化为纯粹的图论来考虑。给定贝叶斯网络分解如下图所示，我们将通过VE推理$$P(A\mid h)$$,初始因子为：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)P(f\mid a)P(g\mid e)P(h\mid e,f)$$在执行VE之前，我们选择消除顺序为$$H,G,F,E,D,C,B$$:  

![_config.yml]({{ site.baseurl }}/images/10708/image59.jpg)   

Step 1: to handle conditioning $$H$$  

$$H$$变量节点是观察节点，我们可以添加额外的证据指标(evidence indicator)因子，将对观察证据的处理化作为边缘化步骤:  

$$m_{h}(e,f)=P(h=\bar{h}\mid e,f)=\sum_{h}P(h\mid e,f)\delta(h=\bar{h})$$

因子乘积变成：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)P(f\mid a)P(g\mid e)m_{h}(e,f)$$  

**Graph transformation:**对$$h$$处理后，去掉旧因子中的$$h$$，生成新因子$$m_{h}(e,f)$$.从图演化的角度来看，如下所示，删除节点$$h$$这对应于在我们消除旧因子中的$$h$$,由于生成的新因子取决于E和F，因此在节点E和F之间也增加了一条边。简化后的因子分解是根据新图因子分解的gibbs分布

![_config.yml]({{ site.baseurl }}/images/10708/image60.jpg)   

Step 2: eliminate $$G$$        

$$m_{g}(e)=\sum_{g}P(g\mid e)=1$$  

新的因子乘积：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)P(f\mid a)m_{g}(e)m_{h}(e,f)$$    

**Graph transformation:**  

只需如下删除节点$$G$$:   

![_config.yml]({{ site.baseurl }}/images/10708/image61.jpg)   

Step 3: eliminate $$F$$         

$$m_{f}(a,e)=\sum_{f}P(f\mid a)m_{h}(e,f)$$    

新的因子乘积：  

$$P(a)P(b)P(c\mid b)P(d\mid a)P(e\mid c,d)m_{g}(e)m_{f}(a,e)$$  

**Graph transformation:**删除节点$$F$$并连接$$A$$和$$E$$   

![_config.yml]({{ site.baseurl }}/images/10708/image62.jpg)   

Step 4: eliminate $$E$$   

$$m_{e}(a,c,d)=\sum_{e}P(e\mid c,d)m_{g}(e)m_{f}(a,e)$$    

新的因子乘积：  

$$P(a)P(b)P(c\mid b)P(d\mid a)m_{e}(a,c,d)$$    

**Graph transformation:** 根据Gibbs分布属性，生成完全连通的子图(完全图：图的每两个顶点之间有边链接,连通图：图的每两个顶点之间有路径链接)。 如以下所示，节点$$E$$被删除，$$E$$的相邻点相互连接。  

![_config.yml]({{ site.baseurl }}/images/10708/image63.jpg)   

Step 5: eliminate $$D$$  

$$m_{d}(a,c)=\sum_{d}P(d\mid a)m_{e}(a,c,d)$$  

新的因子乘积： 

$$P(a)P(b)P(c\mid b)m_{d}(a,c)$$    

**Graph transformation:** 如下  

![_config.yml]({{ site.baseurl }}/images/10708/image64.jpg)   

Step 6: eliminate $$C$$  

$$m_{c}(a,b)=\sum_{c}P(c\mid b)m_{d}(a,c)$$  

新的因子乘积： 

$$P(a)P(b)m_{c}(a,b)$$    

**Graph transformation:**如下连接$$A,B$$

![_config.yml]({{ site.baseurl }}/images/10708/image65.jpg)   

## VE to Graph Elimination (GE): Formal Connection  

Step 7: eliminate B

$$m_{b}(a)=\sum_{b}P(b)m_{c}(a,b)$$  

新的因子乘积：   

$$P(a)P(b)m_{b}(a)$$    

**Graph transformation:**现在只剩下一个节点$$A$$    

在最后一步，我们只是标准化剩余乘积     

总而言之，我们可以看到对应的图形转换,如下所示。 在每一步中，我们从先前的图形中删除一个节点，并对删除的节点的相邻点相互连接。

![_config.yml]({{ site.baseurl }}/images/10708/image66.jpg)   

## Complexity Analysis in Graph Perspective  

如上例所示，直观地说，graph elimination过程与variable elimination 算法有着密切的联系。本文首先对graph elimination算法进行了总结，给出了图结构重构图(reconstituted graph)的定义，给出了GE中elimination clique与VE中生成的中间项( intermediate term)对应的定理。  

**Graph Elimination Algorithm**  

给定：无向/有向图$$G$$,消除顺序$$I$$   

初始化：如果$$G$$是有向的，首先对$$G$$正则处理(moralize)(《Probabilistic Graphical Models Principles and Techniques》4.16定义提到相关概念)   

过程: 对于每一个在$$I$$中的节点$$X_{i}$$,将$$X_{i}$$从图中移除，将与$$X_{i}$$相邻的点互相连接。 

Reconstituted Graph $$G_{I}^{'}(V,E')$$也称导出图(induced graph)   

注意：$$I$$是消除顺序，不同的$$I$$ Reconstituted Graph 是不同的  

定义：reconstituted graph $$G_{I}^{'}(V,E')$$的边$$E'$$是$$E$$的父集，包含$$E$$所有的边以及在执行 Graph Elimination Algorithm过程中产生的新边  

Reconstituted graph 记录在执行 graph elimination algorithm 过程中产生的elimination cliques，在每一步从图中移除节点$$X_{i}$$，将$$X_{i}$$相邻节点相互连接形成的团(clique)，称为 elimination cliques   

VE中中间项与GE中 elimination cliques的对应关系:  

按照VE和GE的相应步骤，很容易看出在每一个消除步骤，VE中产生的中间项的辖域就是GE中产生的elimination cliques 。下图根据前面介绍的示例显示了这种关系：

![_config.yml]({{ site.baseurl }}/images/10708/image67.jpg)   

导出图(加上执行 Graph Elimination Algorithm过程中产生的新边):   

![_config.yml]({{ site.baseurl }}/images/10708/image75.png)    

**定理(Theorem):**  

1、在变量消除过程中产生的每个因子的辖域是重构图(reconstituted graph)$$G_{I}^{'}(V,E')$$中的一个团    

2、重构图中的每个最大团是计算中某些中间因子的辖域    

定理的证明可以在Koller的PGM教科书的第9章中找到。这个定理告诉我们中间因子的辖域是一个消去团，是重构图中的一个团。此外，最大中间因子的范围是重构图中最大的最大团。


## Complexity Analysis in Graph Perspective  

在本节的开头，我们认为VE复杂性的瓶颈取决于VE过程中生成的最大中间因子的辖域大小。 在上面的小节中，我们显示了VE过程中的每个中间因子都是graph elimination 算法中的elimination clique，最大elimination elimination 也是重构图中的最大clique。  

然后在给定消除顺序$$I$$的情况下，获得中间因子的最大辖域$$N_{max}$$的问题等同于在重构图$$G_{I}^{'}(V,E')$$中找到最大clique，这是一个纯粹的图论问题,并且有有效的成熟算法可以解决这个问题。

## Elimination Ordering  

我们可以将重构图(reconstituted graph)的宽度定义为最大clique的大小减去1.定义$$W_{G,I}$$为$$G_{I}^{'}(V,E')$$的宽。对于不同的顺序$$I$$,$$W_{G,I}$$是不同的。  

将$$G$$的**tree-width**定义为：  

$$W_{G}^{*} =\mathop{\min}_{I}W_{G,I}$$   

这个术语为我们提供了最佳性能的界限，我们希望应用VE对根据$$G$$因子分解为的概率进行推断   

但是，找到图的最佳消除顺序是一个NP难题。 如前所述，推理任务本身也是NP难题。 但是这两个NP难题并不相同。更具体地说，即使我们找到了最佳的消除顺序，如果图$$G$$的tree-width很大，推理的复杂度仍可能是指数级的。   

尽管设计一个通用的查找最佳消除顺序的算法是NP难题，但是有些启发式算法可以生成接近最佳的消除顺序（有关详细信息，请参阅Koller的PGM）。 另一方面，对于某些特定的图G，可以轻松获得“明显”的最佳排序。 现在我们举一些例子来说明相反的情况。  

**Example 1: Star graph**   

如果先去掉质心，很容易看出导出图的宽度等于$$N-1$$，其中$$N$$是节点总数。但是，如果我们最后删除质心，则很容易看到导出图是原始星图，宽度仅为1。使用这种消除顺序可以使variable elimination 非常有效。  

**Example 2: Tree graph**  

很明显，从叶到根删除节点不会引入任何 induced dependency，因此induced graph只是原始树。我们知道树上没有一个比3大的clique。所以宽度只有1。  

**Example 3: Ising model**  

很难找到一个最优的消去顺序。实际上，ising模型的tree width比$$\sqrt{n}$$大，所以即使找到一个最优的消去顺序，VE算法复杂度仍然是为$$n$$的指数。   

# Message Passing Algorithms    

**OVERVIEW**   

现在我们已经设计了一种通用的Eliminate algorithm，该算法可以在每个图上使用。但是它有几个缺点，其中之一就是最坏情况下的指数复杂度，另一个是它被设计为仅回答single-node查询。在本节中，我们基于相同的想法利用图的局部结构控制因子，并基于在Clique树数据结构上传递消息来制定一类精确的推理算法。 这样做将使我们对推理的工作方式有重要的了解，并且当必须基于相同的evidence计算多个查询时提供了计算上的好处。 接下来我们将说明对于特殊的tree-like graphs情况， 可以更有效地实现message-passing。最后我们以精确推断(exact inference)的总结结束。

本节将简要概述前面提到的技术，目的是直觉上展示它们如何相互关联的，并理清一些令人困惑的术语。为了更深入地解释和证明每一个主题，建议查阅参考文献。

**VARIABLE ELIMINATION AND CLIQUE TREES(变量消除和团树)**  

让我们首先画出我们在消除算法中看到的变量消除过程与称为Clique树（也称为Junction或Join树的名称）的特殊数据结构之间的联系。

回想执行变量消除的一个步骤涉，通过将几个现有因子相乘来创建因子$$\psi_{i}$$，然后在$$\psi_{i}$$中消除一个变量来创建因子$$\tau_{i}$$,该因子作为“消息”发送给其他因子。该算法定义了一个团树(Clique tree)：它是一个无向图，以因子$$\psi_{i}$$或变量团$$C_{i}$$作为节点。如果$$C_{i}$$的信息$$\tau_{i}$$用于计算$$C_{j}$$的信息$$\tau_{j}$$,$$C_{i}$$与$$C_{j}$$连接一条边。注意任何消息$$\tau_{i}$$只传递一次：当一个因子$$\phi_{i}$$用于创建新的因子$$\psi_{j}$$,则不会再被使用，因此，该图可以看作是一棵树。 下图显示了一个示例。   

![_config.yml]({{ site.baseurl }}/images/10708/image68.png)     

![_config.yml]({{ site.baseurl }}/images/10708/image76.png)     

![_config.yml]({{ site.baseurl }}/images/10708/image69.png)   

一种更符合算法原理的方法，在给定消元顺序的情况下，将$$G$$三角化为其弦图(chordal graph)，提取其中的最大团(cliques)，并在生成的团(cliques)图中找到最小生成树。  

通过对长度大于4的圈(cycle),不相邻顶点之间添加边，对图进行三角化(triangulates)。给定消除顺序，我们可以添加要进行消除过程中产生的边。下一步从图中提取最大团$$C_{i}$$，两个最大团通过$$S_{ij}=C_{i}\bigcap C_{j}$$连接。   

最后，该图中的最小生成树是团树。下例显示了初始图，三角剖分图(triangulated graph)，最大团和相应的团树。   

![_config.yml]({{ site.baseurl }}/images/10708/image70.png)   

此外，这些树的一个简单特征就是以$$C_{i}\subseteq X$$为节点，以$$S_{ij}=C_{i}\bigcap C_{j}$$为边，它们是由某些变量消除程序（使用称为“ Running Intersections”的性质）定义的团树。这使我们可以通过执行变量消除识别出团树。 就像我们将看到的那样，用团树来解释变量消除有几个计算上的优点。例如一棵树可用作执行几种不同消除顺序的基础。 此外，它可以缓存中间消息，以便更有效地回答多个边际概率查询。

**GENERAL SUM-PRODUCT ON A CLIQUE TREE**  

Sum-Product算法提供了一种使用Clique树指导变量消除的方法。 从带有团$$C_{}i$$的团树$$T$$开始，我们执行以下步骤:

1、将分配给每个团$$C_{i}$$的因子相乘，产生初始势势能(initial potentials)        

$$\psi_{i}(C_{i})=\prod \phi$$  

例子:   

$$\psi_{5}(J,L,G,S)=\prod \phi_{L}(L,G)\phi_{J}(J,L,S)$$    

![_config.yml]({{ site.baseurl }}/images/10708/image77.jpg)   

2、选择含感兴趣变量$$C_{r}$$作为根团。  

例子:(a) 是以$$C_{5}$$为根团。(b) 是以$$C_{3}$$为根团    

![_config.yml]({{ site.baseurl }}/images/10708/image78.jpg)   

3、将图形向上指向根。这定义了树上操作的部分顺序。  

4、自下而上地传递消息（收集证据阶段collect evidence phase）：按照从叶到根的拓扑顺序进行计算和存储

$$\delta_{i \to j}=\sum_{C_{i}-S_{i,j}}\psi_{i} \cdot \prod_{k \in Pred(i)}\delta_{k \to i}$$

5、自上而下分发消息（分发证据阶段distribute evidence phase）：针对每个团$$C_{i}$$:

$$\beta_{i} = \psi_{r} \cdot \prod_{k \in Pred(i)}\delta_{k \to r}$$

在第4步之后，我们可以得到根节点的边际,$$P(C_{r})$$,通过将传入的信息与$$C_{r}$$的势函数相乘：  

$$P(C_{r})= \psi_{r} \cdot \prod_{k \in Pred(r)}\delta_{k \to r}$$   

(要获得感兴趣的变量的概率(likelihood)，必须对不相关的变量进行求和)然而，在的自上而下阶段(步骤5)之后，显示了使用团树的好处，此后，我们获得的 beliefs$$\beta_{i}$$实际上等于每个团的边际概率$$P(C_{i})$$,这样，执行$$N$$个边际查询的成本从$$N_{c}$$减少到$$2c$$（在自下而上和自上而下的传递之间存储树和消息的成本）。  

该算法有几种修改。 一种方法是在收集证据阶段将max-product替换为sum-product，而在分发证据阶段则用traceback进行追溯，生成$$g$$的MAP(最大后验概率)估计。另一种方法给出了从模型中进行后验抽样的方法,收集证据阶段照常进行,分发证据阶段(sampling variables given the values higher up the tree.?)


由此得到的团(Junction)树算法是一种通用的精确推理算法。但是，它继承了消除算法的最坏情况的复杂度，在最大的团中是指数级的。在消除顺序上最小的团称为图的treewidth，它捕获了VE和CTA的复杂性。但是，找到顺序(ordering)以及treewidth通常很难。 这限制了这两种算法的适用性。  

接下来，我们将介绍一种更专业的实例化message-passing算法，该算法仅限于树或类树结构，但效率更高。 而且，它可以以迭代的方式应用于非树，从而产生近似的推理算法称为Loopy Belief propagation  

**SUM-PRODUCT ALGORITHM ON TREES**  

有一类特殊的模型可以特别有效地进行精确的推理。如果是无向树或有向图(其moralization是无向树)，我们可以像前面描述的那样使用Sum-Product message，使用图本身代替团树。MAP推断和后验抽样都可以使用max-product variation。而且，通过构造所谓的因子图(Factor graphs)，该算法可以扩展到某些树状结构。此算法也称为Backward algorithm,当应用于链表(chain graps)时称为Forward-Backward algorithm.大量的名称反映了该算法的实际意义：虽然它仅限于某些图形模型结构，但由于这些模型的 treewidth 较低,并且不需要构造一个团树来获得所有singleton marginals(边际概率？),因此它是有效的。

tree Sum-Product algorithm一个更有趣的特性是我们仍然可以通过反复运行 message passing直到收敛来将其应用于不是树的图（即具有环loops）：在这种情况下，它产生一种近似的推理方法。该算法被称为Loopy Belief Propagation，实验表明，该算法对不同类型的模型都有很好的效果。

**SUMMARY OF EXACT INFERENCE**   

让我们回顾一下有关精确推论的知识。 我们已经看到Eliminate, Clique tree, and Sum-Product algorithms。   
Eliminate algorithm 在概念上很简单，并且适用于任何图模型。 但是，它只允许我们计算单个查询，并且在treewidth方面具有最坏情况的指数时间复杂度。      
Clique tree algorithm适用于一般图形，并且能够通过使用消息缓存计算来解决Eliminate’s issues的第一个问题，但其计算复杂度与graph properties相同。     
Sum-product algorithm可以被认为实现了在图周围 passing messages的相同思想，因此可以用于多个查询应用程序，但是降低了 Clique tree algorithm的计算复杂性，代价是仅限于树状图形模型。

一般来说，在通用性和计算复杂性之间的权衡是不可避免的：可以证明精确的推理是 NP-hard。一般来说，精确推理的这种难处理性导致了对近似推理算法的需求，我们将在本课程的后面部分进行研究。然而，理解精确推理是值得的。首先，如我们所见，在一些关于模型结构的假设下，精确推理是可行的，例如当图形模型是树或具有低treewidth时。这个例子在应用程序中非常重要，因为许多有趣的模型都有类似树的结构。此外，一些近似推理算法，如Loopy-BP算法，受到精确推理算法的启发。  


参考：
[Lecture 4: Exact Inference](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-04/)



