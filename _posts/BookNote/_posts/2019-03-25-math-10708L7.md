---
layout: post
title: 译Lecture 7 Maximum likelihood learning of undirected GM
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

学习UGM的算法以及CRF的简要概述。

# Introduction and IPF(iterative proportional fitting)   

**MLE适用于无向图形模型**  

我们有一个无向图模型，并且我们知道使用Hammersley-Clifford定理可以用吉布斯分布表示它。现在的问题是,是否可以按照针对有向图模型的合理程序来找到UGM的最大似然期望(MLE)？ 答案是否定的-我们不能。 这是因为对于有向图模型，对数似然分解为项的总和，即每个项（节点，其父节点）。 但是，这不适用于无向图模型，因为存在归一化常数$$Z$$，该归一化常数是所有参数的函数，因此概率分布不会拆分为项的和。

$$
\begin{aligned}
P \left( x _ { 1 } , \ldots , x _ { n } \right) = \frac { 1 } { Z } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right) \\
Z  = \sum _ { x _ { 1 } , \ldots , x _ { n } } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right)
\end{aligned}
$$

尽管这是一个很大的缺点，但是我们仍然使用无向图模型，因为它们很有用-它们用于表示某些特殊情况，这些特殊情况不能通过有向图模型来表示。 下一节概述了通过对数似然的导数查找MLE的过程以及由此产生的困难。

**关于UGMs的对数似然** 

在这里，我们介绍两个新的数量-总计数(Total Count)和集团数(Clique Count)。无向图形模型$$(V,E)$$的总计数就是在数据集中观测到结构$$\mathbf { x }$$的次数,即($$X_{v}=\mathbf { x }$$)

$$m ( \mathbf { x } ) = \sum _ { n } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right)$$   

$$m ( \mathbf { x } )$$表示结构$$\mathbf { x }$$在数据中观测到的次数，当$$\mathbf { x } _ { n }=\mathbf { x }$$时，$$\delta \left( \mathbf { x } , \mathbf { x } _ { n } \right)$$为1，其他时候为0

集团数用总计数的边际数表示：  

$$m \left( \mathbf { x } _ { c } \right) = \sum _ {\mathbf { x }-\mathbf { x } _ {c } } m ( \mathbf { x } )$$  

$$\mathbf { x } _ { c }$$是$$\mathbf { x }$$的子集，计算$$\mathbf { x } _ { c }$$在数据中观测到的次数  

示例：

$$m(x_{1},x_{2})=\sum_{x_{3}}m(x_{1},x_{2},x_{3})$$   

用计数来表示对数似然：   

$$
p ( D ; \theta ) = \prod _ { n } \sum _ { \mathbf { x } } p ( \mathbf { x } ; \theta ) ^ { \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) } \\ 
\log p ( D ; \theta )  = \sum _ { n } \sum _ { \mathbf { x } } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) \log p ( \mathbf { x } ; \theta ) = \sum _ { \mathbf { x } } \sum _ { n } \delta \left( \mathbf { x } , \mathbf { x } _ { n } \right) \log p ( \mathbf { x } ; \theta ) \\
\ell  = \sum _ { \mathbf { x } } m ( \mathbf { x } ) \log p ( \mathbf { x } ; \theta )\\
= \sum _ { \mathbf { x } } m ( \mathbf { x } ) \log \left( \frac { 1 } { Z } \prod _ { c } \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) \\ 
=\sum _ { \mathbf { x } _ { c } } \sum _ { \mathbf { x }- \mathbf { x } _ { c } } m ( \mathbf { x } ) \log \left( \frac { 1 } { Z } \prod _ { c } \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) \\
= \sum _ { c } \sum _ { \mathbf { x } _ { c } } m \left( \mathbf { x } _ { c } \right) \log \psi _ { c } \left( \mathbf { x } _ { c } \right) - N \log Z
$$

求一阶导数:  



**迭代比例恰当(Iterative Proportional Fitting)**  

**IPF的信息理论观点** 

# 广义迭代标度(Generalized Iterative Scaling，GIS):介绍

# 广义迭代标度: 算法  

# IPF和GIS概述  

# 指数族

**PITMAN-KOOPMAN-DARMOIS定理** 

**指数族为什么如此频繁出现？** 

# 条件随机场(Conditional Random Fields,CRFs)  

**介绍**  

**推断与学习**

# 其他资源 

Jordan textbook, Ch. 11   
Koller textbook, Ch. 19.1-19.4  
Borman, The EM algorithm (A short tutorial)   
Variations on EM algorithm by Neal and Hinton   

参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



