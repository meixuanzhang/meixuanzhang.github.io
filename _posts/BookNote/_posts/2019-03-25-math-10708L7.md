---
layout: post
title: 译Lecture 7 Maximum likelihood learning of undirected GM
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

学习UGM的算法以及CRF的简要概述。

# Introduction and IPF(iterative proportional fitting)   

**MLE适用于无向图形模型**  

我们有一个无向图模型，并且我们知道使用Hammersley-Clifford定理可以用吉布斯分布表示它。现在的问题是,是否可以按照针对有向图模型的合理程序来找到UGM的最大似然期望(MLE)？ 答案是否定的-我们不能。 这是因为对于有向图模型，对数似然分解为项的总和，即每个项（节点，其父节点）。 但是，这不适用于无向图模型，因为存在归一化常数$$Z$$，该归一化常数是所有参数的函数，因此概率分布不会拆分为项的和。

$$
\begin{aligned}
P \left( x _ { 1 } , \ldots , x _ { n } \right) = \frac { 1 } { Z } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right) \\
Z  = \sum _ { x _ { 1 } , \ldots , x _ { n } } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right)
\end{aligned}
$$

尽管这是一个很大的缺点，但是我们仍然使用无向图模型，因为它们很有用-它们用于表示某些特殊情况，这些特殊情况不能通过有向图模型来表示。 下一节概述了通过对数似然的导数查找MLE的过程以及由此产生的困难。

**关于UGMs的对数似然** 

在这里，我们介绍两个新的数量-总计数(Total Count)和集团数(Clique Count)。无向图形模型$$(V,E)$$的总计数就是在数据集中观测到结构$$\mathbf { x }_{v}$$的次数,即($$X_{v}=\mathbf { x }_{v}$$)

$$m ( \mathbf { x }_ {v} ) = \sum _ { n } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n } \right)$$   

$$m ( \mathbf { x }_ {v} )$$表示结构$$\mathbf { x }_ {v}$$在数据中观测到的次数，当$$\mathbf { x } _ {v,n}=\mathbf { x }_ {v}$$时，$$\delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right)$$为1，其他时候为0

集团数用总计数的边际数表示：  

$$m \left( \mathbf { x } _ { c } \right) = \sum _ {\mathbf { x }_ {v}-\mathbf { x } _ {c } } m ( \mathbf { x } )$$  

$$\mathbf { x } _ { c }$$是$$\mathbf { x }_ {v}$$的子集，计算$$\mathbf { x } _ { c }$$在数据中观测到的次数  

示例：

$$m(x_{1},x_{2})=\sum_{x_{3}}m(x_{1},x_{2},x_{3})$$   

用计数来表示对数似然：   

$$
p ( D ; \theta ) = \prod _ { n } \sum _ { \mathbf { x } } p ( \mathbf { x } ; \theta ) ^ { \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n } \right) } \\ 
\log p ( D ; \theta )  = \sum _ { n } \sum _ { \mathbf { x } } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right) \log p ( \mathbf { x } ; \theta ) = \sum _ { \mathbf { x } } \sum _ { n } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right) \log p ( \mathbf { x } ; \theta ) \\
\ell  = \sum _ { \mathbf { x } } m ( \mathbf { x } ) \log p ( \mathbf { x } ; \theta )\\
=\sum _ { \mathbf { x } } m ( \mathbf { x } ) \log \left( \frac { 1 } { Z } \prod _ { c } \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) \\ 
=\sum _ { \mathbf { x } } m ( \mathbf { x } )\sum _ { c } \log \left( \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) - \sum _ { \mathbf { x } } m ( \mathbf { x } ) \log Z \\
=\sum _ { c } \sum _ { \mathbf { x } _ { c } } (\sum _ { \mathbf { x }_{v}- \mathbf { x } _ { c } } m ( \mathbf { x } )) \log \left( \psi _ { c } \left( \mathbf { x } _ { c } \right) \right)-N \log Z \\
= \sum _ { c } \sum _ { \mathbf { x } _ { c } } m \left( \mathbf { x } _ { c } \right) \log \psi _ { c } \left( \mathbf { x } _ { c } \right) - N \log Z
$$

求一阶导数,第一项:  

$$\frac { \partial l _ { 1 } } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = m \left( \mathbf { x } _ { c } \right) / \psi _ { c } \left( \mathbf { x } _ { c } \right)$$

第二项会更复杂一些：  

$$
\frac { \partial \log Z } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) }
= \frac { 1 } { Z } \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \sum _ { \tilde { \mathbf { x } } } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
= \frac { 1 } { Z } \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
= \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { 1 } { \psi _ { c } \left(  \mathbf { x } _ { c } \right) } \frac { 1 } { Z } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \\
= \frac { 1 } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) p ( \widetilde { \mathbf { x } } ) = \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }

$$

把第一项和第二项放在一起，导数变成:  

$$
\frac { \partial \ell } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = \frac { m \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } - N \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
$$

等于零，然后解出:  

$$
p _ { M L E } ^ { * } \left( \mathbf { x } _ { c } \right) = \frac { m \left( \mathbf { x } _ { c } \right) ^ {  } } { N } = \tilde { p } \left( \mathbf { x } _ { c } \right)
$$

注意$$\psi _ { c } \left( \mathbf { x } _ { c } \right)$$项被取消了，我们只剩下一个条件。换句话说，在参数的最大似然设置下，对于每个集团，模型边缘必须等于观察到的边缘（经验计数）。这并没有告诉我们如何得到ML参数，它只是给了我们一个条件，当我们有了这些参数时，必须满足这个条件。

因此，我们转向两个重要的算法，称之为workhorse algorithms。

**迭代比例恰当(Iterative Proportional Fitting)**  

**IPF的信息理论观点** 

# 广义迭代标度(Generalized Iterative Scaling，GIS):介绍

# 广义迭代标度: 算法  

# IPF和GIS概述  

# 指数族

**PITMAN-KOOPMAN-DARMOIS定理** 

**指数族为什么如此频繁出现？** 

# 条件随机场(Conditional Random Fields,CRFs)  

**介绍**  

**推断与学习**

# 其他资源 

Jordan textbook, Ch. 11   
Koller textbook, Ch. 19.1-19.4  
Borman, The EM algorithm (A short tutorial)   
Variations on EM algorithm by Neal and Hinton   

参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



