---
layout: post
title: 译Lecture 7 Maximum likelihood learning of undirected GM
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  

学习UGM的算法以及CRF的简要概述。

# Introduction and IPF(iterative proportional fitting)   

**MLE适用于无向图形模型**  

我们有一个无向图模型，并且我们知道使用Hammersley-Clifford定理可以用吉布斯分布表示它。现在的问题是,是否可以按照针对有向图模型的合理程序来找到UGM的最大似然期望(MLE)？ 答案是否定的-我们不能。 这是因为对于有向图模型，对数似然分解为项的总和，即每个项（节点，其父节点）。 但是，这不适用于无向图模型，因为存在归一化常数$$Z$$，该归一化常数是所有参数的函数，因此概率分布不会拆分为项的和。

$$
\begin{aligned}
P \left( x _ { 1 } , \ldots , x _ { n } \right) = \frac { 1 } { Z } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right) \\
Z  = \sum _ { x _ { 1 } , \ldots , x _ { n } } \prod _ { c \in C } \psi _ { c } \left( \mathbf { x } _ { c } \right)
\end{aligned}
$$

尽管这是一个很大的缺点，但是我们仍然使用无向图模型，因为它们很有用-它们用于表示某些特殊情况，这些特殊情况不能通过有向图模型来表示。 下一节概述了通过对数似然的导数查找MLE的过程以及由此产生的困难。

**关于UGMs的对数似然** 

在这里，我们介绍两个新的数量-总计数(Total Count)和集团数(Clique Count)。无向图形模型$$(V,E)$$的总计数就是在数据集中观测到结构$$\mathbf { x }_{v}$$的次数,即($$X_{v}=\mathbf { x }_{v}$$)

$$m ( \mathbf { x }_ {v} ) = \sum _ { n } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n } \right)$$   

$$m ( \mathbf { x }_ {v} )$$表示结构$$\mathbf { x }_ {v}$$在数据中观测到的次数，当$$\mathbf { x } _ {v,n}=\mathbf { x }_ {v}$$时，$$\delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right)$$为1，其他时候为0

团数用总计数的边际数表示：  

$$m \left( \mathbf { x } _ { c } \right) = \sum _ {\mathbf { x }_ {v}-\mathbf { x } _ {c } } m ( \mathbf { x }_{v} )$$  

$$\mathbf { x } _ { c }$$是$$\mathbf { x }_ {v}$$的子集，计算$$\mathbf { x } _ { c }$$在数据中观测到的次数  

示例：假设$$V=\{1,2,3\}$$,$$\mathbf { x }_ {v}$$表示为3维的表，假设$$X_{2}$$是$$X_{1}$$父节点，计算边际计数$$m(x_{1},x_{2})$$

$$m(x_{1},x_{2})=\sum_{x_{3}}m(x_{1},x_{2},x_{3})$$   

用计数来表示对数似然：   

$$
p ( D ; \theta ) = \prod _ { n } p ( \mathbf { x }_{v,n} ; \theta ) =\prod _ { n } \sum _ { \mathbf { x }_{v} } p ( \mathbf { x }_{v} ; \theta ) ^ { \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n } \right) } \\ 
\log p ( D ; \theta )  = \sum _ { n } \sum _ { \mathbf { x }_{v} } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right) \log p ( \mathbf { x }_{v} ; \theta ) = \sum _ { \mathbf { x }_{v} } \sum _ { n } \delta \left( \mathbf { x }_ {v} , \mathbf { x } _ { v,n }  \right) \log p ( \mathbf { x }_{v} ; \theta ) \\
\ell  = \sum _ { \mathbf { x }_{v} } m ( \mathbf { x }_{v} ) \log p ( \mathbf { x }_{v} ; \theta )\\
=\sum _ { \mathbf { x }_{v} } m ( \mathbf { x }_{v} ) \log \left( \frac { 1 } { Z } \prod _ { c } \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) \\ 
=\sum _ { \mathbf { x }_{v} } m ( \mathbf { x }_{v} )\sum _ { c } \log \left( \psi _ { c } \left( \mathbf { x } _ { c } \right) \right) - \sum _ { \mathbf { x }_{v} } m ( \mathbf { x }_{v} ) \log Z \\
=\sum _ { c } \sum _ { \mathbf { x } _ { c } } (\sum _ { \mathbf { x }_{v}- \mathbf { x } _ { c } } m ( \mathbf { x }_{v} )) \log \left( \psi _ { c } \left( \mathbf { x } _ { c } \right) \right)-N \log Z \\
= \sum _ { c } \sum _ { \mathbf { x } _ { c } } m \left( \mathbf { x } _ { c } \right) \log \psi _ { c } \left( \mathbf { x } _ { c } \right) - N \log Z
$$

求一阶导数,第一项:  

$$\frac { \partial l _ { 1 } } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = m \left( \mathbf { x } _ { c } \right) / \psi _ { c } \left( \mathbf { x } _ { c } \right)$$

第二项会更复杂一些：  

$$
\frac { \partial \log Z } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) }
= \frac { 1 } { Z } \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \sum _ { \tilde { \mathbf { x } } } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
= \frac { 1 } { Z } \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { \partial } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } \left( \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \right) \\
= \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) \frac { 1 } { \psi _ { c } \left(  \mathbf { x } _ { c } \right) } \frac { 1 } { Z } \prod _ { d } \psi _ { d } \left( \widetilde { \mathbf { x } } _ { d } \right) \\
= \frac { 1 } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } \sum _ { \tilde { \mathbf { x } } } \delta \left( \widetilde { \mathbf { x } } _ { c } , \mathbf { x } _ { c } \right) p ( \widetilde { \mathbf { x } } ) = \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }

$$

把第一项和第二项放在一起，导数变成:  

$$
\frac { \partial \ell } { \partial \psi _ { c } \left( \mathbf { x } _ { c } \right) } = \frac { m \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } - N \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
$$

等于零，然后解出:  

$$
p _ { M L E } ^ { * } \left( \mathbf { x } _ { c } \right) = \frac { m \left( \mathbf { x } _ { c } \right) ^ {  } } { N } = \tilde { p } \left( \mathbf { x } _ { c } \right)
$$

注意$$\psi _ { c } \left( \mathbf { x } _ { c } \right)$$项被消除了，我们只剩下一个条件。换句话说，参数在最大似然设置下，对于每个团，模型边际概率必须等于经验边际概率。这并没有告诉我们如何得到$$ML$$参数，它只是给了我们一个条件，这些参数必须满足这个条件。

因此，我们转向两个重要的算法，称之为workhorse algorithms。

**迭代比例恰当(Iterative Proportional Fitting)**  

根据似然导数获得的关系：  

$$
\frac { \tilde { p } \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) } = \frac { p \left( \mathbf { x } _ { c } \right) } { \psi _ { c } \left( \mathbf { x } _ { c } \right) }
$$

既然$$$$出现在方程的两边，用封闭形式(closed form)求解它是不可能的。但是，我们可以把这看作是一个不动点迭代，把左边的一个看作是前面的(未知)，右边的那个作为已知的，然后我们可以用这个方程来估计左边的和右边的。对所有的团中循环迭代：  

$$
\psi _ { c } ^ { ( t + 1 ) } \left( \mathbf { x } _ { c } \right) = \psi _ { c } ^ { ( t ) } \left( \mathbf { x } _ { c } \right) \frac { \tilde { p } \left( \mathbf { x } _ { c } \right) } { p ^ { ( t ) } \left( \mathbf { x } _ { c } \right) }
$$

根据上一轮参数获得$$p ^ { ( t ) } \left( \mathbf { x } _ { c } \right), \psi _ { c } ^ { ( t ) } \left( \mathbf { x } _ { c } \right)$$,注意这里$$p ^ { ( t ) } \left( \mathbf { x } _ { c } \right)$$是根据参数计算的概率

IPF也是一种坐标上升算法，其中坐标是团势的参数。在算法的每一步中，对数似然都会增加，从而收敛到全局最大值。

**IPF的信息理论观点**  

最大化对数似然相当于最小化从观测分布到模型分布的KL散度。 

$$\max \ell \Leftrightarrow \min K L ( \tilde { p } ( x ) \| p ( x ; \theta ) ) = \sum _ { x } \tilde { p } ( x ) \log \frac { \tilde { p } ( x ) } { p ( x ; \theta ) }$$

# 广义迭代标度(Generalized Iterative Scaling，GIS):介绍   

在第二个算法中，我们开始更多地考虑基于特征的模型(feature-based models)。 到目前为止，当我们提到势函数(potential functions)，我们认为它是微不足道的。 你可以学习它，也可以随意指定数字。 但是有一些问题。 为了说明这一点，让我们看一下拼写检查的实际任务。 拼写检查中最有用和最强大的方法之一就是建立字符串的相似性模型(affinity models)。

**基于特征的团势(FEATURE-BASED CLIQUE POTENTIALS)**

例如，连续出现的3个字符$$c_{1},c_{2},c_{3}$$，我们用势函数$$\psi(x_c)$$表示。其背后的理由是，我们可以使用它来对拼写错误的可能性进行评分。 例如，如果我们看到序列“ zyz”，我们就会知道这种序列在英语词汇中不存在，因此很可能会拼错。另一方面，如果我们看到序列‘ink’，我们就知道有些单词包含这个序列，因此拼写错误的可能性较低。这是势函数(potential)如何帮助我们进行拼写检查的一个示例。但是，如果我们要遍历字母表中三元组的所有不同组合，则意味着我们具有$$26^3$$个特征，如果增加序列的长度，情况将变得更糟。 这种一般的“表格式(tabular)”势函数的推理，其成本将成倍增加，并且必须从有限的数据中学习的指数级的参数，这将耗尽计算机的内存。

一个解决方案是改变图模型，使团(cliques)变小。但这改变了依赖关系，并可能迫使我们做出比我们希望的更多的独立性假设。另一个解决方案是保持相同的图模型，但使用不太通用的团势(clique potentials)参数。这就是基于特征的模型背后的思想。

特征：在我们前面的拼写检查示例中，根据现有的知识(例如字典或我们自己的词汇表)，我们知道某些三连音(triplets)出现频率，所以我们可以挑选出我们最有信心的单词，并为其分配特征。在这里，特征被定义为一个函数，对于大多数连接设置都是空的，除了一些特定的设置，它返回高值或低值。例如，我们可以定义一个特征$$f_{ing}=10,f_{ate}=9$$，以此类推。当我们得到第50个或第100个特征时，我们假设这些组合的可能性较小，并给它们指定一些任意的小概率$$\alpha$$。当输入连续时，我们也可以定义特征。这样，拥有一个特征激活的单元的想法就不再重要了，但是我们仍然可以对该特征进行紧凑的参数化。这样，使单元处于活动状态的想法就不再重要了，但我们仍可以对该特征进行紧凑的参数化。

这是一种与深度学习不同的方法，在深度学习中，我们变得过于复杂，并希望大数据能够汇集把这些数字分配好，这没有任何保证，因为我们不知道需要多少数据来确定这些数字的分配，也就是说，我们不知道样本的复杂性。相反，这里有一种利用人类知识建立模型的方法。即使在今天，最成功的语音识别模型也有基于这个概念的基础。

特征作为微势(Micropotentials)的功能：让我们继续进行详细介绍。 通过对它们进行幂运算，可以将每个特征转换为“微势(micropotential)”。 我们可以将这些微势相乘以获得团势(clique potential )。 例如，团势$$\psi(c_1,c_2,c_2)$$可以表示为：

$$\psi_c (c_1,c_2,c_3) = e^{\theta_{ing} f_{ing}} \times e^{\theta_{?ed} f_{?ed}} \times \dots = \exp \left \{ \sum^K_{k=1} \theta_k f_k(c_1,c_2,c_3) \right \}$$

势仍然有$$26^3$$种可能的设置，但如果有$$K$$个特征，则仅使用$$K$$个参数，并且通过对$$x_{c}$$的每个组合使用一个指标函数，我们可以复原标准的势表格(tabular potential)。

组合特征：注意这些特征可以手动设置。 我们还将权重$$\theta_k$$附加到函数中，因为我们不知道每个特征有多重要。团的边际是广义指数族分布，实际上是GLIM(广义线性模型)：

$$\psi_c (c_1,c_2,c_3) \propto \exp \left \{ \theta_{ing}f_{ing}(c_1,c_2,c_3) + \theta_{qu?}f_{qu?}(c_1,c_2,c_3) +\theta_{zzz}f_{zzz}(c_1,c_2,c_3) + \dots \right \}$$

一般来说，特征可以是重叠的、不受约束的指标或团变量任何子集的任何函数。例如，在我们的拼写检查示例中，如果您有多个团，覆盖所有可能的三元组组合的重叠窗口，这并没有改变势函数的结构，我们仍然有一个加权的特征和。


$$
\psi_c(x_c) := \exp \left \{ \sum_{i\in I_c} \theta_k f_k(x_{c_i}) \right \} 
$$

$$I_c$$是所有可能的组合

基于特征的模型：我们可以像往常一样将这些团相乘：

$$
p(x) = \frac{1}{Z(\theta)} \prod_c \psi_c (x_c) = \frac{1}{Z(\theta)} \exp \left \{\sum_c \sum_{i \in I_c} \theta_k f_k(x_{c_i}) \right \}
$$

但是，通常我们可以忘记将特征与团相关联，而只需使用简化形式即可：

$$p(x) = \frac{1}{Z(\theta)}\exp \left \{\sum_{i} \theta_i f_i(x_{c_i}) \right \}$$

$$f_i(x_{c_i})$$只有下标相等时为1，其余为0

通过这种重新设计，我们得到了指数族模型，特征作为充分的统计。所以我们得到了紧凑的先验知识。然而，学习的难度更大。回想一下，在IPF中，我们有：

$$p(x) = \frac{1}{Z(\theta)}\exp \left \{\sum_{i} \theta_i f_i(x_{c_i}) \right \}$$



# 广义迭代标度: 算法  

# IPF和GIS概述  

# 指数族

**PITMAN-KOOPMAN-DARMOIS定理** 

**指数族为什么如此频繁出现？** 

# 条件随机场(Conditional Random Fields,CRFs)  

**介绍**  

**推断与学习**

# 其他资源 

Jordan textbook, Ch. 11   
Koller textbook, Ch. 19.1-19.4  
Borman, The EM algorithm (A short tutorial)   
Variations on EM algorithm by Neal and Hinton   

参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



