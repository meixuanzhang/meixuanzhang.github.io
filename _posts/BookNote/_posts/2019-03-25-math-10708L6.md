---
layout: post
title: 译Lecture 6 Learning Partially Observed GM and the EM Algorithm
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  


介绍了利用EM（Baum-Welch）算法从数据中估计图模型参数的过程。

# 介绍  

在之前的讲座中，我们介绍了从数据中学习图模型的概念，其中学习是估计参数的过程，在某些情况下，还包括从数据中估计网络结构的过程。第五讲在完全观测GMs的背景下介绍了这个概念。对于每个条件概率分布，在完全观测的节点和全局独立参数的设置中的最大似然估计非常简单，因为似然函数可以完全分解为独立项的乘积，每个项对应于网络中的条件概率分布。这意味着我们可以独立于网络的其余部分最大化每个局部似然函数，然后组合解决方案以获得MLE解决方案。

在本讲座中，我们将注意力转向部分观测的图模型，这是同等重要的一类模型，包括隐马尔可夫模型和高斯混合模型。 我们将看到，在部分观测的数据下，我们失去了似然函数的重要属性：其单峰性，闭式表示以及分解为不同参数的似然的乘积。因此，学习问题变得更加复杂，我们转向Expectation-Maximization算法来估计模型参数。

# 部分观测的图模型(Partially Observed GMs)    

## 有向但部分观测的图模型(Directed but partially observed GM)  

首先考虑完全观测的有向图模型的情况：  

![_config.yml]({{ site.baseurl }}/images/10708/image133.png)   

完全观测的有向图模型对数似然函数可分解为局部项的总和。我们可以独立地最大化每个局部似然函数。  

![_config.yml]({{ site.baseurl }}/images/10708/image136.png)  


将此与有向但部分观测的GM进行比较。 假设现在有一个变量没有观测到，但是我们仍然想写下数据的似然( likelihood)。 为此，为此，我们将不可观测概率边缘化(即积分或求和)。  

![_config.yml]({{ site.baseurl }}/images/10708/image134.png)  

部分观测的有向图模型。对于未观测(unobserved)或潜在变量(latent variables)，对数似然函数不再分解为局部项之和。所有参数通过边缘化耦合在一起。

![_config.yml]({{ site.baseurl }}/images/10708/image137.png) 

**为观测变量(UNOBSERVED VARIABLES)**   

+ 变量可以是未观测的(unobserved)或潜在的(latent），因为它是一个：  

1、抽象变量(Abstract)或虚拟变量(imaginary quantity )，用于简化数据生成过程例如语音识别模型、混合模型。

2、难以或不可能测量的真实对象，如恒星的温度、疾病的起因、进化的祖先。  

3、由于样本丢失而无法测量的真实对象，例如传感器故障。  

+ 离散潜在变量(Discrete latent variables)可用于将数据划分或分组  

+ 连续潜在变量(Continuous latent variables)(因子)可用于降维(如因子分析等)   

**示例：用于语音识别(SPEECH RECOGNITION)的HMMS**

我们的手机能够识别语音并将其转换为文本。解决这一问题的最初方法是基于隐马尔可夫模型(HMMs)。我们假设存在一种潜在的状态( latent state )，它会产生带噪声信号的语音，这些语音可以被“分块”成不同的成分或音素(phonemes)。我们为不同的语言创建不同音素的词典，然后我们试图推断出生成语音的音素序列。

![_config.yml]({{ site.baseurl }}/images/10708/image135.png) 

**示例：用于生物进化的贝叶斯网络**   

无法测量地球上不再存在的进化祖先的数据，但它们仍然是模型的一个有用的潜在变量。

![_config.yml]({{ site.baseurl }}/images/10708/image138.png)    

## 概率推断(Probabilistic Inference)  

GM的$$M$$描述了一个唯一的概率分布$$P$$。为了学习部分可观测的GMs，我们必须在由$$P$$描述的推断关系和从数据$$D$$估计一个可信的模型$$M$$之间迭代。这样推理就可以被视为学习问题的一个子例程(子程序)：  

任务一.我们如何回答关于$$P$$的问题,例如$$P(X\mid Y)$$  

+ 我们使用推理(inference)作为计算这些问题答案的过程的名称。  

任务二.我们如何从数据$$D$$估计一个可信的模型$$M$$   

+ 我们使用学习(learning)作为$$M$$的点估计的获得过程的名称。对于贝叶斯方法，我们寻求$$P(M\mid D)$$,这也是一个推理(inference)问题。  

GMs中有许多推理方法。它们可以分为两类： 

+ **Exact inference algorithms**.包括消元算法(elimination algorithm)、信息传递算法(message-passing algorithm)（和积、信念传播(sum-product, belief propagation)）、连接树算法(junction tree algorithms)。这些算法可以给出问题的精确结果。本课程的主要主题是精确推理算法。

+ **Approximate inference techniques**. 包括随机模拟/抽样方法(stochastic simulation / sampling methods)、马尔可夫链蒙特卡罗方法(Markov chain Monte Carlo (MCMC) methods)、变分算法(variational algorithms)。这些算法只给出了推理问题(inference query)的近似答案。我们将在以后的讲座中介绍这些方法。  

## 混合模型(Mixture Models)  

一个密度模型(density model)$$p(x)$$可以是多峰的(multi-modal),但是我们可以将其建模为单峰分布(uni-modal)的混合.(例如高斯分布).假设我们有数据集$$x_{n}\in R^D,n=1,..,N$$,我们认为数据是由$$K$$个高斯分布的混合生成的。让$$z\in 1,...K$$为未观测随机变量，$$P(z=k)=\pi_{k},k=1,...,K$$,为了$$P(z)$$是有效的概率分布,$$0\le \pi \ge 1$$并且$$\sum_{k=1}^K \pi_{k} = 1$$ ,我们称$$\pi_{k}$$为混合比例，我们假设对给定混合标签$$k$$的高斯分量表示为$$p(x\mid z=k)=N(x\mid \mu_{k},\Sigma_{k})$$,$$\mu_{k},\Sigma_{k}$$是每个高斯分量的参数  


$$p(x_{n})=\sum_{z}P(x_{n}\mid z)P(z)=\sum_{k=1}^K N(x_{n} ; \mu_{k},\Sigma_{k})\pi_{k}$$   

$$Z$$是潜在类别指标向量：  

$$p(z_{n})=multi(z_{n} ;\pi)=\prod_{k}(\pi_{k})^{z_{n}^k}$$   

Multinoulli distribution  

$$X$$是具有类特定均值/协方差的条件高斯变量:  

$$p(x_{n}\mid z_{n}^k =1)=p(x_{n} ; \mu_{k},\Sigma_{k})\\
=\frac{1}{(2\pi)^{m/2} \mid \Sigma_{k} \mid^{1/2}} exp(-\frac{1}{2}(x_{n}-\mu_{k})^T \Sigma_{k}^{-1} (x_{n}-\mu_{k}))$$

高斯混合模型是部分观测图模型的示例。 引入潜在变量$$Z$$作为类指标，以将多峰分布分成更简单的单峰分布。  

![_config.yml]({{ site.baseurl }}/images/10708/image139.png) 

The likelihood of a sample:   

$$p(x_{n}) = \sum_{k}p(z^k =1\ ; \pi)p(x_{n}\mid z^k =1)\\
=\sum_{z_{n}} \prod_{k}(\pi_{k})^{z_{n}^k} N(x ; \mu_{k},\Sigma_{k})^{z_{n}^k}\\
=\sum_{k=1}^K N(x_{n}; \mu_{k},\Sigma_{k})\pi_{k} $$

**全观测高斯混合模型的MLE解**  

当能观察到潜在变量$$Z$$时，数据对数似然可以分解： 

![_config.yml]({{ site.baseurl }}/images/10708/image140.png) 

因此，可以针对以下参数分别找到参数的MLE解决方案：

![_config.yml]({{ site.baseurl }}/images/10708/image141.png)  

我们通常不了解类标签$$z_{n}^k$$。如果是知道，计算模型参数将非常容易。 如果我们不知道$$z_{n}^k$$，则无法将数据对数似然分解或独立地找到参数（注意这里的参数都取决于$$z_{n}^k$$）  

## 部分观测高斯混合模型(Gaussian Mixture Model)的参数估计(Estimating the Parameters of a Partially-Observed GMM)  

# Expectation-Maximization (E-M) Algorithm  



## 完全和不完全对数似然(Complete and Incomplete Log Likelihood)   

**完全对数似然**   

完全对数似然是$$x$$和$$z$$都是可观测的似然。$$x$$和$$z$$可观测的情况下优化(Optimization)可以直接使用MLE，以及可以将联合概率分解成子项独立最大化。  

$$L(\theta;x,z):=logp(x,z;\theta)=logp(x\mid z;\theta_{x})+logp(z;\theta_{z})$$  

**不完全对数似然**   

$$z$$是不可观测的，我们可以通过对$$z$$求和，获得边际分布，但不能再解耦成独立的项

$$L(\theta;x):=logp(x,;\theta)=log\sum_{z}p(x,z;\theta)$$ 

## 完全对数似然期望(Expected Complete Log Likelihood)   

对于任意分布$$z \sim q$$,定义完全对数似然期望，这个函数是$$L$$的线性组合:  

$$<L(\theta;x,z)>_{q}:= \sum_{z}q(z\mid x;\theta)logp(x,z;\theta)=E_{q(z\mid x;\theta)}logp(x,z;\theta)$$   

完全对数似然期望可以用来建立不完全对数似然的下界,证明使用了Jensen不等式($$logE[x]\ge E[logx]$$)证明也使用了重要性抽样技巧Importance sampling($$E_{p}[f(x)]=E_{q}[\frac{f(x)p(x)}{q(x)}]$$)   

![_config.yml]({{ site.baseurl }}/images/10708/image142.png) 

第二项$$H_{q}$$是$$q(z\mid x)$$的熵，即$$H_{q}=\sum_{z}q(z\mid x)log \frac{1}{q(z\mid x)}$$。分布取值范围[0,1],所以分布取值的对数范围$$<-\infty,0>$$,所以常数$$H_{q}$$是正。因为$$H_{q}$$是正且不依赖$$\theta$$,我们可以尝试最大化$$<L(\theta;x,z)>_{q}$$来最大化下界$$L(\theta ,x)$$

## Lower Bounds and Free Energy   

固定数据$$x$$,定义一个函数称为free energy ,即$$<L(\theta;x,z)>_{q}+H_{q}$$  

$$F(q;\theta):=\sum_{z}q(z\mid x)log\frac{p(x,z; \theta)}{q(z\mid x)}\le L(\theta ;x)$$

我们可以在$$F$$上执行EM算法（通常是MM算法）：  

+ E-step:$$q^{t+1} = \mathop{\arg\max}_{q}F(q;\theta^t)$$  

+ M-step:$$\theta^{t+1}=\mathop{\arg\max}_{\theta}F(q^t;\theta)$$  

**E-STEP**

E-step是给定数据和参数情况下,通过隐变量(latent variables)后验概率$$q(z\mid x;\theta)$$最大化$$F$$:  

$$q^{t+1}=\mathop{\arg\max}_{q}F(q;\theta^t)$$  

如果$$q$$是最优后验概率$$p(z\mid x;\theta^t)$$,此最大化可达到界值$$L(\theta^t;x)$$.证明使用了Bayes规则$$p(x,z; \theta^t)=p(x;\theta^t)p(z\mid x;\theta^t)$$ 

![_config.yml]({{ site.baseurl }}/images/10708/image143.png) 

**M-STEP**  

M步是给定数据和隐变量(latent variables)情况下，通过参数最大化$$F$$。如前所述，free energy分为两项，其中一项$$H_{q}$$与$$\theta$$无关。因此，我们只需要在M步中考虑第一项。  

$$F(q;\theta)=<L(\theta;x,z)>_{q}+ H_{q}$$  

$$\theta^{t+1} = \mathop{\arg\max}_{\theta}F(q^t,\theta)=\mathop{\arg\max}_{\theta}<L(\theta;x,z)>_{q^t}$$  

如果$$q^t = p(z\mid x;\theta)$$,这相当于完全可观测的最大似然估计，其中统计数据被$$p(z\mid x;\theta)$$的期望所取代.我们正在最小化分布$$p$$上的期望损失   

## 示例：隐马尔可夫模型和Baum-Welch算法  

![_config.yml]({{ site.baseurl }}/images/10708/image144.png)   

隐马尔可夫模型(HMMs)是时间序列数据建模的常用方法之一。它被用于传统的语音识别系统、自然语言处理、计算生物学等领域。HMMs允许我们同时处理观察到(observed)的和隐藏的(hidden)事件。大多数常见的HMM遵循一阶马尔可夫假设( markov assumption)，即在预测未来时，过去并不重要，只关心现在。如果存在一系列状态$$q_{1},q_{2},..,q_{i}$$,那么一阶马尔可夫假设就是这样:  

$$P(q_{i}=a\mid q_{1},q_{2},..,q_{i-1})=P(q_{i}=a\mid q_{i-1})$$   

**隐马尔可夫模型定义**  

HMM由以下组成部分指定：  

+ N个状态，$$\mathbf{y} = y_{1},y_{2},..,y_{N}$$  

+ 任意两种状态之间的转移概率  

$$
p(y_{t}^j=1\mid y_{t-1}^j=1)=a_{i,j}=\mathbf{A},or \\
p(y_{t}^j=1\mid y_{t-1}^j=1)\sim Multionmial(a_{i,1},..,a_{i,m}),\forall_{i} \in I
$$

Multinomial Distribution:多项式分布

+ 输出观测值，$$\mathbf{x} = x_{1},x_{2},..,x_{T}$$  

+ 发射概率(Emmision Probabilities)，从一个状态产生一个观测值的概率。

$$p(x_{t}\mid y_{t}^i)\sim Multionmial(b_{i,1},..,b_{i,K}),\forall_{i} \in I$$

+ 开始概率或先验概率 

$$p(y_{1})\sim Multionmial(\pi_{1},..,\pi_{m})$$  

**隐马尔可夫模型的EM算法**  

HMM有N个隐藏的状态以及T个观测值，由于存在$$N^T$$个序列，计算总观测似然的计算成本很高。这可以使用Baum-Welch算法（也称为前向-后向算法）有效地计算   

+ 完全的对数似然  

$$L(\theta;\mathbf{x},\mathbf{y}) = logp(\mathbf{x},\mathbf{y};\theta)\\
=log\prod_{n}p(y_{1}^n)\prod_{t=2}^T p(y_{t}^n\mid y_{t-1}^n)\prod_{t=1}^T p(x_{t}^n\mid y_{t}^n)$$

+ 完全对数似然期望  

$$<L(\theta;\mathbf{x},\mathbf{y})>_{q} =\sum_{n}(<y_{n,1}^i>_{p(y_{n,1}\mid \mathbf{x}_{n})}log\pi_{i})+ \sum_{n}\sum_{t=2}^{T}(<y_{n,t-1}^i,y_{n,t}^j>_{p(y_{n,t-1},y_{n,t}\mid \mathbf{x}_{n})}loga_{i,j})+\sum_{n}\sum_{t=1}^{T}(x_{n,t}^k<y_{n,t}^i>_{p(y_{n,t}\mid \mathbf{x}_{n})}logb_{i,k})
$$  


+ Expectation Step：固定$$\theta$$并计算边缘后验  

$$\gamma_{n,t}^i=<y_{n,t}^i>=p(y_{n,t}^i=1\mid \mathbf{x}_{n}),\\
\xi_{n,t}^{i,j}=<y_{n,t-1}^i,y_{n,t}^j>=p(y_{n,t}^i=1,y_{n,t}^j=1\mid \mathbf{x}_{n})$$


+ Maximization Step:通过MLE更新$$\theta$$   

$$
\pi^{ML}_{i} = \frac{\sum_{n}\gamma_{n,1}^i}{N}\\
a^{ML}_{ij}=\frac{\sum_{n}\sum_{t=2}^{T}\xi_{n,t}^{i,j}}{\sum_{n}\sum_{t}^{T-1}\gamma_{n,t}^i}\\
b^{ML}_{ik}=\frac{\sum_{n}\sum_{t}^{T}\gamma_{n,t}^i x_{n,t}^k}{\sum_{n}\sum_{t}^{T-1}\gamma_{n,t}^i}
$$

## 示例：常规BN的EM(EM for a general BN )  

对于普遍的贝叶斯网络$$p(x) = \prod_{i}p(x_{i}\mid pa(x_{i}))$$,$$pa(x_{i})$$是节点$$x_{i}$$的父节点，期望最大化算法可以写成(expectation maximization algorithm)

![_config.yml]({{ site.baseurl }}/images/10708/image145.png) 

## 示例： EM for conditional mixture models(条件混合模型)   

例如，我们将建立$$p(Y\mid X)$$模型为$$X$$的不同区域使用不同的专家,我们选择的专家是潜在变量$$z$$,$$p(y\mid x)$$模型是基于$$x$$选择一个专家，专家基于$$x$$产生回归

$$
p(y\mid x)=\sum_{k}p(z^k=1\mid x;\chi)p(y\mid z^k=1,x;\theta_{i},\sigma)\\
=\sum_{k}\frac{p(z^k=1,x)}{p(x)}\frac{p(y,z^k=1,x)}{p(z^k=1,x)}\\
=\sum_{k}\frac{p(y,z^k=1,x)}{p(x)}\\
=\frac{p(y,x)}{p(x)}
$$

+ 潜在变量$$Z$$使用softmax选择专家进行标准化（softmax保证输出是一个分布）

$$p(z^k=1\mid x) = softmax(\chi^T x)$$  

+ 每个专家都是一个线性回归。输出的分布有方差$$\sigma_{k}^2$$和均值$$\theta_{k}^T x$$  

$$p(y\mid x,z^k = 1)=\eta(y;\theta_{k}^T x,\sigma_{k}^2)$$

+ expert responsibilities 后验概率

$$p(z^k=1\mid x,y;\theta)=\frac{p(z^k=1\mid x)p_{k}(y\mid x,z^k = 1;\theta_{k},\sigma_{k}^2)}{\sum_{j}p(z^j=1\mid x)p_{j}(y\mid x,z^j = 1;\theta_{j},\sigma_{j}^2)}$$

$$p(z^k=1\mid x)p_{k}(y\mid x,z^k = 1;\theta_{k},\sigma_{k}^2) = p_{k}(z^k=1,y\mid x)$$  


+ 用完全对数似然的期望作为损失函数  

![_config.yml]({{ site.baseurl }}/images/10708/image146.png) 

EM算法： 

E-step计算responsibilities 后验概率

$$\tau_{n}^{k(t)}=p(z_{n}^k=1\mid x_{n},y_{n};\theta)=\frac{p(z_{n}^k=1\mid x_{n})p_{k}(y_{n}\mid x_{n},z_{n}^k = 1;\theta_{k},\sigma_{k}^2)}{\sum_{j}p(z^j=1\mid x)p_{j}(y_{n}\mid x_{n},z_{n}^j = 1;\theta_{j},\sigma_{j}^2)}$$

M-step计算每个专家的线性回归，每个数据点由expert responsibilities加权

EM变体  

+ 部分隐藏的变量：当某些情况下（而不是其他情况）缺少（隐藏）变量时，我们可以使用EM。 在E-Step中，我们只能估计不完整案例的隐藏变量。 然后，M步骤优化完整数据的对数似然性，再加上使用E步骤找到的不完整数据的预期可能性。

+ 稀疏EM：在此变体中，您没有完全重新计算所有模型下每个数据点的后验概率，因为它实际上为零。 相反，您保留一个“活动列表”，不时更新一次。

+ 通用（不完全）EM：即使有完整的数据，也可能很难在M步中找到ML参数。 我们仍然可以通过执行M步（稍微提高几率）（例如梯度步）来取得进步。 回顾专家模型混合中的IRLS步骤。

# 总结

# 其他资源 

Jordan textbook, Ch. 11   
Koller textbook, Ch. 19.1-19.4  
Borman, The EM algorithm (A short tutorial)   
Variations on EM algorithm by Neal and Hinton   

参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



