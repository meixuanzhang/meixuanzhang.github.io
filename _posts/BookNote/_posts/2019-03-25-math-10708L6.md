---
layout: post
title: 译Lecture 6 Learning Partially Observed GM and the EM Algorithm
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  


介绍了利用EM（Baum-Welch）算法从数据中估计图模型参数的过程。

# 介绍  

在之前的讲座中，我们介绍了从数据中学习图模型的概念，其中学习是估计参数的过程，在某些情况下，还包括从数据中估计网络结构的过程。第五讲在完全观测GMs的背景下介绍了这个概念。对于每个条件概率分布，在完全观测的节点和全局独立参数的设置中的最大似然估计非常简单，因为似然函数可以完全分解为独立项的乘积，每个项对应于网络中的条件概率分布。这意味着我们可以独立于网络的其余部分最大化每个局部似然函数，然后组合解决方案以获得MLE解决方案。

在本讲座中，我们将注意力转向部分观测的图模型，这是同等重要的一类模型，包括隐马尔可夫模型和高斯混合模型。 我们将看到，在部分观测的数据下，我们失去了似然函数的重要属性：其单峰性，闭式表示以及分解为不同参数的似然的乘积。因此，学习问题变得更加复杂，我们转向Expectation-Maximization算法来估计模型参数。

# 部分观测的图模型(Partially Observed GMs)    

## 有向但部分观测的图模型(Directed but partially observed GM)  

首先考虑完全观测的有向图模型的情况：  

![_config.yml]({{ site.baseurl }}/images/10708/image133.png)   

完全观测的有向图模型对数似然函数可分解为局部项的总和。我们可以独立地最大化每个局部似然函数。  

![_config.yml]({{ site.baseurl }}/images/10708/image136.png)  


将此与有向但部分观测的GM进行比较。 假设现在有一个变量没有观测到，但是我们仍然想写下数据的似然( likelihood)。 为此，为此，我们将不可观测概率边缘化(即积分或求和)。  

![_config.yml]({{ site.baseurl }}/images/10708/image134.png)  

部分观测的有向图模型。对于未观测(unobserved)或潜在变量(latent variables)，对数似然函数不再分解为局部项之和。所有参数通过边缘化耦合在一起。

![_config.yml]({{ site.baseurl }}/images/10708/image137.png) 

参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



