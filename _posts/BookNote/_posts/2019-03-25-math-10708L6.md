---
layout: post
title: 译Lecture 6 Learning Partially Observed GM and the EM Algorithm
date:   2019-03-25
categories: ["Probabilistic Graphical Models"]
---  


介绍了利用EM（Baum-Welch）算法从数据中估计图模型参数的过程。

# 介绍  

在之前的讲座中，我们介绍了从数据中学习图模型的概念，其中学习是估计参数的过程，在某些情况下，还包括从数据中估计网络结构的过程。第五讲在完全观测GMs的背景下介绍了这个概念。对于每个条件概率分布，在完全观测的节点和全局独立参数的设置中的最大似然估计非常简单，因为似然函数可以完全分解为独立项的乘积，每个项对应于网络中的条件概率分布。这意味着我们可以独立于网络的其余部分最大化每个局部似然函数，然后组合解决方案以获得MLE解决方案。

在本讲座中，我们将注意力转向部分观测的图模型，这是同等重要的一类模型，包括隐马尔可夫模型和高斯混合模型。 我们将看到，在部分观测的数据下，我们失去了似然函数的重要属性：其单峰性，闭式表示以及分解为不同参数的似然的乘积。因此，学习问题变得更加复杂，我们转向Expectation-Maximization算法来估计模型参数。



参考：
[Lecture 6: Learning Partially Observed GM and the EM Algorithm](https://sailinglab.github.io/pgm-spring-2019/notes/lecture-06/)



